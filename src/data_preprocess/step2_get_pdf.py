# -*- coding: utf-8 -*-
"""
Author: Zhengyuan Dong
Created: 2025-03-29
Description: Batch-download PDFs from a Parquet file containing "openaccessurl".
             Caches each downloaded PDF's file path in JSON to avoid re-downloading.
             Implements domain-based round-robin fetching to avoid rate limits.
             Uses URL hash for filename to ensure uniqueness and avoid duplicate downloads.
             Supports two download modes: 'wget' (default) and 'request'.
             If downloads fail, the failed URLs will be retried once.
"""

import os
import time
import json
import hashlib  # ç”¨äºç”Ÿæˆ URL å“ˆå¸Œ
import requests
import subprocess  # ç”¨äº wget æ¨¡å¼
import pandas as pd
import pyarrow.parquet as pq
from urllib.parse import urlparse  # ç”¨äºåŸŸåæå–
from tqdm import tqdm  # æ·»åŠ è¿›åº¦æ¡

DOWNLOAD_MODE = "wget"  # é»˜è®¤ä¸‹è½½æ¨¡å¼ ("wget" æˆ– "request")

######## JSON cache load/save functions ########
def load_json_cache(file_path):
    if not os.path.isfile(file_path):
        print("âš ï¸  JSON cache file not found:", file_path)
        return {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print("âœ…  Loaded JSON cache from", file_path, "with", len(data), "entries.")
        return data
    except Exception as e:
        print("âŒ  Could not load JSON cache:", e)
        return {}

def save_json_cache(data, file_path):
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print("âŒ  Could not save JSON cache:", e)

######## Utility: extract domain ########
def extract_domain(url):
    try:
        parsed = urlparse(url)
        return parsed.netloc.lower()
    except Exception as e:
        print("âŒ  Failed to extract domain from", url, ":", e)
        return "unknown"

######## PDF download function ########
def download_pdf(url, output_folder, mode=DOWNLOAD_MODE, max_retries=3, sleep_time=3, timeout=15):
    """
    ä¸‹è½½ URL å¯¹åº”çš„ PDFï¼Œè¿”å›æœ¬åœ°è·¯å¾„ï¼›è‹¥æ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™ç›´æ¥è¿”å›ã€‚
    """
    if not os.path.isdir(output_folder):
        os.makedirs(output_folder, exist_ok=True)

    # ç”¨ SHA256 ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    url_hash = hashlib.sha256(url.encode('utf-8')).hexdigest()
    safe_filename = url_hash + ".pdf"
    local_path = os.path.join(output_folder, safe_filename)

    # å·²å­˜åœ¨åˆ™ç›´æ¥è¿”å›
    if os.path.isfile(local_path):
        print("ğŸ“‚  Retrieved local file for", url)
        return local_path

    attempts = 0
    while attempts < max_retries:
        if mode == "wget":
            try:
                # ä½¿ç”¨ wget ä¸‹è½½
                subprocess.run(["wget", "-q", "-O", local_path, url], timeout=timeout)
                if os.path.isfile(local_path) and os.path.getsize(local_path) > 0:
                    print("âœ…  Downloaded (wget) for", url)
                    return local_path
                else:
                    print("âŒ  Error downloading", url)
            except Exception as e:
                print("âŒ  Error downloading", url, ":", e)
        else:  # request æ¨¡å¼
            try:
                headers = {
                    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.81 Safari/537.36"
                }
                resp = requests.get(url, timeout=timeout, headers=headers)
                if resp.status_code == 200 and resp.content:
                    with open(local_path, 'wb') as f:
                        f.write(resp.content)
                    print("âœ…  Downloaded (request) for", url)
                    return local_path
                else:
                    print("âŒ  Error downloading", url, "- status", resp.status_code)
            except Exception as e:
                print("âŒ  Error downloading", url, "on attempt", attempts+1, ":", e)

        attempts += 1
        if attempts < max_retries:
            time.sleep(sleep_time)
    print("âŒ  Failed to download after", max_retries, "attempts:", url)
    return None

######## Domain-based round-robin download function ########
def domain_round_robin_download(urls, output_folder, pdf_cache, cache_path):
    """
    æŒ‰åŸŸååˆ†ç»„åé‡‡ç”¨è½®è½¬æ–¹å¼ä¸‹è½½ï¼Œæ›´æ–°ç¼“å­˜å¹¶ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ã€‚
    è¿”å›ä¸€ä¸ª (downloaded_paths, failed_urls) çš„å…ƒç»„ã€‚
    """
    # åˆ†ç»„
    domain_groups = {}
    for url in urls:
        domain = extract_domain(url)
        domain_groups.setdefault(domain, []).append(url)

    total = len(urls)
    pbar = tqdm(total=total, desc="Downloading PDFs", unit="url")
    downloaded_paths = {}
    failed_urls = []
    # éå†å„ç»„
    while any(domain_groups.values()):
        for domain in list(domain_groups.keys()):
            if domain_groups[domain]:
                url = domain_groups[domain].pop(0)
                local_pdf_path = download_pdf(url, output_folder)
                pbar.update(1)
                if local_pdf_path is not None:
                    pdf_cache[url] = local_pdf_path
                    downloaded_paths[url] = local_pdf_path
                else:
                    failed_urls.append(url)
                save_json_cache(pdf_cache, cache_path)  # æ¯æ¬¡æ›´æ–°ç¼“å­˜
            if not domain_groups.get(domain):
                del domain_groups[domain]
    pbar.close()
    return downloaded_paths, failed_urls

######## Main Script ########
def main():
    parquet_path = "extracted_annotations.parquet"
    if not os.path.isfile(parquet_path):
        print("âŒ  Parquet file not found:", parquet_path)
        return
    df_parquet = pd.read_parquet(parquet_path)
    if "extracted_openaccessurl" not in df_parquet.columns:
        print("âŒ  'extracted_openaccessurl' column not found in the parquet file.")
        return
    all_urls = set(df_parquet["extracted_openaccessurl"].dropna().unique())
    print("ğŸ“„  Loaded", len(df_parquet), "rows from", parquet_path, "with", len(all_urls), "unique URLs.")

    pdf_cache_path = "pdf_download_cache.json"
    pdf_cache = load_json_cache(pdf_cache_path)
    # æ£€æŸ¥ç¼“å­˜ä¸­å­˜åœ¨ä¸”æœ¬åœ°æ–‡ä»¶å­˜åœ¨çš„ URL
    cached_urls = {url for url, path in pdf_cache.items() if path and os.path.isfile(path)}
    missing_urls = all_urls - cached_urls
    print("ğŸ“Š  Total URLs:", len(all_urls))
    print("ğŸ“‚  Already cached:", len(cached_urls))
    print("ğŸ†•  Missing (need fetch):", len(missing_urls))

    output_folder = "downloaded_pdfs"
    # ç¬¬ä¸€è½®ä¸‹è½½
    downloaded_paths, failed_urls = domain_round_robin_download(missing_urls, output_folder, pdf_cache, pdf_cache_path)
    
    # å¦‚æœæœ‰å¤±è´¥çš„é“¾æ¥ï¼Œé‡è¯•ä¸€æ¬¡
    if failed_urls:
        print("ğŸ”„  Retrying failed downloads for", len(failed_urls), "URLs...")
        _, failed_urls = domain_round_robin_download(failed_urls, output_folder, pdf_cache, pdf_cache_path)
    
    if failed_urls:
        print("âŒ  Final failed URLs:")
        for url in failed_urls:
            print("   ", url)
    else:
        print("ğŸ‰  All downloads succeeded.")

    print("ğŸ‰  PDF download process complete. Cache now has", len(pdf_cache), "entries.")

if __name__ == "__main__":
    main()
