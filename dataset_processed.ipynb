{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_ingestion.citation_fetcher import AcademicAPIFactory\n",
    "from src.data_ingestion.readme_parser import BibTeXExtractor, LinkExtractor, MarkdownHandler, ExtractionFactory\n",
    "from src.data_ingestion.bibtex_parser import BibTeXFactory, ensure_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done by 250212\n",
    "\n",
    "---\n",
    "\n",
    "## Simplified and Reliable Data Collection Plan\n",
    "\n",
    "### Objective:\n",
    "The goal is to create a reliable and minimal dataset by filtering and validating model cards from multiple sources. We aim to ensure that every card retained in the dataset:\n",
    "1. Contains valid and verified metadata (BibTeX entries, paper links, GitHub repositories).\n",
    "2. Has no duplicate entriesâ€”retain only the card with the highest `downloads` count if duplicates exist.\n",
    "3. Downloads the associated data locally and reads it for further processing.\n",
    "4. Annotates the dataset with citation information to build a **citation graph** that reflects inter-card relationships (i.e., a cardâ€™s associated paper cites another card's associated paper).  \n",
    "\n",
    "We will leverage a **Citation Graph API** to annotate and establish these relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Plan:\n",
    "\n",
    "#### 1. **Data Filtering Phase**\n",
    "   - **Input Sources:** \n",
    "     - card_readme content\n",
    "     - BibTeX entries\n",
    "     - GitHub repo READMEs\n",
    "   - **Filtering Criteria:**\n",
    "     - Remove any card that lacks all of the following:\n",
    "       - Paper link (arXiv, DOI, or PDF)\n",
    "       - BibTeX entry\n",
    "       - GitHub link\n",
    "     - Eliminate duplicate cards; keep the one with the highest `downloads` count.\n",
    "     - Validate remaining links to ensure they are functional and non-placeholder.\n",
    "\n",
    "#### 2. **Data Downloading Phase**\n",
    "   - Download the related resources (PDFs, code repos, etc.).\n",
    "   - Organize and read the downloaded content from the local directory for further processing.\n",
    "\n",
    "#### 3. **Citation Annotation Phase**\n",
    "   - Use the **Citation Graph API** to analyze and annotate each card by identifying citations between associated papers.\n",
    "   - Establish inter-card relationships (i.e., identify which cardâ€™s paper cites another).\n",
    "\n",
    "#### 4. **Final Dataset Construction**\n",
    "   - Create a structured and annotated dataset that includes:\n",
    "     - Valid and non-duplicate card data.\n",
    "     - Citation relationships between cards.\n",
    "     - Metadata for each card (downloads, likes, BibTeX, and link status).\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_type = 'modelcard'\n",
    "df_split_temp = pd.read_parquet(f\"data/{data_type}_step3_markdown_gated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_type = 'modelcard'\n",
    "df_split_temp = pd.read_parquet(f\"data/{data_type}_step4_markdown_gated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'extracted_bibtex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'extracted_bibtex'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m key_downloads \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert extracted_bibtex and extracted_markdown_table to tuples for uniqueness checks\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df_split_temp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted_bibtex_tuple\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_split_temp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey_bibtex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (x,))\n\u001b[1;32m     14\u001b[0m df_split_temp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted_markdown_table_tuple\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_split_temp[key_markdown_table]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (x,))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# First Layer: Initial filtering based on your specified conditions\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'extracted_bibtex'"
     ]
    }
   ],
   "source": [
    "# Filter: \n",
    "# - extracted_bibtex is not null or an empty list\n",
    "# - contains_markdown_table is True\n",
    "# - they are valid, unique\n",
    "\n",
    "import pandas as pd\n",
    "# Columns and filtering keys\n",
    "key_bibtex = \"extracted_bibtex\"\n",
    "key_markdown = \"contains_markdown_table\"\n",
    "key_markdown_table = \"extracted_markdown_table\"\n",
    "key_downloads = \"downloads\"\n",
    "# Convert extracted_bibtex and extracted_markdown_table to tuples for uniqueness checks\n",
    "df_split_temp[\"extracted_bibtex_tuple\"] = df_split_temp[key_bibtex].apply(lambda x: tuple(x) if isinstance(x, list) else (x,))\n",
    "df_split_temp[\"extracted_markdown_table_tuple\"] = df_split_temp[key_markdown_table].apply(lambda x: tuple(x) if isinstance(x, list) else (x,))\n",
    "# First Layer: Initial filtering based on your specified conditions\n",
    "filtered_df = df_split_temp[\n",
    "    df_split_temp[key_bibtex].notnull() & \n",
    "    (df_split_temp[key_bibtex].apply(lambda x: len(x) > 0)) & \n",
    "    (df_split_temp[key_markdown] == True)\n",
    "]\n",
    "print(\"Number of rows after first filter:\", len(filtered_df))\n",
    "# Second Layer: Remove duplicates based on markdown table and downloads\n",
    "filtered_df = filtered_df.sort_values(by=key_downloads, ascending=False)\n",
    "unique_by_markdown = filtered_df.drop_duplicates(subset=[\"extracted_markdown_table_tuple\"], keep=\"first\")\n",
    "# Final Step: Ensure no duplicates for the combination of bibtex and markdown table\n",
    "final_unique_df = unique_by_markdown.drop_duplicates(subset=[\"extracted_bibtex_tuple\", \"extracted_markdown_table_tuple\"])\n",
    "# Display the final result\n",
    "print(\"Number of unique rows after full filtering:\", len(unique_by_markdown), len(final_unique_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming df_split_temp has a column \"csv_path\" storing the local CSV table paths\n",
    "key_csv_path = \"csv_path\"\n",
    "valid_csv_df = unique_by_markdown[unique_by_markdown[key_csv_path].apply(lambda x: os.path.exists(x) if isinstance(x, str) else False)]\n",
    "num_tables = len(valid_csv_df)\n",
    "num_cols = 0\n",
    "total_rows = 0\n",
    "for csv_file in tqdm(valid_csv_df[key_csv_path]):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        num_cols += df.shape[1]  # Number of columns in the CSV\n",
    "        total_rows += df.shape[0]  # Number of rows in the CSV\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_file}: {e}\")\n",
    "avg_rows = total_rows / num_tables if num_tables > 0 else 0\n",
    "new_row = {\n",
    "    \"Benchmark\": \"SciLake\",\n",
    "    \"# Tables\": num_tables,\n",
    "    \"# Cols\": num_cols,\n",
    "    \"Avg # Rows\": avg_rows,\n",
    "    \"Size (GB)\": \"nan\"\n",
    "}\n",
    "benchmark_data = {\n",
    "    \"Benchmark\": [\"SANTOS Small\", \"TUS Small\", \"TUS Large\", \"SANTOS Large\", \"WDC\"],\n",
    "    \"# Tables\": [550, 1530, 5043, 11090, 50000000],\n",
    "    \"# Cols\": [6322, 14810, 54923, 123477, 250000000],\n",
    "    \"Avg # Rows\": [6921, 4466, 1915, 7675, 14],\n",
    "    \"Size (GB)\": [0.45, 1, 1.5, 11, 500]\n",
    "}\n",
    "\n",
    "# Create benchmark DataFrame and append new row\n",
    "benchmark_df = pd.DataFrame(benchmark_data)\n",
    "benchmark_df = pd.concat([benchmark_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "benchmark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import LinkExtractor, BibTeXExtractor, ExtractionFactory\n",
    "import pandas as pd\n",
    "df_split_temp['link_info'] = df_split_temp['card_readme'].apply(\n",
    "    lambda x: LinkExtractor().extract_links(str(x)) if pd.notnull(x) else {\"pdf_link\": None, \"github_link\": None, \"all_links\": []}\n",
    ")\n",
    "df_split_temp['pdf_link'] = df_split_temp['link_info'].apply(lambda x: x['pdf_link'])\n",
    "df_split_temp['github_link'] = df_split_temp['link_info'].apply(lambda x: x['github_link'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   []\n",
       "1                                                   []\n",
       "2                                                   []\n",
       "3    [@article{DBLP:journals/corr/abs-1810-04805, a...\n",
       "4    [@article{DBLP:journals/corr/abs-1911-02116, a...\n",
       "5    [@misc{radford2022whisper, doi = {10.48550/ARX...\n",
       "6                                                   []\n",
       "7                                                   []\n",
       "8                                                   []\n",
       "9    [@misc{grosman2021xlsr53-large-english, title=...\n",
       "Name: extracted_bibtex, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the extraction function to each row in the specified column\n",
    "df_split_temp[\"extracted_bibtex\"] = df_split_temp[\"card_readme\"].apply(lambda x: BibTeXExtractor().extract(x) if isinstance(x, str) else [])\n",
    "df_split_temp[\"extracted_bibtex\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "import pandas as pd\n",
    "data_type = 'modelcard'\n",
    "df_split_temp = pd.read_parquet(f\"data/{data_type}_step4_markdown_gated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 130019/151086 [04:03<00:35, 595.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading cleaned_markdown_csvs/yasakoko_NoAI-Diffusion-variety_markdown_553967.csv: No columns to parse from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138599/151086 [04:35<00:26, 464.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading cleaned_markdown_csvs/OzzyGT_melvyn_markdown_735407.csv: No columns to parse from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142861/151086 [04:45<00:15, 534.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading cleaned_markdown_csvs/yuneun92_koCSN_SAPR_markdown_811179.csv: No columns to parse from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151086/151086 [05:05<00:00, 494.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Benchmark</th>\n",
       "      <th># Tables</th>\n",
       "      <th># Cols</th>\n",
       "      <th>Avg # Rows</th>\n",
       "      <th>Size (GB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SANTOS Small</td>\n",
       "      <td>550</td>\n",
       "      <td>6322</td>\n",
       "      <td>6921.000000</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TUS Small</td>\n",
       "      <td>1530</td>\n",
       "      <td>14810</td>\n",
       "      <td>4466.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TUS Large</td>\n",
       "      <td>5043</td>\n",
       "      <td>54923</td>\n",
       "      <td>1915.000000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SANTOS Large</td>\n",
       "      <td>11090</td>\n",
       "      <td>123477</td>\n",
       "      <td>7675.000000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WDC</td>\n",
       "      <td>50000000</td>\n",
       "      <td>250000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SciLake</td>\n",
       "      <td>151086</td>\n",
       "      <td>898350</td>\n",
       "      <td>18.194154</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Benchmark  # Tables     # Cols   Avg # Rows Size (GB)\n",
       "0  SANTOS Small       550       6322  6921.000000      0.45\n",
       "1     TUS Small      1530      14810  4466.000000       1.0\n",
       "2     TUS Large      5043      54923  1915.000000       1.5\n",
       "3  SANTOS Large     11090     123477  7675.000000      11.0\n",
       "4           WDC  50000000  250000000    14.000000     500.0\n",
       "5       SciLake    151086     898350    18.194154       nan"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(151086)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_temp['csv_path'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          cleaned_markdown_csvs/sentence-transformers_al...\n",
       "2          cleaned_markdown_csvs/sentence-transformers_al...\n",
       "3          cleaned_markdown_csvs/google-bert_bert-base-un...\n",
       "5          cleaned_markdown_csvs/openai_whisper-large-v2_...\n",
       "9          cleaned_markdown_csvs/jonatasgrosman_wav2vec2-...\n",
       "                                 ...                        \n",
       "1108728    cleaned_markdown_csvs/touhidulislam_BERTweet_r...\n",
       "1108730    cleaned_markdown_csvs/tttx_problem226_model_au...\n",
       "1108739    cleaned_markdown_csvs/featherless-ai-quants_gr...\n",
       "1108742    cleaned_markdown_csvs/tttx_problem301_model_mo...\n",
       "1108757    cleaned_markdown_csvs/mradermacher_Mistral-qui...\n",
       "Name: csv_path, Length: 151086, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_temp[df_split_temp['csv_path'].notnull()]['csv_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### double check the extracted markdown first\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing using citation_fetcher.py functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 0/34691 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 3] ðŸ”Ž Searching for: DOI=, Title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 1/34691 [00:02<19:59:16,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 3] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 3] âœ… Found 2 references and 3 citations.\n",
      "[Row 4] ðŸ”Ž Searching for: DOI=, Title=Unsupervised Cross-lingual Representation Learning at Scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 2/34691 [00:03<16:21:51,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 4] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 4] âœ… Found 2 references and 3 citations.\n",
      "[Row 5] ðŸ”Ž Searching for: DOI=10.48550/arxiv.2212.04356, Title=Robust Speech Recognition via Large-Scale Weak Supervision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 3/34691 [00:04<11:59:38,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 5] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 5] âœ… Found 2 references and 2 citations.\n",
      "[Row 9] ðŸ”Ž Searching for: DOI=, Title=Fine-tuned XLSR-53 large model for speech recognition in English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 4/34691 [00:05<12:03:37,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 9] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 9] âœ… Found 2 references and 2 citations.\n",
      "[Row 11] ðŸ”Ž Searching for: DOI=, Title=RoBERTa: A Robustly Optimized BERT Pretraining Approach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 5/34691 [00:35<113:07:17, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 11] âœ… Found 45 references and 25 citations.\n",
      "[Row 13] ðŸ”Ž Searching for: DOI=, Title=Qwen2.5: A Party of Foundation Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 6/34691 [00:37<80:29:50,  8.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 13] âš ï¸ Low results! Only 3 references & 2 citations.\n",
      "[Row 13] âœ… Found 3 references and 2 citations.\n",
      "[Row 14] ðŸ”Ž Searching for: DOI=, Title=OPT: Open Pre-trained Transformer Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 7/34691 [00:39<60:38:12,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 14] âœ… Found 3 references and 3 citations.\n",
      "[Row 15] ðŸ”Ž Searching for: DOI=, Title=Visual Transformers: Token-based Image Representation and Processing for Computer Vision\n",
      "[Row 15] âŒ Error: object of type 'NoneType' has no len()\n",
      "[Row 15] ðŸ”Ž Searching for: DOI=, Title=Imagenet: A large-scale hierarchical image database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 8/34691 [00:42<51:27:57,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 15] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 15] âœ… Found 2 references and 3 citations.\n",
      "[Row 16] ðŸ”Ž Searching for: DOI=, Title=Language Models are Unsupervised Multitask Learners\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 9/34691 [00:43<38:13:37,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 16] âŒ Error: object of type 'NoneType' has no len()\n",
      "[Row 17] ðŸ”Ž Searching for: DOI=, Title=Crosslingual generalization through multitask finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 10/34691 [00:44<29:18:24,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 17] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 17] âœ… Found 2 references and 3 citations.\n",
      "[Row 19] ðŸ”Ž Searching for: DOI=, Title=ResNet strikes back: An improved training procedure in timm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 11/34691 [00:46<24:16:01,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 19] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 19] âœ… Found 2 references and 3 citations.\n",
      "[Row 20] ðŸ”Ž Searching for: DOI=, Title=Wespeaker: A research and production oriented speaker embedding learning toolkit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 12/34691 [00:47<20:26:03,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 20] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 20] âœ… Found 2 references and 3 citations.\n",
      "[Row 21] ðŸ”Ž Searching for: DOI=, Title=Deep residual learning for image recognition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 13/34691 [00:49<19:40:18,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 21] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 21] âœ… Found 2 references and 3 citations.\n",
      "[Row 23] ðŸ”Ž Searching for: DOI=, Title=Powerset multi-class cross entropy loss for neural speaker diarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 14/34691 [00:50<17:00:48,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 23] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 23] âœ… Found 2 references and 2 citations.\n",
      "[Row 24] ðŸ”Ž Searching for: DOI=, Title=DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 15/34691 [00:51<14:20:17,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 24] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 24] âœ… Found 2 references and 3 citations.\n",
      "[Row 25] ðŸ”Ž Searching for: DOI=, Title=Unsupervised Cross-lingual Representation Learning at Scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 16/34691 [00:52<13:47:50,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 25] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 25] âœ… Found 2 references and 3 citations.\n",
      "[Row 26] ðŸ”Ž Searching for: DOI=10.48550/arxiv.2212.04356, Title=Robust Speech Recognition via Large-Scale Weak Supervision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 17/34691 [00:53<12:31:19,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 26] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 26] âœ… Found 2 references and 2 citations.\n",
      "[Row 27] ðŸ”Ž Searching for: DOI=10.34740/kaggle/m/3301, Title=Gemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 18/34691 [00:54<11:48:35,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 27] âš ï¸ Low results! Only 3 references & 2 citations.\n",
      "[Row 27] âœ… Found 3 references and 2 citations.\n",
      "[Row 31] ðŸ”Ž Searching for: DOI=, Title=SimCSE: Simple Contrastive Learning of Sentence Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 19/34691 [01:35<125:39:14, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 31] âœ… Found 70 references and 25 citations.\n",
      "[Row 32] ðŸ”Ž Searching for: DOI=, Title=Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 20/34691 [02:07<182:04:11, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 32] âœ… Found 45 references and 25 citations.\n",
      "[Row 33] ðŸ”Ž Searching for: DOI=, Title=Powerset multi-class cross entropy loss for neural speaker diarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 21/34691 [02:08<130:19:23, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 33] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 33] âœ… Found 2 references and 2 citations.\n",
      "[Row 34] ðŸ”Ž Searching for: DOI=, Title=Fine-tuned XLSR-53 large model for speech recognition in Chinese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 22/34691 [02:09<94:49:13,  9.85s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 34] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 34] âœ… Found 2 references and 2 citations.\n",
      "[Row 35] ðŸ”Ž Searching for: DOI=, Title=RoBERTa: A Robustly Optimized BERT Pretraining Approach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 23/34691 [02:12<72:38:13,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 35] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 35] âœ… Found 2 references and 3 citations.\n",
      "[Row 36] ðŸ”Ž Searching for: DOI=, Title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 24/34691 [02:46<150:50:26, 15.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 36] âœ… Found 55 references and 25 citations.\n",
      "[Row 38] ðŸ”Ž Searching for: DOI=, Title=Chronos: Learning the Language of Time Series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 25/34691 [02:48<109:13:55, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 38] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 38] âœ… Found 2 references and 3 citations.\n",
      "[Row 40] ðŸ”Ž Searching for: DOI=, Title=Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 26/34691 [02:49<79:48:44,  8.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 40] âœ… Found 3 references and 3 citations.\n",
      "[Row 43] ðŸ”Ž Searching for: DOI=, Title=High-Resolution Image Synthesis With Latent Diffusion Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 27/34691 [03:42<210:36:14, 21.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 43] âœ… Found 94 references and 25 citations.\n",
      "[Row 47] ðŸ”Ž Searching for: DOI=, Title=ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 28/34691 [04:28<278:28:00, 28.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 47] âŒ Error: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "[Row 48] ðŸ”Ž Searching for: DOI=, Title=OPT: Open Pre-trained Transformer Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 29/34691 [04:41<234:05:28, 24.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 48] âš ï¸ Low results! Only 0 references & 25 citations.\n",
      "[Row 48] âœ… Found 0 references and 25 citations.\n",
      "[Row 51] ðŸ”Ž Searching for: DOI=, Title=Longformer: The Long-Document Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 30/34691 [04:43<167:48:53, 17.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 51] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 51] âœ… Found 2 references and 3 citations.\n",
      "[Row 52] ðŸ”Ž Searching for: DOI=, Title=C-Pack: Packaged Resources To Advance General Chinese Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 31/34691 [04:44<120:53:28, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 52] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 52] âœ… Found 2 references and 3 citations.\n",
      "[Row 56] ðŸ”Ž Searching for: DOI=, Title=Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 32/34691 [04:45<88:12:39,  9.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 56] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 56] âœ… Found 2 references and 2 citations.\n",
      "[Row 59] ðŸ”Ž Searching for: DOI=, Title=End-to-end speaker segmentation for overlap-aware resegmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 33/34691 [05:13<142:44:22, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 59] âœ… Found 25 references and 25 citations.\n",
      "[Row 60] ðŸ”Ž Searching for: DOI=, Title=DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 34/34691 [05:48<199:29:06, 20.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 60] âœ… Found 47 references and 25 citations.\n",
      "[Row 62] ðŸ”Ž Searching for: DOI=, Title=Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 35/34691 [05:50<146:14:24, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 62] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 62] âœ… Found 2 references and 3 citations.\n",
      "[Row 63] ðŸ”Ž Searching for: DOI=, Title=LAION-5B: An open large-scale dataset for training next generation image-text models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 36/34691 [05:51<106:52:05, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 63] âœ… Found 3 references and 3 citations.\n",
      "[Row 64] ðŸ”Ž Searching for: DOI=, Title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 37/34691 [05:53<78:57:12,  8.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 64] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 64] âœ… Found 2 references and 3 citations.\n",
      "[Row 65] ðŸ”Ž Searching for: DOI=, Title=DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n",
      "[Row 65] âŒ Error: object of type 'NoneType' has no len()\n",
      "[Row 65] ðŸ”Ž Searching for: DOI=, Title=DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 38/34691 [06:24<145:10:28, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 65] âœ… Found 47 references and 25 citations.\n",
      "[Row 66] ðŸ”Ž Searching for: DOI=10.18653/v1/2020.findings-emnlp.148, Title=TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 39/34691 [08:11<411:13:11, 42.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 66] âœ… Found 191 references and 25 citations.\n",
      "[Row 67] ðŸ”Ž Searching for: DOI=, Title=BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 40/34691 [08:43<381:02:26, 39.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 67] âœ… Found 44 references and 25 citations.\n",
      "[Row 68] ðŸ”Ž Searching for: DOI=10.48550/arxiv.2212.04356, Title=Robust Speech Recognition via Large-Scale Weak Supervision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 41/34691 [08:47<275:38:51, 28.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 68] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 68] âœ… Found 2 references and 3 citations.\n",
      "[Row 69] ðŸ”Ž Searching for: DOI=, Title=Fine-tuned XLSR-53 large model for speech recognition in Russian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 42/34691 [08:48<196:13:51, 20.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 69] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 69] âœ… Found 2 references and 2 citations.\n",
      "[Row 71] ðŸ”Ž Searching for: DOI=, Title=DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 43/34691 [09:08<195:00:24, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 71] âœ… Found 18 references and 25 citations.\n",
      "[Row 72] ðŸ”Ž Searching for: DOI=, Title=Visual Transformers: Token-based Image Representation and Processing for Computer Vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 44/34691 [09:41<231:50:20, 24.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 72] âœ… Found 49 references and 25 citations.\n",
      "[Row 73] ðŸ”Ž Searching for: DOI=, Title=Chronos: Learning the Language of Time Series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 45/34691 [09:42<165:45:44, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 73] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 73] âœ… Found 2 references and 3 citations.\n",
      "[Row 75] ðŸ”Ž Searching for: DOI=, Title=Fine-tuned XLSR-53 large model for speech recognition in Portuguese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 46/34691 [09:43<119:35:27, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 75] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 75] âœ… Found 2 references and 2 citations.\n",
      "[Row 76] ðŸ”Ž Searching for: DOI=, Title=CamemBERT: a Tasty French Language Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 47/34691 [09:45<89:23:22,  9.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 76] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 76] âœ… Found 2 references and 3 citations.\n",
      "[Row 79] ðŸ”Ž Searching for: DOI=10.18653/v1/2022.acl-demo.25, Title=TimeLMs: Diachronic Language Models from Twitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 48/34691 [09:47<68:10:46,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 79] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 79] âœ… Found 2 references and 3 citations.\n",
      "[Row 81] ðŸ”Ž Searching for: DOI=, Title=BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 49/34691 [09:48<51:17:07,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 81] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 81] âœ… Found 2 references and 3 citations.\n",
      "[Row 83] ðŸ”Ž Searching for: DOI=, Title=JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create State-of-the-Art Japanese Retrievers with Constrained Resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 50/34691 [09:49<38:45:34,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 83] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 83] âœ… Found 2 references and 2 citations.\n",
      "[Row 84] ðŸ”Ž Searching for: DOI=, Title=BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 51/34691 [10:19<113:57:54, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 84] âœ… Found 44 references and 25 citations.\n",
      "[Row 86] ðŸ”Ž Searching for: DOI=, Title=FNet: Mixing Tokens with Fourier Transforms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 52/34691 [11:18<250:42:12, 26.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 86] âœ… Found 95 references and 25 citations.\n",
      "[Row 87] ðŸ”Ž Searching for: DOI=, Title=BERTimbau: pretrained BERT models for Brazilian Portuguese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 53/34691 [11:20<179:24:18, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 87] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 87] âœ… Found 2 references and 3 citations.\n",
      "[Row 88] ðŸ”Ž Searching for: DOI=, Title=Open Source Strikes Bread - New Fluffy Embeddings Model\n",
      "[Row 88] âŒ Error: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "[Row 88] ðŸ”Ž Searching for: DOI=, Title=AnglE-optimized Text Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 54/34691 [12:31<330:27:07, 34.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 88] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 88] âœ… Found 2 references and 2 citations.\n",
      "[Row 89] ðŸ”Ž Searching for: DOI=, Title=C-Pack: Packaged Resources To Advance General Chinese Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 55/34691 [12:32<234:13:02, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 89] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 89] âœ… Found 2 references and 3 citations.\n",
      "[Row 93] ðŸ”Ž Searching for: DOI=, Title=SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 56/34691 [12:45<200:34:38, 20.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 93] âš ï¸ Low results! Only 0 references & 25 citations.\n",
      "[Row 93] âœ… Found 0 references and 25 citations.\n",
      "[Row 94] ðŸ”Ž Searching for: DOI=, Title=Sigmoid Loss for Language Image Pre-Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 57/34691 [12:46<144:16:24, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 94] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 94] âœ… Found 2 references and 3 citations.\n",
      "[Row 98] ðŸ”Ž Searching for: DOI=, Title=C-Pack: Packaged Resources To Advance General Chinese Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 58/34691 [13:00<142:21:33, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 98] âš ï¸ Low results! Only 0 references & 25 citations.\n",
      "[Row 98] âœ… Found 0 references and 25 citations.\n",
      "[Row 99] ðŸ”Ž Searching for: DOI=, Title=High-Resolution Image Synthesis With Latent Diffusion Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 59/34691 [13:02<104:58:07, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 99] âœ… Found 3 references and 3 citations.\n",
      "[Row 101] ðŸ”Ž Searching for: DOI=, Title=DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 60/34691 [13:47<203:32:58, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 101] âŒ Error: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "[Row 102] ðŸ”Ž Searching for: DOI=, Title=mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 61/34691 [13:49<148:00:15, 15.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 102] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 102] âœ… Found 2 references and 2 citations.\n",
      "[Row 103] ðŸ”Ž Searching for: DOI=, Title=Powerset multi-class cross entropy loss for neural speaker diarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 62/34691 [14:14<175:51:57, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 103] âœ… Found 23 references and 20 citations.\n",
      "[Row 105] ðŸ”Ž Searching for: DOI=, Title=Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 63/34691 [14:16<129:18:15, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 105] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 105] âœ… Found 2 references and 2 citations.\n",
      "[Row 107] ðŸ”Ž Searching for: DOI=10.48550/arxiv.2201.12086, Title=BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 64/34691 [14:17<93:54:39,  9.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 107] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 107] âœ… Found 2 references and 2 citations.\n",
      "[Row 109] ðŸ”Ž Searching for: DOI=, Title=Multilingual E5 Text Embeddings: A Technical Report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 65/34691 [14:22<78:31:12,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 109] âš ï¸ Low results! Only 0 references & 4 citations.\n",
      "[Row 109] âœ… Found 0 references and 4 citations.\n",
      "[Row 110] ðŸ”Ž Searching for: DOI=, Title=Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 66/34691 [14:34<89:32:17,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 110] âš ï¸ Low results! Only 0 references & 25 citations.\n",
      "[Row 110] âœ… Found 0 references and 25 citations.\n",
      "[Row 115] ðŸ”Ž Searching for: DOI=, Title=Unsupervised Cross-lingual Representation Learning at Scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 67/34691 [2:15:10<20940:41:54, 2177.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 115] âœ… Found 47 references and 25 citations.\n",
      "[Row 121] ðŸ”Ž Searching for: DOI=, Title=Llama 3 Model Card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 69/34691 [2:15:13<11281:28:46, 1173.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 121] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 121] âœ… Found 2 references and 2 citations.\n",
      "[Row 122] ðŸ”Ž Searching for: DOI=, Title=ResNet strikes back: An improved training procedure in timm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 69/34691 [11:02:00<5536:15:45, 575.66s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[13], line 85\u001b[0m\n",
      "\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 3. Process them synchronously using citation_fetcher.py functions\u001b[39;00m\n",
      "\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting batch processing using citation_fetcher.py functions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m---> 85\u001b[0m processed_results \u001b[38;5;241m=\u001b[39m \u001b[43msync_process_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_rows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# 4. Store results back in the DataFrame\u001b[39;00m\n",
      "\u001b[1;32m     88\u001b[0m references_list, citations_list, success_flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mprocessed_results)\n",
      "\n",
      "Cell \u001b[0;32mIn[13], line 48\u001b[0m, in \u001b[0;36msync_process_queries\u001b[0;34m(df, doi_col, title_col)\u001b[0m\n",
      "\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] ðŸ”Ž Searching for: DOI=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Title=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m---> 48\u001b[0m     info_dict \u001b[38;5;241m=\u001b[39m \u001b[43msearch_and_fetch_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m     50\u001b[0m         references \u001b[38;5;241m=\u001b[39m info_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "\n",
      "File \u001b[0;32m~/Repo/CitationLake/src/data_ingestion/citation_fetcher.py:198\u001b[0m, in \u001b[0;36msearch_and_fetch_info\u001b[0;34m(doi, title, api_priority)\u001b[0m\n",
      "\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# If no match by DOI, try searching by title (if provided)\u001b[39;00m\n",
      "\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m best_paper \u001b[38;5;129;01mand\u001b[39;00m title:\n",
      "\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# print(f\"[DEBUG] Searching by Title: {title} using {api_name}\")\u001b[39;00m\n",
      "\u001b[0;32m--> 198\u001b[0m     paper \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_paper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m paper:\n",
      "\u001b[1;32m    200\u001b[0m         best_paper \u001b[38;5;241m=\u001b[39m paper\n",
      "\n",
      "File \u001b[0;32m~/Repo/CitationLake/src/data_ingestion/citation_fetcher.py:91\u001b[0m, in \u001b[0;36mOpenAlexAPI.search_paper\u001b[0;34m(self, query)\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fuzzy search by DOI or title.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m     90\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?search=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 91\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[1;32m     93\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n",
      "\u001b[1;32m     64\u001b[0m \n",
      "\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n",
      "\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n",
      "\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n",
      "\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n",
      "\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n",
      "\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n",
      "\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n",
      "\u001b[1;32m    587\u001b[0m }\n",
      "\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n",
      "\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n",
      "\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n",
      "\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n",
      "\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n",
      "\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n",
      "\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n",
      "\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n",
      "\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n",
      "\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n",
      "\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n",
      "\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n",
      "\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n",
      "\u001b[1;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n",
      "\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Removes unnecessary BibTeX characters like {} and trims spaces.\"\"\"\n",
    "    if title:\n",
    "        return re.sub(r\"[{}]\", \"\", title).strip()\n",
    "    return title\n",
    "\n",
    "def sync_process_queries(df, doi_col=\"doi\", title_col=\"title\"):\n",
    "    \"\"\"\n",
    "    Synchronously processes each row with search_and_fetch_info().\n",
    "    Returns references, citations, and success flags for each row.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_success = 0\n",
    "    total_failed = 0\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Rows\"):\n",
    "        parsed_bibtex_entries = row[\"parsed_bibtex_tuple_list\"]\n",
    "        if not isinstance(parsed_bibtex_entries, list) or len(parsed_bibtex_entries) == 0:\n",
    "            print(f\"[Row {idx}] âŒ No valid BibTeX entries.\")\n",
    "            results.append((json.dumps([]), json.dumps([]), False))\n",
    "            total_failed += 1\n",
    "            continue\n",
    "        references, citations = [], []\n",
    "        success = False\n",
    "        for parsed_data in parsed_bibtex_entries:\n",
    "            if not isinstance(parsed_data, dict):\n",
    "                continue\n",
    "            doi = parsed_data.get(\"doi\")\n",
    "            title = parsed_data.get(\"title\")\n",
    "            # âœ… Fix: Clean title and lowercase DOI\n",
    "            title = clean_title(title)\n",
    "            if doi:\n",
    "                doi = doi.lower().strip()\n",
    "            print(f\"[Row {idx}] ðŸ”Ž Searching for: DOI={doi}, Title={title}\")\n",
    "            try:\n",
    "                info_dict = search_and_fetch_info(doi=doi, title=title)\n",
    "                if info_dict is not None:\n",
    "                    references = info_dict.get(\"references\", [])\n",
    "                    citations = info_dict.get(\"citations\", [])\n",
    "                    # âœ… Debugging: Print when few references/citations are found\n",
    "                    if len(references) < 3 or len(citations) < 3:\n",
    "                        print(f\"[Row {idx}] âš ï¸ Low results! Only {len(references)} references & {len(citations)} citations.\")\n",
    "                    print(f\"[Row {idx}] âœ… Found {len(references)} references and {len(citations)} citations.\")\n",
    "                    success = True\n",
    "                    total_success += 1\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"[Row {idx}] âš ï¸ No results found.\")\n",
    "                    total_failed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[Row {idx}] âŒ Error: {e}\")\n",
    "                total_failed += 1\n",
    "        results.append((json.dumps(references), json.dumps(citations), success))\n",
    "    print(f\"\\nâœ… Total Success: {total_success}, âŒ Total Failed: {total_failed}\")\n",
    "    return results\n",
    "# 1. Filter rows with non-empty parsed_bibtex_tuple_list\n",
    "valid_bibtex_indices = df_split_temp[\n",
    "    df_split_temp[\"parsed_bibtex_tuple_list\"].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "].index\n",
    "# 2. Select only those rows\n",
    "valid_rows = df_split_temp.loc[valid_bibtex_indices].copy()\n",
    "# 3. Process them synchronously using citation_fetcher.py functions\n",
    "print(\"Starting batch processing using citation_fetcher.py functions...\")\n",
    "processed_results = sync_process_queries(valid_rows)\n",
    "# 4. Store results back in the DataFrame\n",
    "references_list, citations_list, success_flags = zip(*processed_results)\n",
    "valid_rows[\"references_within_dataset\"] = references_list\n",
    "valid_rows[\"citations_within_dataset\"] = citations_list\n",
    "valid_rows[\"success_flag\"] = success_flags\n",
    "# 5. Update original DataFrame\n",
    "df_split_temp.loc[valid_rows.index, \"references_within_dataset\"] = valid_rows[\"references_within_dataset\"]\n",
    "df_split_temp.loc[valid_rows.index, \"citations_within_dataset\"] = valid_rows[\"citations_within_dataset\"]\n",
    "df_split_temp.loc[valid_rows.index, \"success_flag\"] = valid_rows[\"success_flag\"]\n",
    "# 6. Save results\n",
    "df_split_temp.to_csv(\"./data/scilake/annotated_groundtruth.csv\", index=False)\n",
    "valid_rows.to_csv(\"./data/scilake/debug_results.csv\", index=False)  # Save debug file\n",
    "print(\"\\nðŸ”¹ Annotated ground truth saved to: './data/scilake/annotated_groundtruth.csv'\")\n",
    "print(\"ðŸ”¹ Debug results saved to: './data/scilake/debug_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–        | 164604/1108759 [05:20<16:59, 925.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to convert markdown to CSV for cleaned_markdown_csvs/mireillfares_BERTIS_markdown_164440.csv: '|' expected after '\"'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 289359/1108759 [07:29<14:42, 928.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to convert markdown to CSV for cleaned_markdown_csvs/lmaccarini_setfit-ep-v1_markdown_289213.csv: '|' expected after '\"'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1108759/1108759 [14:12<00:00, 1300.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown tables converted and paths saved.\n",
      "                                   modelId  \\\n",
      "0  sentence-transformers/all-mpnet-base-v2   \n",
      "1                     nesaorg/benchmark_v0   \n",
      "2   sentence-transformers/all-MiniLM-L6-v2   \n",
      "3            google-bert/bert-base-uncased   \n",
      "4             FacebookAI/xlm-roberta-large   \n",
      "\n",
      "                            extracted_markdown_table  \\\n",
      "0  | Dataset                                     ...   \n",
      "1                                               None   \n",
      "2  | Dataset                                     ...   \n",
      "3  | Model | #params | Language |\\n|-------------...   \n",
      "4                                               None   \n",
      "\n",
      "                                            csv_path  \n",
      "0  cleaned_markdown_csvs/sentence-transformers_al...  \n",
      "1                                               None  \n",
      "2  cleaned_markdown_csvs/sentence-transformers_al...  \n",
      "3  cleaned_markdown_csvs/google-bert_bert-base-un...  \n",
      "4                                               None  \n"
     ]
    }
   ],
   "source": [
    "### extract markdown and save to local files\n",
    "import os, re\n",
    "from tqdm import tqdm\n",
    "# Initialize tqdm with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Create output folder\n",
    "output_folder = \"cleaned_markdown_csvs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "# Define function to create file name with modelId and markdown index\n",
    "def generate_csv_path(model_id, index, folder):\n",
    "    \"\"\"Generate a unique file path using modelId and index.\"\"\"\n",
    "    sanitized_model_id = re.sub(r\"[^\\w\\-]\", \"_\", str(model_id) if model_id else \"unknown_model\")\n",
    "    return os.path.join(folder, f\"{sanitized_model_id}_markdown_{index}.csv\")\n",
    "# Apply the MarkdownHandler with a tqdm progress bar\n",
    "df_split_temp[\"csv_path\"] = df_split_temp.progress_apply(\n",
    "    lambda row: MarkdownHandler.markdown_to_csv(\n",
    "        row[\"extracted_markdown_table\"],\n",
    "        generate_csv_path(row[\"modelId\"], row.name, output_folder)\n",
    "    ) if pd.notnull(row[\"extracted_markdown_table\"]) else None,\n",
    "    axis=1\n",
    ")\n",
    "# Show the result\n",
    "print(\"Markdown tables converted and paths saved.\")\n",
    "print(df_split_temp[[\"modelId\", \"extracted_markdown_table\", \"csv_path\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n# distilbert-base-multilingual-cased-finetuned\\n\\nThis model is a fine-tuned version of [distilbert-base-multilingual-cased](https://huggingface.co/distilbert-base-multilingual-cased) on the Emotone Arabic dataset, which includes tweets labeled for various emotions: none, anger, joy, sadness, love, sympathy, surprise, and fear. It achieves the following results on the evaluation set:\\n- Loss: 1.3099\\n- Accuracy: 0.6632\\n- F1: 0.6647\\n\\n## Model description\\n\\nThis model is designed for emotion recognition in Arabic text. It can classify tweets into one of the eight emotional categories.\\n\\n## Intended uses & limitations\\n\\nThis model is intended for applications in sentiment analysis and emotion detection in Arabic tweets. It may not perform well on texts outside the domain of social media or on languages other than Arabic.\\n\\n## Training and evaluation data\\n\\nThe model was fine-tuned on the Emotone Arabic dataset, which consists of tweets labeled with the following emotions:\\n- none\\n- anger\\n- joy\\n- sadness\\n- love\\n- sympathy\\n- surprise\\n- fear\\n\\n### Label Mapping\\n\\n| Label Name | Numeric Label |\\n|------------|---------------|\\n| none       | 0             |\\n| anger      | 1             |\\n| joy        | 2             |\\n| sadness    | 3             |\\n| love       | 4             |\\n| sympathy   | 5             |\\n| surprise   | 6             |\\n| fear       | 7             |\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 2e-05\\n- train_batch_size: 32\\n- eval_batch_size: 32\\n- seed: 42\\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\\n- lr_scheduler_type: linear\\n- num_epochs: 10\\n\\n### Training results\\n\\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\\n| 1.0026        | 1.0   | 252  | 1.0417          | 0.6408   | 0.6321 |\\n| 0.8422        | 2.0   | 504  | 1.0355          | 0.6508   | 0.6425 |\\n| 0.7114        | 3.0   | 756  | 1.0611          | 0.6364   | 0.6342 |\\n| 0.5709        | 4.0   | 1008 | 1.0672          | 0.6692   | 0.6665 |\\n| 0.459        | 5.0   | 1260 | 1.1167          | 0.6731   | 0.6693 |\\n| 0.3694        | 6.0   | 1512 | 1.1709          | 0.6637   | 0.6672 |\\n| 0.2975        | 7.0   | 1764 | 1.2094          | 0.6716   | 0.6699 |\\n| 0.2402        | 8.0   | 2016 | 1.2777          | 0.6642   | 0.6633 |\\n| 0.209        | 9.0   | 2268 | 1.2997          | 0.6692   | 0.6685 |\\n| 0.1792        | 10.0  | 2520 | 1.3099          | 0.6632   | 0.6647 |\\n\\n### Example Outputs\\n\\nHere are some example inputs and their corresponding model predictions:\\n\\n| Input Tweet                                      | Predicted Emotion | Numeric Label |\\n|--------------------------------------------------|-------------------|---------------|\\n| \"Ø£Ù†Ø§ Ø³Ø¹ÙŠØ¯ Ø¬Ø¯Ù‹Ø§ Ø§Ù„ÙŠÙˆÙ…!\"                          | joy               | 2             |\\n| \"Ù‡Ø°Ø§ Ø£Ù…Ø± Ù…Ø­Ø¨Ø· Ø­Ù‚Ù‹Ø§.\"                             | sadness           | 3             |\\n| \"Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ ØªØ­Ù…Ù„ Ù‡Ø°Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø¢Ù†.\"                 | anger             | 1             |\\n| \"Ø£Ø­Ø¨ ÙƒÙ„ Ù…Ù† ÙŠØ¯Ø¹Ù…Ù†ÙŠ.\"                             | love              | 4             |\\n\\n### Framework versions\\n\\n- Transformers 4.44.2\\n- Pytorch 2.4.1+cu121\\n- Datasets 3.0.0\\n- Tokenizers 0.19.1']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# check sample\n",
    "print(df_split_temp[df_split_temp['modelId'].str.contains('0marr/distilbert-base-multilingual-cased-finetuned', na=False)]['card_readme'].values)\n",
    "print('-'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local \n",
    "df_split_temp.to_parquet(f\"data/{data_type}_step4_markdown_gated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "import pandas as pd\n",
    "data_type = 'modelcard'\n",
    "df_split_temp = pd.read_parquet(f\"data/{data_type}_step4_markdown_gated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_statistics\u001b[39m(df):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# pass\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m num_tables, num_cols, avg_rows\n\u001b[0;32m----> 5\u001b[0m num_tables, num_cols, avg_rows \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_split_temp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m new_row \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBenchmark\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSciLake\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Tables\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_tables,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize (GB)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# extracted from starmie paper: https://arxiv.org/pdf/2210.01922\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36mgenerate_statistics\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_statistics\u001b[39m(df):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# pass\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnum_tables\u001b[49m, num_cols, avg_rows\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_tables' is not defined"
     ]
    }
   ],
   "source": [
    "# statistics table\n",
    "def generate_statistics(df):\n",
    "    # pass\n",
    "    return 7173, num_cols, avg_rows\n",
    "num_tables, num_cols, avg_rows = generate_statistics(df_split_temp)\n",
    "new_row = {\n",
    "    \"Benchmark\": \"SciLake\",\n",
    "    \"# Tables\": num_tables,\n",
    "    \"# Cols\": num_cols,\n",
    "    \"Avg # Rows\": avg_rows,\n",
    "    \"Size (GB)\": \"nan\"\n",
    "}\n",
    "# extracted from starmie paper: https://arxiv.org/pdf/2210.01922\n",
    "benchmark_data = {\n",
    "    \"Benchmark\": [\"SANTOS Small\", \"TUS Small\", \"TUS Large\", \"SANTOS Large\", \"WDC\"],\n",
    "    \"# Tables\": [550, 1530, 5043, 11090, 50000000],\n",
    "    \"# Cols\": [6322, 14810, 54923, 123477, 250000000],\n",
    "    \"Avg # Rows\": [6921, 4466, 1915, 7675, 14],\n",
    "    \"Size (GB)\": [0.45, 1, 1.5, 11, 500]\n",
    "}\n",
    "benchmark_df = pd.DataFrame(benchmark_data)\n",
    "benchmark_df = pd.concat([benchmark_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "benchmark_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def inspect_column_values(column):\n",
    "    \"\"\"Inspect each value in the column and categorize its format.\"\"\"\n",
    "    for i, value in column.items():\n",
    "        if value is None:\n",
    "            print(f\"Row {i}: None value detected\")\n",
    "        elif isinstance(value, list):\n",
    "            print(f\"Row {i}: List with {len(value)} entries\")\n",
    "        elif isinstance(value, str):\n",
    "            print(f\"Row {i}: Single string detected\")\n",
    "        else:\n",
    "            print(f\"Row {i}: Unexpected type {type(value)}\")\n",
    "import numpy as np\n",
    "\n",
    "def inspect_and_convert(value):\n",
    "    \"\"\"Inspect and convert numpy arrays to lists or strings.\"\"\"\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"Array detected: {value}\")\n",
    "        return value.tolist()  # Convert to Python list\n",
    "    return value\n",
    "# Apply the function and inspect the first few rows\n",
    "df_split_temp[\"extracted_bibtex\"] = df_split_temp[\"extracted_bibtex\"].apply(inspect_and_convert)\n",
    "# Recheck the types\n",
    "#inspect_column_values(df_split_temp[\"extracted_bibtex\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_ingestion.citation_fetcher import AcademicAPIFactory\n",
    "from src.data_ingestion.readme_parser import BibTeXExtractor, LinkExtractor, MarkdownHandler, ExtractionFactory\n",
    "from src.data_ingestion.bibtex_parser import BibTeXFactory, ensure_string\n",
    "from src.data_ingestion.citation_fetcher import search_and_fetch_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: July\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: ContactDoctor\n",
      "Final parsing attempt after fix failed: undefined string in line 1: March\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: January\n",
      "Final parsing attempt after fix failed: undefined string in line 1: March\n",
      "Final parsing attempt after fix failed: undefined string in line 1: March\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Augest\n",
      "Final parsing attempt after fix failed: undefined string in line 1: April\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: January\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: ContactDoctor\n",
      "Final parsing attempt after fix failed: undefined string in line 1: july\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type dataset not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: july\n",
      "Final parsing attempt after fix failed: undefined string in line 1: February\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: july\n",
      "Final parsing attempt after fix failed: undefined string in line 1: ContactDoctor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: July\n",
      "Final parsing attempt after fix failed: undefined string in line 1: February\n",
      "Final parsing attempt after fix failed: undefined string in line 1: July\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: july\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: july\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: February\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: july\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: July\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: July\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: july\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: july\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: December\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: march\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: june\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: june\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: June\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: july\n",
      "Final parsing attempt after fix failed: undefined string in line 1: june\n",
      "Final parsing attempt after fix failed: undefined string in line 1: june\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: june\n",
      "Final parsing attempt after fix failed: undefined string in line 1: june\n",
      "Final parsing attempt after fix failed: undefined string in line 1: june\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: june\n",
      "Final parsing attempt after fix failed: undefined string in line 1: june\n",
      "Final parsing attempt after fix failed: undefined string in line 1: june\n",
      "Final parsing attempt after fix failed: undefined string in line 1: june\n",
      "Final parsing attempt after fix failed: undefined string in line 1: june\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: July\n",
      "Final parsing attempt after fix failed: undefined string in line 1: December\n",
      "Final parsing attempt after fix failed: undefined string in line 1: December\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: December\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: july\n",
      "Final parsing attempt after fix failed: undefined string in line 1: july\n",
      "Final parsing attempt after fix failed: undefined string in line 1: October\n",
      "Final parsing attempt after fix failed: undefined string in line 1: July\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: july\n",
      "Final parsing attempt after fix failed: undefined string in line 1: March\n",
      "Final parsing attempt after fix failed: undefined string in line 1: March\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: December\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: Collaiborator\n",
      "Final parsing attempt after fix failed: undefined string in line 1: july\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: july\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type online not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: June\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type dataset not standard. Not considered.\n",
      "Entry type dataset not standard. Not considered.\n",
      "Entry type dataset not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n",
      "Final parsing attempt after fix failed: undefined string in line 1: June\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type dataset not standard. Not considered.\n",
      "Entry type dataset not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 34691 BibTeX tuples.\n",
      "Failed parses: 823 (2.37% failure rate)\n",
      "\n",
      "\n",
      "Processed 34691 BibTeX tuples.\n",
      "Failed parses: 823 (2.37% failure rate)\n",
      "\n",
      "\n",
      "--- Row 117 ---\n",
      "Model ID: cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\n",
      "Extracted BibTeX: ['@inproceedings{camacho-collados-etal-2022-tweetnlp, title = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\", author = \"Camacho-collados, Jose  and Rezaee, Kiamehr  and Riahi, Talayeh  and Ushio, Asahi  and Loureiro, Daniel  and Antypas, Dimosthenis  and Boisson, Joanne  and Espinosa Anke, Luis  and Liu, Fangyu  and Mart{\\\\\\'\\\\i}nez C{\\\\\\'a}mara, Eugenio\" and others, booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\", month = dec, year = \"2022\", address = \"Abu Dhabi, UAE\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2022.emnlp-demos.5\", pages = \"38--49\" }']\n",
      "Extracted BibTeX Tuple: ['@inproceedings{camacho-collados-etal-2022-tweetnlp, title = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\", author = \"Camacho-collados, Jose  and Rezaee, Kiamehr  and Riahi, Talayeh  and Ushio, Asahi  and Loureiro, Daniel  and Antypas, Dimosthenis  and Boisson, Joanne  and Espinosa Anke, Luis  and Liu, Fangyu  and Mart{\\\\\\'\\\\i}nez C{\\\\\\'a}mara, Eugenio\" and others, booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\", month = dec, year = \"2022\", address = \"Abu Dhabi, UAE\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2022.emnlp-demos.5\", pages = \"38--49\" }']\n",
      "Parsed BibTeX Tuple List: [None]\n",
      "Successful Parse Count: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 332 ---\n",
      "Model ID: avsolatorio/GIST-Embedding-v0\n",
      "Extracted BibTeX: ['@article{solatorio2024gistembed, title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}, author={Aivin V. Solatorio}, journal={arXiv preprint arXiv:2402.16829}, year={2024}, URL={https://arxiv.org/abs/2402.16829} eprint={2402.16829}, archivePrefix={arXiv}, primaryClass={cs.LG} }']\n",
      "Extracted BibTeX Tuple: ['@article{solatorio2024gistembed, title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}, author={Aivin V. Solatorio}, journal={arXiv preprint arXiv:2402.16829}, year={2024}, URL={https://arxiv.org/abs/2402.16829} eprint={2402.16829}, archivePrefix={arXiv}, primaryClass={cs.LG} }']\n",
      "Parsed BibTeX Tuple List: [None]\n",
      "Successful Parse Count: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 469 ---\n",
      "Model ID: avsolatorio/GIST-all-MiniLM-L6-v2\n",
      "Extracted BibTeX: ['@article{solatorio2024gistembed, title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}, author={Aivin V. Solatorio}, journal={arXiv preprint arXiv:2402.16829}, year={2024}, URL={https://arxiv.org/abs/2402.16829} eprint={2402.16829}, archivePrefix={arXiv}, primaryClass={cs.LG} }']\n",
      "Extracted BibTeX Tuple: ['@article{solatorio2024gistembed, title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}, author={Aivin V. Solatorio}, journal={arXiv preprint arXiv:2402.16829}, year={2024}, URL={https://arxiv.org/abs/2402.16829} eprint={2402.16829}, archivePrefix={arXiv}, primaryClass={cs.LG} }']\n",
      "Parsed BibTeX Tuple List: [None]\n",
      "Successful Parse Count: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 477 ---\n",
      "Model ID: avsolatorio/GIST-small-Embedding-v0\n",
      "Extracted BibTeX: ['@article{solatorio2024gistembed, title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}, author={Aivin V. Solatorio}, journal={arXiv preprint arXiv:2402.16829}, year={2024}, URL={https://arxiv.org/abs/2402.16829} eprint={2402.16829}, archivePrefix={arXiv}, primaryClass={cs.LG} }']\n",
      "Extracted BibTeX Tuple: ['@article{solatorio2024gistembed, title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}, author={Aivin V. Solatorio}, journal={arXiv preprint arXiv:2402.16829}, year={2024}, URL={https://arxiv.org/abs/2402.16829} eprint={2402.16829}, archivePrefix={arXiv}, primaryClass={cs.LG} }']\n",
      "Parsed BibTeX Tuple List: [None]\n",
      "Successful Parse Count: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 519 ---\n",
      "Model ID: avsolatorio/GIST-large-Embedding-v0\n",
      "Extracted BibTeX: ['@article{solatorio2024gistembed, title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}, author={Aivin V. Solatorio}, journal={arXiv preprint arXiv:2402.16829}, year={2024}, URL={https://arxiv.org/abs/2402.16829} eprint={2402.16829}, archivePrefix={arXiv}, primaryClass={cs.LG} }']\n",
      "Extracted BibTeX Tuple: ['@article{solatorio2024gistembed, title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}, author={Aivin V. Solatorio}, journal={arXiv preprint arXiv:2402.16829}, year={2024}, URL={https://arxiv.org/abs/2402.16829} eprint={2402.16829}, archivePrefix={arXiv}, primaryClass={cs.LG} }']\n",
      "Parsed BibTeX Tuple List: [None]\n",
      "Successful Parse Count: 0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "factory = BibTeXFactory()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Function to process a list of BibTeX entries from extracted_bibtex_tuple\n",
    "def process_bibtex_tuple(entry):\n",
    "    \"\"\"Process all BibTeX items in a tuple and return a list of parsed results.\"\"\"\n",
    "    parsed_results = []\n",
    "    success_count = 0\n",
    "    if isinstance(entry, tuple):\n",
    "        entry = list(entry)\n",
    "    if isinstance(entry, np.ndarray):\n",
    "        entry = list(entry)\n",
    "    if isinstance(entry, list):\n",
    "        for single_entry in entry:\n",
    "            single_entry = ensure_string(single_entry)\n",
    "            if single_entry:\n",
    "                parsed_entry, flag = BibTeXFactory.parse_bibtex(single_entry)\n",
    "                parsed_results.append(parsed_entry)\n",
    "                if flag:\n",
    "                    success_count += 1\n",
    "    elif isinstance(entry, str):\n",
    "        single_entry = ensure_string(entry)\n",
    "        parsed_entry, flag = BibTeXFactory.parse_bibtex(single_entry)\n",
    "        parsed_results.append(parsed_entry)\n",
    "        if flag:\n",
    "            success_count += 1\n",
    "    return parsed_results, success_count\n",
    "\n",
    "results = df_split_temp[\"extracted_bibtex_tuple\"].apply(process_bibtex_tuple)\n",
    "df_split_temp[\"parsed_bibtex_tuple_list\"] = results.apply(lambda x: x[0])\n",
    "df_split_temp[\"successful_parse_count\"] = results.apply(lambda x: x[1])\n",
    "df_failed_parsing = df_split_temp[\n",
    "    (df_split_temp[\"extracted_bibtex_tuple\"].notnull()) & \n",
    "    (df_split_temp[\"extracted_bibtex_tuple\"].apply(lambda x: isinstance(x, (list, tuple, np.ndarray)) and len(x) > 0)) & \n",
    "    (df_split_temp[\"parsed_bibtex_tuple_list\"].apply(lambda x: x is None or len([i for i in x if i]) == 0))\n",
    "]\n",
    "total_items = len(df_split_temp[(df_split_temp[\"extracted_bibtex_tuple\"].notnull()) & \n",
    "    (df_split_temp[\"extracted_bibtex_tuple\"].apply(lambda x: isinstance(x, (list, tuple, np.ndarray)) and len(x) > 0))])\n",
    "total_failed = len(df_failed_parsing)\n",
    "print(f\"\\nProcessed {total_items} BibTeX tuples.\")\n",
    "print(f\"Failed parses: {total_failed} ({(total_failed / total_items) * 100:.2f}% failure rate)\\n\")\n",
    "\n",
    "print(f\"\\nProcessed {total_items} BibTeX tuples.\")\n",
    "print(f\"Failed parses: {total_failed} ({(total_failed / total_items) * 100:.2f}% failure rate)\\n\")\n",
    "\n",
    "# double check the failed parsing\n",
    "k = 5\n",
    "df_failed_sample = df_failed_parsing[['modelId', 'extracted_bibtex', 'extracted_bibtex_tuple',  'parsed_bibtex_tuple_list', 'successful_parse_count']].head(k)\n",
    "for idx, row in df_failed_sample.iterrows():\n",
    "    print(f\"\\n--- Row {idx} ---\")\n",
    "    print(f\"Model ID: {row['modelId']}\")\n",
    "    print(f\"Extracted BibTeX: {row['extracted_bibtex']}\")\n",
    "    print(f\"Extracted BibTeX Tuple: {row['extracted_bibtex_tuple']}\")\n",
    "    print(f\"Parsed BibTeX Tuple List: {row['parsed_bibtex_tuple_list']}\")\n",
    "    print(f\"Successful Parse Count: {row['successful_parse_count']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_split_temp.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import json\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# Enable asyncio inside Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize API (Ensure these functions are correctly implemented)\n",
    "api_name = 'semantic_scholar'  # Change to 'semantic_scholar' if needed\n",
    "api = AcademicAPIFactory.get_api(api_name)\n",
    "\n",
    "# Fix: Ensure API methods can handle async requests\n",
    "async def get_references_safe(api, identifier):\n",
    "    try:\n",
    "        return await api.get_references(identifier)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching references for {identifier}: {e}\")\n",
    "        return []\n",
    "\n",
    "async def get_citations_safe(api, identifier):\n",
    "    try:\n",
    "        return await api.get_citations(identifier)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching citations for {identifier}: {e}\")\n",
    "        return []\n",
    "\n",
    "async def query_local_or_api(parsed_bibtex_entries, row_index, session):\n",
    "    \"\"\"\n",
    "    Query the local dataset first. If not found, fallback to the API.\n",
    "    \"\"\"\n",
    "    references, citations = [], []\n",
    "    # Ensure parsed_bibtex_entries is valid\n",
    "    if not isinstance(parsed_bibtex_entries, list) or not parsed_bibtex_entries:\n",
    "        print(f\"[{row_index}] Skipping: No valid parsed BibTeX.\")\n",
    "        return \"[]\", \"[]\", False  # Mark as failed\n",
    "    success = False\n",
    "    for parsed_data in parsed_bibtex_entries:\n",
    "        if not isinstance(parsed_data, dict):  # Fix NoneType error\n",
    "            print(f\"[{row_index}] Skipping invalid parsed data: {parsed_data}\")\n",
    "            continue\n",
    "        doi = parsed_data.get(\"doi\")\n",
    "        title = parsed_data.get(\"title\")\n",
    "        url = parsed_data.get(\"url\")\n",
    "        try:\n",
    "            # Step 1: Query API with DOI if available\n",
    "            if doi:\n",
    "                print(f\"[{row_index}] Querying API with DOI: {doi}\")\n",
    "                references = await get_references_safe(api, doi)\n",
    "                citations = await get_citations_safe(api, doi)\n",
    "                success = True\n",
    "                break  # Stop if we successfully get data\n",
    "            # Step 2: Fallback to Title if DOI is not available\n",
    "            elif title:\n",
    "                print(f\"[{row_index}] Querying API with Title: {title}\")\n",
    "                references = await get_references_safe(api, title)\n",
    "                citations = await get_citations_safe(api, title)\n",
    "                success = True\n",
    "                break\n",
    "            # Step 3: Fallback to URL if available\n",
    "            elif url:\n",
    "                print(f\"[{row_index}] Querying API with URL: {url}\")\n",
    "                references = await get_references_safe(api, url)\n",
    "                citations = await get_citations_safe(api, url)\n",
    "                success = True\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"[{row_index}] Error querying API: {e}\")\n",
    "    print(f\"[{row_index}] Found {len(references)} references and {len(citations)} citations.\")\n",
    "    return json.dumps(references), json.dumps(citations), success\n",
    "\n",
    "async def process_queries(valid_bibtex_entries):\n",
    "    \"\"\"\n",
    "    Handles all API requests asynchronously using aiohttp.\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [query_local_or_api(bibtex_entries, i, session) for i, bibtex_entries in enumerate(valid_bibtex_entries)]\n",
    "        # Wrap tasks with tqdm for progress tracking\n",
    "        results = await asyncio.gather(*tqdm(tasks, desc=\"Processing Queries\", leave=True))\n",
    "    return results\n",
    "\n",
    "valid_bibtex_indices = df_split_temp[\n",
    "    df_split_temp[\"parsed_bibtex_tuple_list\"].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "].index\n",
    "valid_bibtex_entries = df_split_temp.loc[valid_bibtex_indices, \"parsed_bibtex_tuple_list\"]\n",
    "print(\"Starting asynchronous API queries with progress bar...\")\n",
    "results = await process_queries(valid_bibtex_entries)\n",
    "references_within_dataset, citations_within_dataset, success_flags = zip(*results)\n",
    "df_split_temp.loc[valid_bibtex_indices, \"references_within_dataset\"] = references_within_dataset\n",
    "df_split_temp.loc[valid_bibtex_indices, \"citations_within_dataset\"] = citations_within_dataset\n",
    "df_split_temp.loc[valid_bibtex_indices, \"success_flag\"] = success_flags\n",
    "df_split_temp.to_csv(\"./data/scilake/annotated_groundtruth.csv\", index=False)\n",
    "print(\"Annotated ground truth saved to './data/scilake/annotated_groundtruth.csv'\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing using citation_fetcher.py functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 0/34691 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 3] ðŸ”Ž Searching for: DOI=, Title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 1/34691 [00:02<19:59:16,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 3] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 3] âœ… Found 2 references and 3 citations.\n",
      "[Row 4] ðŸ”Ž Searching for: DOI=, Title=Unsupervised Cross-lingual Representation Learning at Scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 2/34691 [00:03<16:21:51,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 4] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 4] âœ… Found 2 references and 3 citations.\n",
      "[Row 5] ðŸ”Ž Searching for: DOI=10.48550/arxiv.2212.04356, Title=Robust Speech Recognition via Large-Scale Weak Supervision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 3/34691 [00:04<11:59:38,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 5] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 5] âœ… Found 2 references and 2 citations.\n",
      "[Row 9] ðŸ”Ž Searching for: DOI=, Title=Fine-tuned XLSR-53 large model for speech recognition in English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 4/34691 [00:05<12:03:37,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 9] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 9] âœ… Found 2 references and 2 citations.\n",
      "[Row 11] ðŸ”Ž Searching for: DOI=, Title=RoBERTa: A Robustly Optimized BERT Pretraining Approach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 5/34691 [00:35<113:07:17, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 11] âœ… Found 45 references and 25 citations.\n",
      "[Row 13] ðŸ”Ž Searching for: DOI=, Title=Qwen2.5: A Party of Foundation Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 6/34691 [00:37<80:29:50,  8.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 13] âš ï¸ Low results! Only 3 references & 2 citations.\n",
      "[Row 13] âœ… Found 3 references and 2 citations.\n",
      "[Row 14] ðŸ”Ž Searching for: DOI=, Title=OPT: Open Pre-trained Transformer Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 7/34691 [00:39<60:38:12,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 14] âœ… Found 3 references and 3 citations.\n",
      "[Row 15] ðŸ”Ž Searching for: DOI=, Title=Visual Transformers: Token-based Image Representation and Processing for Computer Vision\n",
      "[Row 15] âŒ Error: object of type 'NoneType' has no len()\n",
      "[Row 15] ðŸ”Ž Searching for: DOI=, Title=Imagenet: A large-scale hierarchical image database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 8/34691 [00:42<51:27:57,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 15] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 15] âœ… Found 2 references and 3 citations.\n",
      "[Row 16] ðŸ”Ž Searching for: DOI=, Title=Language Models are Unsupervised Multitask Learners\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 9/34691 [00:43<38:13:37,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 16] âŒ Error: object of type 'NoneType' has no len()\n",
      "[Row 17] ðŸ”Ž Searching for: DOI=, Title=Crosslingual generalization through multitask finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 10/34691 [00:44<29:18:24,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 17] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 17] âœ… Found 2 references and 3 citations.\n",
      "[Row 19] ðŸ”Ž Searching for: DOI=, Title=ResNet strikes back: An improved training procedure in timm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 11/34691 [00:46<24:16:01,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 19] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 19] âœ… Found 2 references and 3 citations.\n",
      "[Row 20] ðŸ”Ž Searching for: DOI=, Title=Wespeaker: A research and production oriented speaker embedding learning toolkit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 12/34691 [00:47<20:26:03,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 20] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 20] âœ… Found 2 references and 3 citations.\n",
      "[Row 21] ðŸ”Ž Searching for: DOI=, Title=Deep residual learning for image recognition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 13/34691 [00:49<19:40:18,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 21] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 21] âœ… Found 2 references and 3 citations.\n",
      "[Row 23] ðŸ”Ž Searching for: DOI=, Title=Powerset multi-class cross entropy loss for neural speaker diarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 14/34691 [00:50<17:00:48,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 23] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 23] âœ… Found 2 references and 2 citations.\n",
      "[Row 24] ðŸ”Ž Searching for: DOI=, Title=DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 15/34691 [00:51<14:20:17,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 24] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 24] âœ… Found 2 references and 3 citations.\n",
      "[Row 25] ðŸ”Ž Searching for: DOI=, Title=Unsupervised Cross-lingual Representation Learning at Scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 16/34691 [00:52<13:47:50,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 25] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 25] âœ… Found 2 references and 3 citations.\n",
      "[Row 26] ðŸ”Ž Searching for: DOI=10.48550/arxiv.2212.04356, Title=Robust Speech Recognition via Large-Scale Weak Supervision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 17/34691 [00:53<12:31:19,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 26] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 26] âœ… Found 2 references and 2 citations.\n",
      "[Row 27] ðŸ”Ž Searching for: DOI=10.34740/kaggle/m/3301, Title=Gemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 18/34691 [00:54<11:48:35,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 27] âš ï¸ Low results! Only 3 references & 2 citations.\n",
      "[Row 27] âœ… Found 3 references and 2 citations.\n",
      "[Row 31] ðŸ”Ž Searching for: DOI=, Title=SimCSE: Simple Contrastive Learning of Sentence Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 19/34691 [01:35<125:39:14, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 31] âœ… Found 70 references and 25 citations.\n",
      "[Row 32] ðŸ”Ž Searching for: DOI=, Title=Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 20/34691 [02:07<182:04:11, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 32] âœ… Found 45 references and 25 citations.\n",
      "[Row 33] ðŸ”Ž Searching for: DOI=, Title=Powerset multi-class cross entropy loss for neural speaker diarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 21/34691 [02:08<130:19:23, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 33] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 33] âœ… Found 2 references and 2 citations.\n",
      "[Row 34] ðŸ”Ž Searching for: DOI=, Title=Fine-tuned XLSR-53 large model for speech recognition in Chinese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 22/34691 [02:09<94:49:13,  9.85s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 34] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 34] âœ… Found 2 references and 2 citations.\n",
      "[Row 35] ðŸ”Ž Searching for: DOI=, Title=RoBERTa: A Robustly Optimized BERT Pretraining Approach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 23/34691 [02:12<72:38:13,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 35] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 35] âœ… Found 2 references and 3 citations.\n",
      "[Row 36] ðŸ”Ž Searching for: DOI=, Title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 24/34691 [02:46<150:50:26, 15.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 36] âœ… Found 55 references and 25 citations.\n",
      "[Row 38] ðŸ”Ž Searching for: DOI=, Title=Chronos: Learning the Language of Time Series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 25/34691 [02:48<109:13:55, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 38] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 38] âœ… Found 2 references and 3 citations.\n",
      "[Row 40] ðŸ”Ž Searching for: DOI=, Title=Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 26/34691 [02:49<79:48:44,  8.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 40] âœ… Found 3 references and 3 citations.\n",
      "[Row 43] ðŸ”Ž Searching for: DOI=, Title=High-Resolution Image Synthesis With Latent Diffusion Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 27/34691 [03:42<210:36:14, 21.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 43] âœ… Found 94 references and 25 citations.\n",
      "[Row 47] ðŸ”Ž Searching for: DOI=, Title=ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 28/34691 [04:28<278:28:00, 28.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 47] âŒ Error: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "[Row 48] ðŸ”Ž Searching for: DOI=, Title=OPT: Open Pre-trained Transformer Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 29/34691 [04:41<234:05:28, 24.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 48] âš ï¸ Low results! Only 0 references & 25 citations.\n",
      "[Row 48] âœ… Found 0 references and 25 citations.\n",
      "[Row 51] ðŸ”Ž Searching for: DOI=, Title=Longformer: The Long-Document Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 30/34691 [04:43<167:48:53, 17.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 51] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 51] âœ… Found 2 references and 3 citations.\n",
      "[Row 52] ðŸ”Ž Searching for: DOI=, Title=C-Pack: Packaged Resources To Advance General Chinese Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 31/34691 [04:44<120:53:28, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 52] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 52] âœ… Found 2 references and 3 citations.\n",
      "[Row 56] ðŸ”Ž Searching for: DOI=, Title=Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 32/34691 [04:45<88:12:39,  9.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 56] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 56] âœ… Found 2 references and 2 citations.\n",
      "[Row 59] ðŸ”Ž Searching for: DOI=, Title=End-to-end speaker segmentation for overlap-aware resegmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 33/34691 [05:13<142:44:22, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 59] âœ… Found 25 references and 25 citations.\n",
      "[Row 60] ðŸ”Ž Searching for: DOI=, Title=DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 34/34691 [05:48<199:29:06, 20.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 60] âœ… Found 47 references and 25 citations.\n",
      "[Row 62] ðŸ”Ž Searching for: DOI=, Title=Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 35/34691 [05:50<146:14:24, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 62] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 62] âœ… Found 2 references and 3 citations.\n",
      "[Row 63] ðŸ”Ž Searching for: DOI=, Title=LAION-5B: An open large-scale dataset for training next generation image-text models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 36/34691 [05:51<106:52:05, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 63] âœ… Found 3 references and 3 citations.\n",
      "[Row 64] ðŸ”Ž Searching for: DOI=, Title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 37/34691 [05:53<78:57:12,  8.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 64] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 64] âœ… Found 2 references and 3 citations.\n",
      "[Row 65] ðŸ”Ž Searching for: DOI=, Title=DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n",
      "[Row 65] âŒ Error: object of type 'NoneType' has no len()\n",
      "[Row 65] ðŸ”Ž Searching for: DOI=, Title=DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 38/34691 [06:24<145:10:28, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 65] âœ… Found 47 references and 25 citations.\n",
      "[Row 66] ðŸ”Ž Searching for: DOI=10.18653/v1/2020.findings-emnlp.148, Title=TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 39/34691 [08:11<411:13:11, 42.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 66] âœ… Found 191 references and 25 citations.\n",
      "[Row 67] ðŸ”Ž Searching for: DOI=, Title=BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 40/34691 [08:43<381:02:26, 39.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 67] âœ… Found 44 references and 25 citations.\n",
      "[Row 68] ðŸ”Ž Searching for: DOI=10.48550/arxiv.2212.04356, Title=Robust Speech Recognition via Large-Scale Weak Supervision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 41/34691 [08:47<275:38:51, 28.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 68] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 68] âœ… Found 2 references and 3 citations.\n",
      "[Row 69] ðŸ”Ž Searching for: DOI=, Title=Fine-tuned XLSR-53 large model for speech recognition in Russian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 42/34691 [08:48<196:13:51, 20.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 69] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 69] âœ… Found 2 references and 2 citations.\n",
      "[Row 71] ðŸ”Ž Searching for: DOI=, Title=DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 43/34691 [09:08<195:00:24, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 71] âœ… Found 18 references and 25 citations.\n",
      "[Row 72] ðŸ”Ž Searching for: DOI=, Title=Visual Transformers: Token-based Image Representation and Processing for Computer Vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 44/34691 [09:41<231:50:20, 24.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 72] âœ… Found 49 references and 25 citations.\n",
      "[Row 73] ðŸ”Ž Searching for: DOI=, Title=Chronos: Learning the Language of Time Series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 45/34691 [09:42<165:45:44, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 73] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 73] âœ… Found 2 references and 3 citations.\n",
      "[Row 75] ðŸ”Ž Searching for: DOI=, Title=Fine-tuned XLSR-53 large model for speech recognition in Portuguese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 46/34691 [09:43<119:35:27, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 75] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 75] âœ… Found 2 references and 2 citations.\n",
      "[Row 76] ðŸ”Ž Searching for: DOI=, Title=CamemBERT: a Tasty French Language Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 47/34691 [09:45<89:23:22,  9.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 76] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 76] âœ… Found 2 references and 3 citations.\n",
      "[Row 79] ðŸ”Ž Searching for: DOI=10.18653/v1/2022.acl-demo.25, Title=TimeLMs: Diachronic Language Models from Twitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 48/34691 [09:47<68:10:46,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 79] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 79] âœ… Found 2 references and 3 citations.\n",
      "[Row 81] ðŸ”Ž Searching for: DOI=, Title=BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 49/34691 [09:48<51:17:07,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 81] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 81] âœ… Found 2 references and 3 citations.\n",
      "[Row 83] ðŸ”Ž Searching for: DOI=, Title=JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create State-of-the-Art Japanese Retrievers with Constrained Resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 50/34691 [09:49<38:45:34,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 83] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 83] âœ… Found 2 references and 2 citations.\n",
      "[Row 84] ðŸ”Ž Searching for: DOI=, Title=BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 51/34691 [10:19<113:57:54, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 84] âœ… Found 44 references and 25 citations.\n",
      "[Row 86] ðŸ”Ž Searching for: DOI=, Title=FNet: Mixing Tokens with Fourier Transforms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 52/34691 [11:18<250:42:12, 26.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 86] âœ… Found 95 references and 25 citations.\n",
      "[Row 87] ðŸ”Ž Searching for: DOI=, Title=BERTimbau: pretrained BERT models for Brazilian Portuguese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 53/34691 [11:20<179:24:18, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 87] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 87] âœ… Found 2 references and 3 citations.\n",
      "[Row 88] ðŸ”Ž Searching for: DOI=, Title=Open Source Strikes Bread - New Fluffy Embeddings Model\n",
      "[Row 88] âŒ Error: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "[Row 88] ðŸ”Ž Searching for: DOI=, Title=AnglE-optimized Text Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 54/34691 [12:31<330:27:07, 34.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 88] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 88] âœ… Found 2 references and 2 citations.\n",
      "[Row 89] ðŸ”Ž Searching for: DOI=, Title=C-Pack: Packaged Resources To Advance General Chinese Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 55/34691 [12:32<234:13:02, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 89] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 89] âœ… Found 2 references and 3 citations.\n",
      "[Row 93] ðŸ”Ž Searching for: DOI=, Title=SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 56/34691 [12:45<200:34:38, 20.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 93] âš ï¸ Low results! Only 0 references & 25 citations.\n",
      "[Row 93] âœ… Found 0 references and 25 citations.\n",
      "[Row 94] ðŸ”Ž Searching for: DOI=, Title=Sigmoid Loss for Language Image Pre-Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 57/34691 [12:46<144:16:24, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 94] âš ï¸ Low results! Only 2 references & 3 citations.\n",
      "[Row 94] âœ… Found 2 references and 3 citations.\n",
      "[Row 98] ðŸ”Ž Searching for: DOI=, Title=C-Pack: Packaged Resources To Advance General Chinese Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 58/34691 [13:00<142:21:33, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 98] âš ï¸ Low results! Only 0 references & 25 citations.\n",
      "[Row 98] âœ… Found 0 references and 25 citations.\n",
      "[Row 99] ðŸ”Ž Searching for: DOI=, Title=High-Resolution Image Synthesis With Latent Diffusion Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 59/34691 [13:02<104:58:07, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 99] âœ… Found 3 references and 3 citations.\n",
      "[Row 101] ðŸ”Ž Searching for: DOI=, Title=DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 60/34691 [13:47<203:32:58, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 101] âŒ Error: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "[Row 102] ðŸ”Ž Searching for: DOI=, Title=mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 61/34691 [13:49<148:00:15, 15.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 102] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 102] âœ… Found 2 references and 2 citations.\n",
      "[Row 103] ðŸ”Ž Searching for: DOI=, Title=Powerset multi-class cross entropy loss for neural speaker diarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 62/34691 [14:14<175:51:57, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 103] âœ… Found 23 references and 20 citations.\n",
      "[Row 105] ðŸ”Ž Searching for: DOI=, Title=Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 63/34691 [14:16<129:18:15, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 105] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 105] âœ… Found 2 references and 2 citations.\n",
      "[Row 107] ðŸ”Ž Searching for: DOI=10.48550/arxiv.2201.12086, Title=BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 64/34691 [14:17<93:54:39,  9.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 107] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 107] âœ… Found 2 references and 2 citations.\n",
      "[Row 109] ðŸ”Ž Searching for: DOI=, Title=Multilingual E5 Text Embeddings: A Technical Report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 65/34691 [14:22<78:31:12,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 109] âš ï¸ Low results! Only 0 references & 4 citations.\n",
      "[Row 109] âœ… Found 0 references and 4 citations.\n",
      "[Row 110] ðŸ”Ž Searching for: DOI=, Title=Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 66/34691 [14:34<89:32:17,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 110] âš ï¸ Low results! Only 0 references & 25 citations.\n",
      "[Row 110] âœ… Found 0 references and 25 citations.\n",
      "[Row 115] ðŸ”Ž Searching for: DOI=, Title=Unsupervised Cross-lingual Representation Learning at Scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 67/34691 [2:15:10<20940:41:54, 2177.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 115] âœ… Found 47 references and 25 citations.\n",
      "[Row 121] ðŸ”Ž Searching for: DOI=, Title=Llama 3 Model Card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 69/34691 [2:15:13<11281:28:46, 1173.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 121] âš ï¸ Low results! Only 2 references & 2 citations.\n",
      "[Row 121] âœ… Found 2 references and 2 citations.\n",
      "[Row 122] ðŸ”Ž Searching for: DOI=, Title=ResNet strikes back: An improved training procedure in timm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 69/34691 [11:02:00<5536:15:45, 575.66s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 3. Process them synchronously using citation_fetcher.py functions\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting batch processing using citation_fetcher.py functions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m processed_results \u001b[38;5;241m=\u001b[39m \u001b[43msync_process_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_rows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# 4. Store results back in the DataFrame\u001b[39;00m\n\u001b[1;32m     88\u001b[0m references_list, citations_list, success_flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mprocessed_results)\n",
      "Cell \u001b[0;32mIn[13], line 48\u001b[0m, in \u001b[0;36msync_process_queries\u001b[0;34m(df, doi_col, title_col)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] ðŸ”Ž Searching for: DOI=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Title=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     info_dict \u001b[38;5;241m=\u001b[39m \u001b[43msearch_and_fetch_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m         references \u001b[38;5;241m=\u001b[39m info_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[0;32m~/Repo/CitationLake/src/data_ingestion/citation_fetcher.py:198\u001b[0m, in \u001b[0;36msearch_and_fetch_info\u001b[0;34m(doi, title, api_priority)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# If no match by DOI, try searching by title (if provided)\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m best_paper \u001b[38;5;129;01mand\u001b[39;00m title:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# print(f\"[DEBUG] Searching by Title: {title} using {api_name}\")\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     paper \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_paper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m paper:\n\u001b[1;32m    200\u001b[0m         best_paper \u001b[38;5;241m=\u001b[39m paper\n",
      "File \u001b[0;32m~/Repo/CitationLake/src/data_ingestion/citation_fetcher.py:91\u001b[0m, in \u001b[0;36mOpenAlexAPI.search_paper\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fuzzy search by DOI or title.\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?search=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 91\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     93\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/modellake/lib/python3.9/ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Removes unnecessary BibTeX characters like {} and trims spaces.\"\"\"\n",
    "    if title:\n",
    "        return re.sub(r\"[{}]\", \"\", title).strip()\n",
    "    return title\n",
    "\n",
    "def sync_process_queries(df, doi_col=\"doi\", title_col=\"title\"):\n",
    "    \"\"\"\n",
    "    Synchronously processes each row with search_and_fetch_info().\n",
    "    Returns references, citations, and success flags for each row.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_success = 0\n",
    "    total_failed = 0\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Rows\"):\n",
    "        parsed_bibtex_entries = row[\"parsed_bibtex_tuple_list\"]\n",
    "        if not isinstance(parsed_bibtex_entries, list) or len(parsed_bibtex_entries) == 0:\n",
    "            print(f\"[Row {idx}] âŒ No valid BibTeX entries.\")\n",
    "            results.append((json.dumps([]), json.dumps([]), False))\n",
    "            total_failed += 1\n",
    "            continue\n",
    "        references, citations = [], []\n",
    "        success = False\n",
    "        for parsed_data in parsed_bibtex_entries:\n",
    "            if not isinstance(parsed_data, dict):\n",
    "                continue\n",
    "            doi = parsed_data.get(\"doi\")\n",
    "            title = parsed_data.get(\"title\")\n",
    "            # âœ… Fix: Clean title and lowercase DOI\n",
    "            title = clean_title(title)\n",
    "            if doi:\n",
    "                doi = doi.lower().strip()\n",
    "            print(f\"[Row {idx}] ðŸ”Ž Searching for: DOI={doi}, Title={title}\")\n",
    "            try:\n",
    "                info_dict = search_and_fetch_info(doi=doi, title=title)\n",
    "                if info_dict is not None:\n",
    "                    references = info_dict.get(\"references\", [])\n",
    "                    citations = info_dict.get(\"citations\", [])\n",
    "                    # âœ… Debugging: Print when few references/citations are found\n",
    "                    if len(references) < 3 or len(citations) < 3:\n",
    "                        print(f\"[Row {idx}] âš ï¸ Low results! Only {len(references)} references & {len(citations)} citations.\")\n",
    "                    print(f\"[Row {idx}] âœ… Found {len(references)} references and {len(citations)} citations.\")\n",
    "                    success = True\n",
    "                    total_success += 1\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"[Row {idx}] âš ï¸ No results found.\")\n",
    "                    total_failed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[Row {idx}] âŒ Error: {e}\")\n",
    "                total_failed += 1\n",
    "        results.append((json.dumps(references), json.dumps(citations), success))\n",
    "    print(f\"\\nâœ… Total Success: {total_success}, âŒ Total Failed: {total_failed}\")\n",
    "    return results\n",
    "# 1. Filter rows with non-empty parsed_bibtex_tuple_list\n",
    "valid_bibtex_indices = df_split_temp[\n",
    "    df_split_temp[\"parsed_bibtex_tuple_list\"].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "].index\n",
    "# 2. Select only those rows\n",
    "valid_rows = df_split_temp.loc[valid_bibtex_indices].copy()\n",
    "# 3. Process them synchronously using citation_fetcher.py functions\n",
    "print(\"Starting batch processing using citation_fetcher.py functions...\")\n",
    "processed_results = sync_process_queries(valid_rows)\n",
    "# 4. Store results back in the DataFrame\n",
    "references_list, citations_list, success_flags = zip(*processed_results)\n",
    "valid_rows[\"references_within_dataset\"] = references_list\n",
    "valid_rows[\"citations_within_dataset\"] = citations_list\n",
    "valid_rows[\"success_flag\"] = success_flags\n",
    "# 5. Update original DataFrame\n",
    "df_split_temp.loc[valid_rows.index, \"references_within_dataset\"] = valid_rows[\"references_within_dataset\"]\n",
    "df_split_temp.loc[valid_rows.index, \"citations_within_dataset\"] = valid_rows[\"citations_within_dataset\"]\n",
    "df_split_temp.loc[valid_rows.index, \"success_flag\"] = valid_rows[\"success_flag\"]\n",
    "# 6. Save results\n",
    "df_split_temp.to_csv(\"./data/scilake/annotated_groundtruth.csv\", index=False)\n",
    "valid_rows.to_csv(\"./data/scilake/debug_results.csv\", index=False)  # Save debug file\n",
    "print(\"\\nðŸ”¹ Annotated ground truth saved to: './data/scilake/annotated_groundtruth.csv'\")\n",
    "print(\"ðŸ”¹ Debug results saved to: './data/scilake/debug_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"modelcard\"\n",
    "df_split_temp.to_parquet(f\"data/{data_type}_step5_markdown_gated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce annotation for groundtruth dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (Ensure it's the latest updated file)\n",
    "df = pd.read_csv(\"./data/scilake/annotated_groundtruth.csv\")\n",
    "# Ensure all references and citations are loaded properly\n",
    "df[\"references_within_dataset\"] = df[\"references_within_dataset\"].apply(json.loads)\n",
    "df[\"citations_within_dataset\"] = df[\"citations_within_dataset\"].apply(json.loads)\n",
    "# Step 1: Create a mapping {title or doi: model_id} from the dataset\n",
    "id_mapping = {}\n",
    "for idx, row in df.iterrows():\n",
    "    unique_id = row.get(\"doi\") or row.get(\"openalex_id\") or row.get(\"semantic_scholar_id\") or row.get(\"title\")\n",
    "    if unique_id:\n",
    "        id_mapping[unique_id] = row.get(\"model_id\", unique_id)  # Use model_id if it exists, otherwise fallback\n",
    "# Step 2: Extract relationships using mapped IDs\n",
    "citation_relationships = []\n",
    "for idx, row in df.iterrows():\n",
    "    target_id = id_mapping.get(row.get(\"doi\") or row.get(\"openalex_id\") or row.get(\"semantic_scholar_id\") or row.get(\"title\"))\n",
    "    if not target_id:\n",
    "        continue  # Skip if there's no valid ID\n",
    "    # Cited papers (references)\n",
    "    for ref in row[\"references_within_dataset\"]:\n",
    "        ref_id = id_mapping.get(ref.get(\"doi\") or ref.get(\"openalex_id\") or ref.get(\"semantic_scholar_id\") or ref.get(\"title\"))\n",
    "        if ref_id:\n",
    "            citation_relationships.append({\n",
    "                \"target_model_id\": target_id,\n",
    "                \"cited_model_id\": ref_id,\n",
    "                \"citing_model_id\": None  # No citing paper here\n",
    "            })\n",
    "    # Citing papers (citations)\n",
    "    for cite in row[\"citations_within_dataset\"]:\n",
    "        cite_id = id_mapping.get(cite.get(\"doi\") or cite.get(\"openalex_id\") or cite.get(\"semantic_scholar_id\") or cite.get(\"title\"))\n",
    "        if cite_id:\n",
    "            citation_relationships.append({\n",
    "                \"target_model_id\": target_id,\n",
    "                \"cited_model_id\": None,  # No cited paper here\n",
    "                \"citing_model_id\": cite_id\n",
    "            })\n",
    "# Step 3: Convert to DataFrame\n",
    "df_relationships = pd.DataFrame(citation_relationships)\n",
    "# Step 4: Save to CSV\n",
    "df_relationships.to_csv(\"./data/scilake/citation_relationships.csv\", index=False)\n",
    "print(\"Citation relationships saved to './data/scilake/citation_relationships.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     [@article{DBLP:journals/corr/abs-1810-04805, author    = {Jacob Devlin and Ming{-}Wei Chang and ...\n",
       "4     [@article{DBLP:journals/corr/abs-1911-02116, author    = {Alexis Conneau and Kartikay Khandelwal...\n",
       "5     [@misc{radford2022whisper, doi = {10.48550/ARXIV.2212.04356}, url = {https://arxiv.org/abs/2212....\n",
       "9     [@misc{grosman2021xlsr53-large-english, title={Fine-tuned {XLSR}-53 large model for speech recog...\n",
       "11    [@article{DBLP:journals/corr/abs-1907-11692, author    = {Yinhan Liu and Myle Ott and Naman Goya...\n",
       "13    [@misc{qwen2.5, title = {Qwen2.5: A Party of Foundation Models}, url = {https://qwenlm.github.io...\n",
       "14    [@misc{zhang2022opt, title={OPT: Open Pre-trained Transformer Language Models}, author={Susan Zh...\n",
       "15    [@misc{wu2020visual, title={Visual Transformers: Token-based Image Representation and Processing...\n",
       "16    [@article{radford2019language, title={Language Models are Unsupervised Multitask Learners}, auth...\n",
       "17    [@article{muennighoff2022crosslingual, title={Crosslingual generalization through multitask fine...\n",
       "Name: extracted_bibtex, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check sample\n",
    "key = \"extracted_bibtex\"\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "filtered_df = df_split_temp[df_split_temp[key].notnull() & (df_split_temp[key].apply(lambda x: len(x) > 0))]\n",
    "filtered_df[key].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['modelId', 'author', 'last_modified', 'downloads', 'likes',\n",
       "       'library_name', 'tags', 'pipeline_tag', 'createdAt', 'card',\n",
       "       'card_tags', 'card_readme', 'card_tags_license', 'card_tags_language',\n",
       "       'card_tags_metrics', 'card_tags_base_model', 'card_tags_new_version',\n",
       "       'card_tags_pipeline_tag', 'card_tags_library_name', 'card_tags_tags',\n",
       "       'card_tags_datasets', 'analysis_keyword_status', 'detected_keywords',\n",
       "       'is_default_card', 'contains_markdown_table',\n",
       "       'extracted_markdown_table', 'link_info', 'pdf_link', 'github_link',\n",
       "       'extracted_bibtex', 'extracted_bibtex_tuple',\n",
       "       'extracted_markdown_table_tuple'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contains_markdown_table\n",
       "False    957671\n",
       "True     151088\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_temp['contains_markdown_table'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extracted_markdown_table\n",
       "| Library | Tokens per Second | Max Memory Usage | BLEU |\\n| :----: | :----: | :----: | :----: |\\n| Transformers 4.26.1 (with PyTorch 1.13.1) | 147.3 | 2332MB | 27.90 |\\n| Marian 1.11.0 (int16) | 330.2 | 5901MB | 27.65 |\\n| Marian 1.11.0 (int8) | 355.8 | 4763MB | 27.27 |\\n| CTranslate2 3.6.0 (int16) | 596.1 | 660MB | 27.53 |\\n| CTranslate2 3.6.0 (int8) | 696.1 | 516MB | 27.65 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             814\n",
       "| Headers               | Type     | Description                                                                                                        |\\n|:----------------------| :------- |:-------------------------------------------------------------------------------------------------------------------|\\n| `API-Key`             | `str` | Get your `API_KEY` from  [imagepipeline.io](https://imagepipeline.io/)                                             |\\n| `Content-Type`        | `str` | application/json - content type of the request body |\\n\\n\\n| Parameter | Type     | Description                |\\n| :-------- | :------- | :------------------------- |\\n| `model_id` | `str` | Your base model, find available lists in  [models page](https://imagepipeline.io/models) or upload your own|\\n| `prompt` | `str` | Text Prompt. Check our [Prompt Guide](https://docs.imagepipeline.io/docs/SD-1.5/docs/extras/prompt-guide) for tips |\\n| `num_inference_steps` | `int [1-50]` | Noise is removed with each step, resulting in a higher-quality image over time. Ideal value 30-50 (without LCM) |\\n| `guidance_scale` | `float [1-20]` | Higher guidance scale prioritizes text prompt relevance but sacrifices image quality. Ideal value 7.5-12.5 |\\n| `lora_models` | `str, array` | \\tPass the model_id(s) of LoRA models that can be found in models page |\\n| `lora_weights` | `str, array` | Strength of the LoRA effect |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      263\n",
       "||Training Data|Params|Content Length|GQA|Tokens|LR|\\n|---|---|---|---|---|---|---|\\n|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\\n|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\\n|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           258\n",
       "| Metric                   | Value                           |\\n|--------------------------|---------------------------------|\\n| Duration (in seconds)    | [More Information Needed]  |\\n| Emissions (Co2eq in kg)  | [More Information Needed] |\\n| CPU power (W)            | [NO CPU]  |\\n| GPU power (W)            | [No GPU]  |\\n| RAM power (W)            | [More Information Needed]  |\\n| CPU energy (kWh)         | [No CPU]  |\\n| GPU energy (kWh)         | [No GPU]  |\\n| RAM energy (kWh)         | [More Information Needed]  |\\n| Consumed energy (kWh)    | [More Information Needed]  |\\n| Country name             | [More Information Needed]  |\\n| Cloud provider           | [No Cloud]  |\\n| Cloud region             | [No Cloud]  |\\n| CPU count                | [No CPU]  |\\n| CPU model                | [No CPU]  |\\n| GPU count                | [No GPU]  |\\n| GPU model                | [No GPU]  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  238\n",
       "| Quant type | Description                                                                                |\\n|------------|--------------------------------------------------------------------------------------------|\\n| Q5_K_M     | High quality, recommended.                                                                 |\\n| Q5_K_S     | High quality, recommended.                                                                 |\\n| Q4_K_M     | Good quality, uses about 4.83 bits per weight, recommended.                                |\\n| Q4_K_S     | Slightly lower quality with more space savings, recommended.                               |\\n| IQ4_NL     | Decent quality, slightly smaller than Q4_K_S with similar performance, recommended.        |\\n| IQ4_XS     | Decent quality, smaller than Q4_K_S with similar performance, recommended.                 |\\n| Q3_K_L     | Lower quality but usable, good for low RAM availability.                                   |\\n| Q3_K_M     | Even lower quality.                                                                        |\\n| IQ3_M      | Medium-low quality, new method with decent performance comparable to Q3_K_M.               |\\n| IQ3_S      | Lower quality, new method with decent performance, recommended over Q3_K_S quant, same size with better performance. |\\n| Q3_K_S     | Low quality, not recommended.                                                              |\\n| IQ3_XS     | Lower quality, new method with decent performance, slightly better than Q3_K_S.            |\\n| Q2_K       | Very low quality but surprisingly usable.                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              187\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ... \n",
       "| Train Loss | Train Accuracy | Validation Loss | Validation Accuracy | Epoch |\\n|:----------:|:--------------:|:---------------:|:-------------------:|:-----:|\\n| 1.9135     | 0.0235         | 1.6378          | 0.0128              | 0     |\\n| 1.6200     | 0.0203         | 1.4978          | 0.0091              | 1     |\\n| 1.5127     | 0.0188         | 1.4323          | 0.0087              | 2     |\\n| 1.4434     | 0.0187         | 1.3812          | 0.0106              | 3     |\\n| 1.3857     | 0.0184         | 1.3845          | 0.0113              | 4     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1\n",
       "| Training Loss | Epoch | Step | Validation Loss |\\n|:-------------:|:-----:|:----:|:---------------:|\\n| No log        | 1.0   | 402  | 0.8233          |\\n| 1.0246        | 2.0   | 804  | 0.6594          |\\n| 0.7063        | 3.0   | 1206 | 0.5881          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
       "| Training Loss | Epoch | Step  | Validation Loss |\\n|:-------------:|:-----:|:-----:|:---------------:|\\n| 5.9858        | 0.24  | 500   | 5.0593          |\\n| 4.752         | 0.48  | 1000  | 4.6760          |\\n| 4.4497        | 0.72  | 1500  | 4.4435          |\\n| 4.2543        | 0.96  | 2000  | 4.2976          |\\n| 4.0555        | 1.21  | 2500  | 4.2137          |\\n| 3.9693        | 1.45  | 3000  | 4.1335          |\\n| 3.906         | 1.69  | 3500  | 4.0568          |\\n| 3.8429        | 1.93  | 4000  | 3.9920          |\\n| 3.6732        | 2.17  | 4500  | 3.9691          |\\n| 3.6327        | 2.41  | 5000  | 3.9306          |\\n| 3.6116        | 2.65  | 5500  | 3.8914          |\\n| 3.5938        | 2.89  | 6000  | 3.8513          |\\n| 3.455         | 3.13  | 6500  | 3.8610          |\\n| 3.3859        | 3.38  | 7000  | 3.8405          |\\n| 3.3923        | 3.62  | 7500  | 3.8156          |\\n| 3.3951        | 3.86  | 8000  | 3.7887          |\\n| 3.2753        | 4.1   | 8500  | 3.8143          |\\n| 3.1704        | 4.34  | 9000  | 3.8108          |\\n| 3.1945        | 4.58  | 9500  | 3.7931          |\\n| 3.1957        | 4.82  | 10000 | 3.7730          |\\n| 3.1308        | 5.06  | 10500 | 3.7997          |\\n| 2.9454        | 5.3   | 11000 | 3.8140          |\\n| 2.981         | 5.54  | 11500 | 3.8037          |\\n| 2.9917        | 5.79  | 12000 | 3.7886          |\\n| 2.9661        | 6.03  | 12500 | 3.8061          |\\n| 2.7333        | 6.27  | 13000 | 3.8368          |\\n| 2.7658        | 6.51  | 13500 | 3.8365          |\\n| 2.7757        | 6.75  | 14000 | 3.8304          |\\n| 2.7771        | 6.99  | 14500 | 3.8187          |\\n| 2.5518        | 7.23  | 15000 | 3.8726          |\\n| 2.56          | 7.47  | 15500 | 3.8759          |\\n| 2.5737        | 7.71  | 16000 | 3.8764          |\\n| 2.5772        | 7.96  | 16500 | 3.8738          |\\n| 2.4267        | 8.2   | 17000 | 3.9046          |\\n| 2.4129        | 8.44  | 17500 | 3.9102          |\\n| 2.4256        | 8.68  | 18000 | 3.9135          |\\n| 2.4177        | 8.92  | 18500 | 3.9138          |\\n| 2.3675        | 9.16  | 19000 | 3.9222          |\\n| 2.3412        | 9.4   | 19500 | 3.9246          |\\n| 2.3399        | 9.64  | 20000 | 3.9256          |\\n| 2.3381        | 9.88  | 20500 | 3.9256          |      1\n",
       "| Training Loss | Epoch | Step  | Validation Loss |\\n|:-------------:|:-----:|:-----:|:---------------:|\\n| 6.0172        | 0.25  | 500   | 5.1073          |\\n| 4.785         | 0.5   | 1000  | 4.7224          |\\n| 4.4827        | 0.75  | 1500  | 4.4942          |\\n| 4.298         | 0.99  | 2000  | 4.3427          |\\n| 4.0754        | 1.24  | 2500  | 4.2557          |\\n| 4.0142        | 1.49  | 3000  | 4.1790          |\\n| 3.9352        | 1.74  | 3500  | 4.1043          |\\n| 3.8786        | 1.99  | 4000  | 4.0364          |\\n| 3.6731        | 2.24  | 4500  | 4.0186          |\\n| 3.6688        | 2.49  | 5000  | 3.9721          |\\n| 3.661         | 2.73  | 5500  | 3.9350          |\\n| 3.6258        | 2.98  | 6000  | 3.8969          |\\n| 3.4249        | 3.23  | 6500  | 3.9079          |\\n| 3.425         | 3.48  | 7000  | 3.8847          |\\n| 3.4348        | 3.73  | 7500  | 3.8593          |\\n| 3.4292        | 3.98  | 8000  | 3.8315          |\\n| 3.2039        | 4.23  | 8500  | 3.8684          |\\n| 3.2172        | 4.48  | 9000  | 3.8502          |\\n| 3.2328        | 4.72  | 9500  | 3.8328          |\\n| 3.2348        | 4.97  | 10000 | 3.8186          |\\n| 2.9915        | 5.22  | 10500 | 3.8657          |\\n| 3.0125        | 5.47  | 11000 | 3.8594          |\\n| 3.0221        | 5.72  | 11500 | 3.8446          |\\n| 3.0287        | 5.97  | 12000 | 3.8301          |\\n| 2.797         | 6.22  | 12500 | 3.8864          |\\n| 2.7847        | 6.46  | 13000 | 3.8881          |\\n| 2.8069        | 6.71  | 13500 | 3.8780          |\\n| 2.8152        | 6.96  | 14000 | 3.8733          |\\n| 2.6075        | 7.21  | 14500 | 3.9199          |\\n| 2.587         | 7.46  | 15000 | 3.9290          |\\n| 2.6053        | 7.71  | 15500 | 3.9274          |\\n| 2.6083        | 7.96  | 16000 | 3.9261          |\\n| 2.4643        | 8.2   | 16500 | 3.9556          |\\n| 2.4402        | 8.45  | 17000 | 3.9621          |\\n| 2.4498        | 8.7   | 17500 | 3.9633          |\\n| 2.4516        | 8.95  | 18000 | 3.9635          |\\n| 2.3806        | 9.2   | 18500 | 3.9736          |\\n| 2.3719        | 9.45  | 19000 | 3.9753          |\\n| 2.372         | 9.7   | 19500 | 3.9759          |\\n| 2.3701        | 9.95  | 20000 | 3.9760          |                                                           1\n",
       "| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     | Precision | Recall | Roc Auc |\\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|:---------:|:------:|:-------:|\\n| No log        | 1.0   | 375  | 0.2136          | 0.9124   | 0.9131 | 0.9087    | 0.9175 | 0.9733  |\\n| 0.3086        | 2.0   | 750  | 0.1971          | 0.9195   | 0.9190 | 0.9277    | 0.9104 | 0.9770  |\\n| 0.1917        | 3.0   | 1125 | 0.1908          | 0.9222   | 0.9220 | 0.9267    | 0.9174 | 0.9781  |\\n| 0.1791        | 4.0   | 1500 | 0.1909          | 0.9224   | 0.9224 | 0.9247    | 0.9202 | 0.9785  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1\n",
       "Name: count, Length: 132270, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_temp['extracted_markdown_table'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "card_readme\n",
       "Entry not found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             359571\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             98047\n",
       "# Model Card for Model ID\\n\\n<!-- Provide a quick summary of what the model is/does. -->\\n\\n\\n\\n## Model Details\\n\\n### Model Description\\n\\n<!-- Provide a longer summary of what this model is. -->\\n\\n\\n\\n- **Developed by:** [More Information Needed]\\n- **Funded by [optional]:** [More Information Needed]\\n- **Shared by [optional]:** [More Information Needed]\\n- **Model type:** [More Information Needed]\\n- **Language(s) (NLP):** [More Information Needed]\\n- **License:** [More Information Needed]\\n- **Finetuned from model [optional]:** [More Information Needed]\\n\\n### Model Sources [optional]\\n\\n<!-- Provide the basic links for the model. -->\\n\\n- **Repository:** [More Information Needed]\\n- **Paper [optional]:** [More Information Needed]\\n- **Demo [optional]:** [More Information Needed]\\n\\n## Uses\\n\\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\\n\\n### Direct Use\\n\\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\\n\\n[More Information Needed]\\n\\n### Downstream Use [optional]\\n\\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\\n\\n[More Information Needed]\\n\\n### Out-of-Scope Use\\n\\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\\n\\n[More Information Needed]\\n\\n## Bias, Risks, and Limitations\\n\\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\\n\\n[More Information Needed]\\n\\n### Recommendations\\n\\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\\n\\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n## How to Get Started with the Model\\n\\nUse the code below to get started with the model.\\n\\n[More Information Needed]\\n\\n## Training Details\\n\\n### Training Data\\n\\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\\n\\n[More Information Needed]\\n\\n### Training Procedure\\n\\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\\n\\n#### Preprocessing [optional]\\n\\n[More Information Needed]\\n\\n\\n#### Training Hyperparameters\\n\\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\\n\\n#### Speeds, Sizes, Times [optional]\\n\\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\\n\\n[More Information Needed]\\n\\n## Evaluation\\n\\n<!-- This section describes the evaluation protocols and provides the results. -->\\n\\n### Testing Data, Factors & Metrics\\n\\n#### Testing Data\\n\\n<!-- This should link to a Dataset Card if possible. -->\\n\\n[More Information Needed]\\n\\n#### Factors\\n\\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\\n\\n[More Information Needed]\\n\\n#### Metrics\\n\\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\\n\\n[More Information Needed]\\n\\n### Results\\n\\n[More Information Needed]\\n\\n#### Summary\\n\\n\\n\\n## Model Examination [optional]\\n\\n<!-- Relevant interpretability work for the model goes here -->\\n\\n[More Information Needed]\\n\\n## Environmental Impact\\n\\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\\n\\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n\\n- **Hardware Type:** [More Information Needed]\\n- **Hours used:** [More Information Needed]\\n- **Cloud Provider:** [More Information Needed]\\n- **Compute Region:** [More Information Needed]\\n- **Carbon Emitted:** [More Information Needed]\\n\\n## Technical Specifications [optional]\\n\\n### Model Architecture and Objective\\n\\n[More Information Needed]\\n\\n### Compute Infrastructure\\n\\n[More Information Needed]\\n\\n#### Hardware\\n\\n[More Information Needed]\\n\\n#### Software\\n\\n[More Information Needed]\\n\\n## Citation [optional]\\n\\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\\n\\n**BibTeX:**\\n\\n[More Information Needed]\\n\\n**APA:**\\n\\n[More Information Needed]\\n\\n## Glossary [optional]\\n\\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\\n\\n[More Information Needed]\\n\\n## More Information [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Authors [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Contact\\n\\n[More Information Needed]\\n### Framework versions\\n\\n- PEFT 0.12.0                                                                                                91829\n",
       "# Model Card for Model ID\\n\\n<!-- Provide a quick summary of what the model is/does. -->\\n\\n\\n\\n## Model Details\\n\\n### Model Description\\n\\n<!-- Provide a longer summary of what this model is. -->\\n\\nThis is the model card of a ðŸ¤— transformers model that has been pushed on the Hub. This model card has been automatically generated.\\n\\n- **Developed by:** [More Information Needed]\\n- **Funded by [optional]:** [More Information Needed]\\n- **Shared by [optional]:** [More Information Needed]\\n- **Model type:** [More Information Needed]\\n- **Language(s) (NLP):** [More Information Needed]\\n- **License:** [More Information Needed]\\n- **Finetuned from model [optional]:** [More Information Needed]\\n\\n### Model Sources [optional]\\n\\n<!-- Provide the basic links for the model. -->\\n\\n- **Repository:** [More Information Needed]\\n- **Paper [optional]:** [More Information Needed]\\n- **Demo [optional]:** [More Information Needed]\\n\\n## Uses\\n\\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\\n\\n### Direct Use\\n\\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\\n\\n[More Information Needed]\\n\\n### Downstream Use [optional]\\n\\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\\n\\n[More Information Needed]\\n\\n### Out-of-Scope Use\\n\\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\\n\\n[More Information Needed]\\n\\n## Bias, Risks, and Limitations\\n\\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\\n\\n[More Information Needed]\\n\\n### Recommendations\\n\\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\\n\\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n## How to Get Started with the Model\\n\\nUse the code below to get started with the model.\\n\\n[More Information Needed]\\n\\n## Training Details\\n\\n### Training Data\\n\\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\\n\\n[More Information Needed]\\n\\n### Training Procedure\\n\\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\\n\\n#### Preprocessing [optional]\\n\\n[More Information Needed]\\n\\n\\n#### Training Hyperparameters\\n\\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\\n\\n#### Speeds, Sizes, Times [optional]\\n\\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\\n\\n[More Information Needed]\\n\\n## Evaluation\\n\\n<!-- This section describes the evaluation protocols and provides the results. -->\\n\\n### Testing Data, Factors & Metrics\\n\\n#### Testing Data\\n\\n<!-- This should link to a Dataset Card if possible. -->\\n\\n[More Information Needed]\\n\\n#### Factors\\n\\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\\n\\n[More Information Needed]\\n\\n#### Metrics\\n\\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\\n\\n[More Information Needed]\\n\\n### Results\\n\\n[More Information Needed]\\n\\n#### Summary\\n\\n\\n\\n## Model Examination [optional]\\n\\n<!-- Relevant interpretability work for the model goes here -->\\n\\n[More Information Needed]\\n\\n## Environmental Impact\\n\\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\\n\\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n\\n- **Hardware Type:** [More Information Needed]\\n- **Hours used:** [More Information Needed]\\n- **Cloud Provider:** [More Information Needed]\\n- **Compute Region:** [More Information Needed]\\n- **Carbon Emitted:** [More Information Needed]\\n\\n## Technical Specifications [optional]\\n\\n### Model Architecture and Objective\\n\\n[More Information Needed]\\n\\n### Compute Infrastructure\\n\\n[More Information Needed]\\n\\n#### Hardware\\n\\n[More Information Needed]\\n\\n#### Software\\n\\n[More Information Needed]\\n\\n## Citation [optional]\\n\\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\\n\\n**BibTeX:**\\n\\n[More Information Needed]\\n\\n**APA:**\\n\\n[More Information Needed]\\n\\n## Glossary [optional]\\n\\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\\n\\n[More Information Needed]\\n\\n## More Information [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Authors [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Contact\\n\\n[More Information Needed]      52675\n",
       "# Model Card for Model ID\\n\\n<!-- Provide a quick summary of what the model is/does. -->\\n\\n\\n\\n## Model Details\\n\\n### Model Description\\n\\n<!-- Provide a longer summary of what this model is. -->\\n\\nThis is the model card of a ðŸ¤— transformers model that has been pushed on the Hub. This model card has been automatically generated.\\n\\n- **Developed by:** [More Information Needed]\\n- **Funded by [optional]:** [More Information Needed]\\n- **Shared by [optional]:** [More Information Needed]\\n- **Model type:** [More Information Needed]\\n- **Language(s) (NLP):** [More Information Needed]\\n- **License:** [More Information Needed]\\n- **Finetuned from model [optional]:** [More Information Needed]\\n\\n### Model Sources [optional]\\n\\n<!-- Provide the basic links for the model. -->\\n\\n- **Repository:** [More Information Needed]\\n- **Paper [optional]:** [More Information Needed]\\n- **Demo [optional]:** [More Information Needed]\\n\\n## Uses\\n\\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\\n\\n### Direct Use\\n\\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\\n\\n[More Information Needed]\\n\\n### Downstream Use [optional]\\n\\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\\n\\n[More Information Needed]\\n\\n### Out-of-Scope Use\\n\\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\\n\\n[More Information Needed]\\n\\n## Bias, Risks, and Limitations\\n\\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\\n\\n[More Information Needed]\\n\\n### Recommendations\\n\\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\\n\\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n## How to Get Started with the Model\\n\\nUse the code below to get started with the model.\\n\\n[More Information Needed]\\n\\n## Training Details\\n\\n### Training Data\\n\\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\\n\\n[More Information Needed]\\n\\n### Training Procedure \\n\\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\\n\\n#### Preprocessing [optional]\\n\\n[More Information Needed]\\n\\n\\n#### Training Hyperparameters\\n\\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\\n\\n#### Speeds, Sizes, Times [optional]\\n\\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\\n\\n[More Information Needed]\\n\\n## Evaluation\\n\\n<!-- This section describes the evaluation protocols and provides the results. -->\\n\\n### Testing Data, Factors & Metrics\\n\\n#### Testing Data\\n\\n<!-- This should link to a Dataset Card if possible. -->\\n\\n[More Information Needed]\\n\\n#### Factors\\n\\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\\n\\n[More Information Needed]\\n\\n#### Metrics\\n\\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\\n\\n[More Information Needed]\\n\\n### Results\\n\\n[More Information Needed]\\n\\n#### Summary\\n\\n\\n\\n## Model Examination [optional]\\n\\n<!-- Relevant interpretability work for the model goes here -->\\n\\n[More Information Needed]\\n\\n## Environmental Impact\\n\\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\\n\\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n\\n- **Hardware Type:** [More Information Needed]\\n- **Hours used:** [More Information Needed]\\n- **Cloud Provider:** [More Information Needed]\\n- **Compute Region:** [More Information Needed]\\n- **Carbon Emitted:** [More Information Needed]\\n\\n## Technical Specifications [optional]\\n\\n### Model Architecture and Objective\\n\\n[More Information Needed]\\n\\n### Compute Infrastructure\\n\\n[More Information Needed]\\n\\n#### Hardware\\n\\n[More Information Needed]\\n\\n#### Software\\n\\n[More Information Needed]\\n\\n## Citation [optional]\\n\\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\\n\\n**BibTeX:**\\n\\n[More Information Needed]\\n\\n**APA:**\\n\\n[More Information Needed]\\n\\n## Glossary [optional]\\n\\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\\n\\n[More Information Needed]\\n\\n## More Information [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Authors [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Contact\\n\\n[More Information Needed]     12131\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ...  \n",
       "# cat-not-cat\\n\\n\\nAutogenerated by HuggingPicsðŸ¤—ðŸ–¼ï¸\\n\\nCreate your own image classifier for **anything** by running [the demo on Google Colab](https://colab.research.google.com/github/nateraw/huggingpics/blob/main/HuggingPics.ipynb).\\n\\nReport any issues with the demo at the [github repo](https://github.com/nateraw/huggingpics).\\n\\n\\n## Example Images\\n\\n\\n#### Cat\\n\\n![Cat](images/Cat.jpg)\\n\\n#### opposite of a cat\\n\\n![opposite of a cat](images/opposite_of_a_cat.jpg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
       "<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n# bert-emotion\\n\\nThis model is a fine-tuned version of [distilbert-base-cased](https://huggingface.co/distilbert-base-cased) on the tweet_eval dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: 1.1951\\n- Precision: 0.7350\\n- Recall: 0.7334\\n- Fscore: 0.7341\\n\\n## Model description\\n\\nMore information needed\\n\\n## Intended uses & limitations\\n\\nMore information needed\\n\\n## Training and evaluation data\\n\\nMore information needed\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 5e-05\\n- train_batch_size: 4\\n- eval_batch_size: 4\\n- seed: 42\\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\\n- lr_scheduler_type: linear\\n- num_epochs: 3\\n\\n### Training results\\n\\n| Training Loss | Epoch | Step | Validation Loss | Precision | Recall | Fscore |\\n|:-------------:|:-----:|:----:|:---------------:|:---------:|:------:|:------:|\\n| 0.8468        | 1.0   | 815  | 0.7465          | 0.7116    | 0.6096 | 0.6325 |\\n| 0.5105        | 2.0   | 1630 | 0.9035          | 0.7532    | 0.7111 | 0.7276 |\\n| 0.2492        | 3.0   | 2445 | 1.1951          | 0.7350    | 0.7334 | 0.7341 |\\n\\n\\n### Framework versions\\n\\n- Transformers 4.25.1\\n- Pytorch 1.13.0+cu116\\n- Datasets 2.8.0\\n- Tokenizers 0.13.2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
       "<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n# Classification of patent title - \"green\" or \"no green\"\\n\\nThis model classifies patents into \"green patents\" or \"no green patents\" by their titles.\\n\\n### Examples of \"green patents\" titles:\\n\\n- \"A method for recycling waste\" - score: 0.714 \\n- \"A method of reducing pollution\" - score: 0.786\\n- \"An apparatus to improve environmental aspects\" - score: 0.570\\n- \"A method to improve waste management\" - score: 0.813\\n- \"A device to use renewable energy sources\" - score: 0.98\\n- \"A technology for efficient electrical power generation\"- score: 0.975\\n- \"A method for the production of fuel of non-fossil origin\" - score: 0.975\\n- \"Biofuels from waste\" - score: 0.88\\n- \"A combustion technology with mitigation potential\" - score: 0.947\\n- \"A device to capture greenhouse gases\" - score: 0.871\\n- \"A method to reduce the greenhouse effect\" - score: 0.887\\n- \"A device to improve the climate\" - score: 0.650\\n- \"A device to stop climate change\" - score: 0.55\\n\\n\\n### Examples of \"no green patents\" titles:\\n\\n- \"A device to destroy the nature\" - score: 0.19\\n- \"A method to produce smoke\" - score: 0.386\\n\\n### Examples of the model's limitation\\n\\n- \"A method to avoid trash\" - score: 0.165\\n- \"A method to reduce trash\" - score: 0.333\\n- \"A method to burn the Amazonas\" - score: 0.501\\n- \"A method to burn wood\" - score: 0.408\\n- \"Green plastics\" - score: 0.126\\n- \"Greta Thunberg\" - score: 0.313 (How dare you, model?); BUT: \"A method of using Greta Thunberg to stop climate change\" - score: 0.715\\n\\nExamples were inspired by https://www.epo.org/news-events/in-focus/classification/classification.html\\n\\n# distilbert-base-uncased-finetuned-greenpatent\\n\\nThis model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on the [green patent dataset](https://huggingface.co/datasets/cwinkler/green_patents). The green patent dataset was split into 70 % training data and 30 % test data (using \".train_test_split(test_size=0.3)\").\\nThe model achieves the following results on the evaluation set:\\n- Loss: 0.3148\\n- Accuracy: 0.8776\\n- F1: 0.8770\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 2e-05\\n- train_batch_size: 64\\n- eval_batch_size: 64\\n- seed: 42\\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\\n- lr_scheduler_type: linear\\n- num_epochs: 2\\n\\n### Training results\\n\\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\\n| 0.4342        | 1.0   | 101  | 0.3256          | 0.8721   | 0.8712 |\\n| 0.3229        | 2.0   | 202  | 0.3148          | 0.8776   | 0.8770 |\\n\\n\\n### Framework versions\\n\\n- Transformers 4.25.1\\n- Pytorch 1.13.1+cpu\\n- Datasets 2.8.0\\n- Tokenizers 0.13.2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1\n",
       "# CamemBERT-base for sentiment analysis on tweets\\n\\nThis is a Camembert-base model trained on a corpus of 50K french tweets. \\n\\n - Git Repo containing the dataset and the code (scraping & training) : [Git](https://git.unistra.fr/nierichlo/projet_tutore_gr1)\\n\\nThe model can predict which of the 25 emojis it has been trained with suits the best on a given sentence / tweet. \\nThe 25 emojis are the 25 most frequent in the dataset.\\n\\nWe've succeeded to obtain a 32% accuracy on a small amount of tweets.\\n\\n**Note**: We've also decided to keep the emojis in their *demojized* versions because some emojis could be seen as two (ex : ðŸ‘ðŸ¿)\\n\\n## Loading the model \\n\\n```python\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TFAutoModelForSequenceClassification\\n\\nMODEL = f\"Jessy3ric/camembert-twitter-emoji\"\\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\\n\\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\\nmodel.save_pretrained(MODEL)\\n\\n```                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1\n",
       "# Introduction\\n\\nspaCy NER model for Spanish trained with interviews in the domain of tourism related to the Way of Saint Jacques. It recognizes four types of entities: location (LOC), organizations (ORG), person (PER) and miscellaneous (MISC). It was fine-tuned using `PlanTL-GOB-ES/roberta-base-bne`.\\n\\n| Feature | Description |\\n| --- | --- |\\n| **Name** | `bne-spacy-corgale-ner-es` |\\n| **Version** | `0.0.2` |\\n| **spaCy** | `>=3.5.2,<3.6.0` |\\n| **Default Pipeline** | `transformer`, `ner` |\\n| **Components** | `transformer`, `ner` |\\n\\n### Label Scheme\\n\\n<details>\\n\\n<summary>View label scheme (4 labels for 1 components)</summary>\\n\\n| Component | Labels |\\n| --- | --- |\\n| **`ner`** | `LOC`, `MISC`, `ORG`, `PER` |\\n\\n</details>\\n\\n## Usage\\n\\nYou can use this model with the spaCy *pipeline* for NER.\\n\\n```python\\nimport spacy\\nfrom spacy.pipeline import merge_entities\\n\\n\\nnlp = spacy.load(\"bne-spacy-corgale-ner-es\")\\nnlp.add_pipe('sentencizer')\\n\\nexample = \"Fue antes de llegar a SigÃ¼eiro, en el Camino de Santiago. Si te metes en el Franco desde la Alameda, vas hacia la Catedral. Y allÃ­ precisamente es Santiago el patrÃ³n del pueblo.\"\\nner_pipe = nlp(example)\\n\\nprint(ner_pipe.ents)\\nfor token in merge_entities(ner_pipe):\\n    print(token.text, token.ent_type_)\\n```\\n\\n## Dataset\\n\\nToDo\\n\\n## Model performance\\n\\nentity|precision|recall|f1\\n-|-|-|-\\nLOC|0.985|0.987|0.986\\nMISC|0.862|0.865|0.863\\nORG|0.938|0.779|0.851\\nPER|0.921|0.941|0.931\\nmicro avg|0.971|0.972|0.971\\nmacro avg|0.926|0.893|0.908\\nweighted avg|0.971|0.972|0.971                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       "Name: count, Length: 387098, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_temp['card_readme'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç­‰äºŽ 1 çš„ content æ•°é‡: 374443\n",
      "ç­‰äºŽ 1 çš„ content å¯¹åº”çš„ index æ•°é‡: 374443\n",
      "å‰©ä½™çš„ index æ•°é‡: 734316\n"
     ]
    }
   ],
   "source": [
    "value_counts = df_split_temp['card_readme'].value_counts()\n",
    "unique_content = value_counts[value_counts == 1].index\n",
    "count_unique_content = len(unique_content)\n",
    "unique_indexes = df_split_temp[df_split_temp['card_readme'].isin(unique_content)].index\n",
    "count_unique_indexes = len(unique_indexes)\n",
    "count_remaining_indexes = len(df_split_temp) - count_unique_indexes\n",
    "print(\"ç­‰äºŽ 1 çš„ content æ•°é‡:\", count_unique_content)\n",
    "print(\"ç­‰äºŽ 1 çš„ content å¯¹åº”çš„ index æ•°é‡:\", count_unique_indexes)\n",
    "print(\"å‰©ä½™çš„ index æ•°é‡:\", count_remaining_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‰ 5 ä¸ªå‡ºçŽ°æ¬¡æ•°ç­‰äºŽ 2 çš„å†…å®¹åŠå…¶ `modelId`:\n",
      "ManarSaad/Rahaal_two, ManarSaad/Rahaal\n",
      "linghypshen/xlm-roberta-base-finetuned-panx-all, MrWetsnow/xlm-roberta-base-finetuned-panx-all\n",
      "admruul/anything-v3.0, BlooBarri/Bloo\n",
      "hanzohazashi1/symptoms_disease-gguf, hanzohazashi1/physician_model-gguf\n",
      "ChaoticNeutrals/Domain-Fusion-L3-8B, Nitral-Archive/Domain-Fusion-L3-8B-5bpw-exl2\n"
     ]
    }
   ],
   "source": [
    "value_counts = df_split_temp['card_readme'].value_counts()\n",
    "content_with_two_counts = value_counts[value_counts == 2].index.tolist()\n",
    "content_with_two_counts = content_with_two_counts[:5]\n",
    "model_ids_dict = {\n",
    "    content: df_split_temp[df_split_temp['card_readme'] == content]['modelId'].tolist()\n",
    "    for content in content_with_two_counts\n",
    "}\n",
    "print(\"å‰ 5 ä¸ªå‡ºçŽ°æ¬¡æ•°ç­‰äºŽ 2 çš„å†…å®¹åŠå…¶ `modelId`:\")\n",
    "for content, model_ids in model_ids_dict.items():\n",
    "    print(f\"{model_ids[0]}, {model_ids[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-get the links\n",
    "\n",
    "The PDF links can come from \n",
    "- tags like `arxiv:id`\n",
    "- links from `card_readme` columns\n",
    "- arxiv/pdf links from github readme files\n",
    "- bibtex from `card_readme`\n",
    "- possible other card_tags (need to wait code runs and check manually, e.g. `papers` tags might contain the possible links)\n",
    "- Remember to remove the arxiv:1910.09700 or https://arxiv.org/abs/1910.09700 as it is the default PDF link exists from readme file\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_complete_bibtex_final(content: str):\n",
    "    \"\"\"\n",
    "    Final version to extract complete BibTeX entries with balanced braces and multi-line support.\n",
    "    \"\"\"\n",
    "    bibtex_entries = []\n",
    "    bibtex_pattern = r\"@(\\w+)\\{\"\n",
    "    current_entry = \"\"\n",
    "    open_braces = 0\n",
    "    inside_entry = False\n",
    "    for line in content.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if not inside_entry and re.match(bibtex_pattern, line):\n",
    "            inside_entry = True\n",
    "            current_entry = line\n",
    "            open_braces = line.count(\"{\") - line.count(\"}\")\n",
    "        elif inside_entry:\n",
    "            current_entry += \" \" + line\n",
    "            open_braces += line.count(\"{\") - line.count(\"}\")\n",
    "        if inside_entry and open_braces == 0:\n",
    "            bibtex_entries.append(current_entry.strip())\n",
    "            inside_entry = False\n",
    "            current_entry = \"\"\n",
    "    return bibtex_entries\n",
    "\n",
    "def detect_bibtex_entries_final(card_content: str):\n",
    "    \"\"\"\n",
    "    Final version for detecting BibTeX entries from content.\n",
    "    \"\"\"\n",
    "    if not isinstance(card_content, str) or not card_content.strip():\n",
    "        return (False, [])\n",
    "    code_block_pattern = r\"```(?:bibtex)?\\n(.*?)(?:```|$)\"\n",
    "    code_blocks = re.findall(code_block_pattern, card_content, re.DOTALL)\n",
    "    bibtex_entries = []\n",
    "    for block in code_blocks:\n",
    "        bibtex_entries += extract_complete_bibtex_final(block)\n",
    "    \n",
    "    ### Also detect ```\n",
    "    # @xxxx{}``` cases\n",
    "    inline_bibtex_pattern = r\"```\\s*@\\w+\\{.*?\\}\\s*```\"\n",
    "    inline_bibtex_matches = re.findall(inline_bibtex_pattern, card_content, re.DOTALL)\n",
    "    for match in inline_bibtex_matches:\n",
    "        bibtex_entries.append(match.strip(\"`\"))\n",
    "    \n",
    "    # Also check for inline BibTeX entries outside of code blocks\n",
    "    content_without_code_blocks = re.sub(code_block_pattern, \"\", card_content, flags=re.DOTALL)\n",
    "    bibtex_entries += extract_complete_bibtex_final(content_without_code_blocks)\n",
    "    if bibtex_entries:\n",
    "        return (True, bibtex_entries)\n",
    "    return (False, [])\n",
    "\n",
    "df_split_temp['contains_bibtex'], df_split_temp['all_extracted_bibtex'] = zip(\n",
    "    *df_split_temp['card_readme'].apply(detect_bibtex_entries_final)\n",
    ")\n",
    "\n",
    "def parse_bibtex_entries(entries):\n",
    "    \"\"\"\n",
    "    Parse BibTeX entries into structured data (type, title, full entry).\n",
    "    \"\"\"\n",
    "    parsed_entries = []\n",
    "    for entry in entries:\n",
    "        bibtex_type = re.match(r\"^@(\\w+)\\{\", entry)\n",
    "        bibtex_type = bibtex_type.group(1) if bibtex_type else None\n",
    "        bibtex_title = re.search(r\"title\\s*=\\s*[{\\\"]([^{}\\\"]+)[}\\\"]\", entry, re.IGNORECASE)\n",
    "        bibtex_title = bibtex_title.group(1) if bibtex_title else None\n",
    "        parsed_entries.append({\n",
    "            \"type\": bibtex_type,\n",
    "            \"title\": bibtex_title,\n",
    "            \"entry\": entry\n",
    "        })\n",
    "    return parsed_entries\n",
    "\n",
    "df_split_temp['parsed_bibtex_entries'] = df_split_temp['all_extracted_bibtex'].apply(parse_bibtex_entries)\n",
    "df_split_temp['bibtex_count'] = df_split_temp['parsed_bibtex_entries'].apply(lambda x: len(x) if x else 0)\n",
    "df_split_temp.to_csv(\"extracted_bibtex_entries_grouped.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BibTeX presence ratio: 0.031386\n",
      "BibTeX Count Distribution:\n",
      "0     1073960\n",
      "2       16499\n",
      "1       14148\n",
      "3        2340\n",
      "4        1220\n",
      "6         221\n",
      "5         189\n",
      "8          60\n",
      "14         29\n",
      "7          27\n",
      "10         24\n",
      "9          17\n",
      "16         13\n",
      "11          4\n",
      "12          3\n",
      "31          2\n",
      "13          1\n",
      "15          1\n",
      "28          1\n",
      "Name: bibtex_count, dtype: int64\n",
      "First five entries with exactly 2 BibTeX records:\n",
      "                                parsed_bibtex_entries\n",
      "15  [{'type': 'misc', 'title': 'Visual Transformer...\n",
      "20  [{'type': 'inproceedings', 'title': 'Wespeaker...\n",
      "23  [{'type': 'inproceedings', 'title': 'Proc. INT...\n",
      "33  [{'type': 'inproceedings', 'title': 'Proc. INT...\n",
      "38  [{'type': None, 'title': 'Chronos: Learning th...\n",
      "Duplicate BibTeX Entries:\n",
      "@article{https://doi.org/10.48550/arxiv.2209.11055, doi = {10.48550/ARXIV.2209.11055}, url = {https://arxiv.org/abs/2209.11055}, author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren}, keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}, title = {Efficient Few-Shot Learning Without Prompts}, publisher = {arXiv}, year = {2022}, copyright = {Creative Commons Attribution 4.0 International} }    2080\n",
      "@inproceedings{reimers-2019-sentence-bert, title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", author = \"Reimers, Nils and Gurevych, Iryna\", booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\", month = \"11\", year = \"2019\", publisher = \"Association for Computational Linguistics\", url = \"https://arxiv.org/abs/1908.10084\", }                                                                                                                                                              1244\n",
      "@article{pratap2023mms, title={Scaling Speech Technology to 1,000+ Languages}, author={Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and Ali Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli}, journal={arXiv}, year={2023} }                                                                                                                                                              1173\n",
      "\\n@article{chang-etal-2024-goldfish,\\n  title={Goldfish: Monolingual Language Models for 350 Languages},\\n  author={Chang, Tyler A. and Arnett, Catherine and Tu, Zhuowen and Bergen, Benjamin K.},\\n  journal={Preprint},\\n  year={2024},\\n  url={https://www.arxiv.org/abs/2408.10441},\\n}\\n                                                                                                                                                                                                                                                                          1154\n",
      "@article{chang-etal-2024-goldfish, title={Goldfish: Monolingual Language Models for 350 Languages}, author={Chang, Tyler A. and Arnett, Catherine and Tu, Zhuowen and Bergen, Benjamin K.}, journal={Preprint}, year={2024}, url={https://www.arxiv.org/abs/2408.10441}, }                                                                                                                                                                                                                                                                                              1154\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ... \n",
      "@misc{wav2lip_uhq, author = {numz}, title = {Wav2Lip UHQ}, year = {2023}, howpublished = {GitHub repository}, publisher = {numz}, url = {https://github.com/numz/sd-wav2lip-uhq} }                                                                                                                                                                                                                                                                                                                                                                                         2\n",
      "\\n@misc{numina_math_7b,\\n  author = {Edward Beeching and Shengyi Costa Huang and Albert Jiang and Jia Li and Benjamin Lipkin and Zihan Qina and Kashif Rasul and Ziju Shen and Roman Soletskyi and Lewis Tunstall},\\n  title = {NuminaMath 7B CoT},\\n  year = {2024},\\n  publisher = {Numina & Hugging Face},\\n  journal = {Hugging Face repository},\\n  howpublished = {\\url{https://huggingface.co/AI-MO/NuminaMath-7B-CoT}}\\n}\\n                                                                                                                                        2\n",
      "@article{zhan2024anygpt, title={AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling}, author={Zhan, Jun and Dai, Junqi and Ye, Jiasheng and Zhou, Yunhua and Zhang, Dong and Liu, Zhigeng and Zhang, Xin and Yuan, Ruibin and Zhang, Ge and Li, Linyang and others}, journal={arXiv preprint arXiv:2402.12226}, year={2024} }                                                                                                                                                                                                                                   2\n",
      "\\n@article{zhang2024longcite,\\n  title = {LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA} \\n  author={Jiajie Zhang and Yushi Bai and Xin Lv and Wanjun Gu and Danqing Liu and Minhao Zou and Shulin Cao and Lei Hou and Yuxiao Dong and Ling Feng and Juanzi Li},\\n  journal={arXiv preprint arXiv:2409.02897},\\n  year={2024}\\n}\\n                                                                                                                                                                                                         2\n",
      "@inproceedings{inoue2024opencole, title={{OpenCOLE: Towards Reproducible Automatic Graphic Design Generation}}, author={Naoto Inoue and Kento Masui and Wataru Shimoda and Kota Yamaguchi}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, year={2024}, }                                                                                                                                                                                                                                                2\n",
      "Length: 4752, dtype: int64\n",
      "Entries containing '@' but not valid BibTeX:\n",
      "                                 modelId  \\\n",
      "29             meta-llama/Llama-3.1-405B   \n",
      "39     M-CLIP/XLM-Roberta-Large-Vit-B-32   \n",
      "50                 pyannote/segmentation   \n",
      "55  cross-encoder/ms-marco-MiniLM-L-4-v2   \n",
      "61      meta-llama/Llama-3.1-8B-Instruct   \n",
      "\n",
      "                                          card_readme  \n",
      "29  ## Model Information\\n\\nThe Meta Llama 3.1 col...  \n",
      "39  ## Multilingual-clip: XLM-Roberta-Large-Vit-B-...  \n",
      "50  Using this open-source model in production?  \\...  \n",
      "55  # Cross-Encoder for MS Marco\\n\\nThis model was...  \n",
      "61  ## Model Information\\n\\nThe Meta Llama 3.1 col...  \n"
     ]
    }
   ],
   "source": [
    "# Compute statistics\n",
    "bibtex_total_ratio = len(df_split_temp[df_split_temp['contains_bibtex']]) / len(df_split_temp)\n",
    "print(f\"BibTeX presence ratio: {bibtex_total_ratio:.6f}\")\n",
    "\n",
    "# Count occurrences for specific BibTeX counts\n",
    "df_split_temp['bibtex_count'] = df_split_temp['parsed_bibtex_entries'].apply(lambda x: len(x) if x else 0)\n",
    "bibtex_count_distribution = df_split_temp['bibtex_count'].value_counts()\n",
    "print(\"BibTeX Count Distribution:\")\n",
    "print(bibtex_count_distribution)\n",
    "\n",
    "# Check first five entries with exactly 2 BibTeX records\n",
    "bibtex_2_entries = df_split_temp[df_split_temp['bibtex_count'] == 2][['parsed_bibtex_entries']].head(5)\n",
    "print(\"First five entries with exactly 2 BibTeX records:\")\n",
    "print(bibtex_2_entries)\n",
    "\n",
    "# Identify duplicate BibTeX entries\n",
    "bibtex_flat_list = [entry['entry'] for entries in df_split_temp['parsed_bibtex_entries'] for entry in entries]\n",
    "bibtex_duplicates = pd.Series(bibtex_flat_list).value_counts()\n",
    "print(\"Duplicate BibTeX Entries:\")\n",
    "print(bibtex_duplicates[bibtex_duplicates > 1])\n",
    "\n",
    "# Identify entries containing @ but without valid BibTeX\n",
    "df_split_temp['contains_at_symbol'] = df_split_temp['card_readme'].apply(lambda x: '@' in x if isinstance(x, str) else False)\n",
    "df_split_temp['invalid_bibtex'] = df_split_temp.apply(lambda x: x['contains_at_symbol'] and not x['contains_bibtex'], axis=1)\n",
    "invalid_bibtex_entries = df_split_temp[df_split_temp['invalid_bibtex']]\n",
    "print(\"Entries containing '@' but not valid BibTeX:\")\n",
    "print(invalid_bibtex_entries[['modelId', 'card_readme']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BibTeX Type Counts:\n",
      "article           18647\n",
      "misc              13803\n",
      "inproceedings     11725\n",
      "software            673\n",
      "online              335\n",
      "techreport          142\n",
      "phdthesis            48\n",
      "unpublished          39\n",
      "incollection         25\n",
      "model                24\n",
      "modelcard            16\n",
      "dataset              14\n",
      "book                 13\n",
      "data                 12\n",
      "mastersthesis         8\n",
      "paper                 8\n",
      "ainproceedings        5\n",
      "thesis                4\n",
      "artical               4\n",
      "proceedings           4\n",
      "preprint              3\n",
      "conference            2\n",
      "citation              2\n",
      "inbook                2\n",
      "articles              1\n",
      "masterthesis          1\n",
      "mics                  1\n",
      "sd_prompts            1\n",
      "miscellaneous         1\n",
      "journal               1\n",
      "contact               1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each BibTeX type\n",
    "bibtex_type_counts = pd.Series([entry['type'].lower() for entries in df_split_temp['parsed_bibtex_entries'] for entry in entries if entry['type']]).value_counts()\n",
    "print(\"BibTeX Type Counts:\")\n",
    "print(bibtex_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BibTeX presence ratio: 0.031300\n",
      "BibTeX Count Distribution:\n",
      "0     1074055\n",
      "1       27589\n",
      "2        4658\n",
      "3        1714\n",
      "4         458\n",
      "5         169\n",
      "6          48\n",
      "7          48\n",
      "8          16\n",
      "30          2\n",
      "15          1\n",
      "14          1\n",
      "Name: bibtex_count, dtype: int64\n",
      "First five entries with exactly 2 BibTeX records:\n",
      "                                parsed_bibtex_entries\n",
      "13  [{'type': 'misc', 'title': 'Qwen2.5: A Party o...\n",
      "15  [{'type': 'misc', 'title': 'Visual Transformer...\n",
      "20  [{'type': 'inproceedings', 'title': 'Wespeaker...\n",
      "23  [{'type': 'inproceedings', 'title': 'Proc. INT...\n",
      "33  [{'type': 'inproceedings', 'title': 'Proc. INT...\n",
      "Duplicate BibTeX Entries:\n",
      "@article{https://doi.org/10.48550/arxiv.2209.11055, doi = {10.48550/ARXIV.2209.11055}, url = {https://arxiv.org/abs/2209.11055}, author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren}, keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}, title = {Efficient Few-Shot Learning Without Prompts}, publisher = {arXiv}, year = {2022}, copyright = {Creative Commons Attribution 4.0 International} }    2080\n",
      "@inproceedings{reimers-2019-sentence-bert, title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", author = \"Reimers, Nils and Gurevych, Iryna\", booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\", month = \"11\", year = \"2019\", publisher = \"Association for Computational Linguistics\", url = \"https://arxiv.org/abs/1908.10084\", }                                                                                                                                                              1244\n",
      "@article{pratap2023mms, title={Scaling Speech Technology to 1,000+ Languages}, author={Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and Ali Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli}, journal={arXiv}, year={2023} }                                                                                                                                                              1173\n",
      "@article{chang-etal-2024-goldfish, title={Goldfish: Monolingual Language Models for 350 Languages}, author={Chang, Tyler A. and Arnett, Catherine and Tu, Zhuowen and Bergen, Benjamin K.}, journal={Preprint}, year={2024}, url={https://www.arxiv.org/abs/2408.10441}, }                                                                                                                                                                                                                                                                                              1154\n",
      "@misc{rw2019timm, author = {Ross Wightman}, title = {PyTorch Image Models}, year = {2019}, publisher = {GitHub}, journal = {GitHub repository}, doi = {10.5281/zenodo.4414861}, howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}} }                                                                                                                                                                                                                                                                                                             915\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ... \n",
      "@article{clavie2024jacolbertv2, title={JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create State-of-the-Art Japanese Retrievers with Constrained Resources}, author={Clavi{\\'e}, Benjamin}, journal={arXiv preprint arXiv:2407.20750}, year={2024} }                                                                                                                                                                                                                                                                                                               2\n",
      "@inproceedings{mroczkowski-etal-2021-herbert, title = \"{H}er{BERT}: Efficiently Pretrained Transformer-based Language Model for {P}olish\", author = \"Mroczkowski, Robert  and Rybak, Piotr  and Wr{\\\\'o}blewska, Alina  and Gawlik, Ireneusz\", booktitle = \"Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing\", month = apr, year = \"2021\", address = \"Kiyv, Ukraine\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/2021.bsnlp-1.1\", pages = \"1--10\", }                                      2\n",
      "@InProceedings{10.1007/978-3-030-83527-9_17, author={Straka, Milan and N{\\'a}plava, Jakub and Strakov{\\'a}, Jana and Samuel, David}, editor={Ek{\\v{s}}tein, Kamil and P{\\'a}rtl, Franti{\\v{s}}ek and Konop{\\'i}k, Miloslav}, title={{RobeCzech: Czech RoBERTa, a Monolingual Contextualized Language Representation Model}}, booktitle=\"Text, Speech, and Dialogue\", year=\"2021\", publisher=\"Springer International Publishing\", address=\"Cham\", pages=\"197--209\", isbn=\"978-3-030-83527-9\" }                                                                              2\n",
      "@article{metalm, title={Language Models are General-Purpose Interfaces}, author={Yaru Hao and Haoyu Song and Li Dong and Shaohan Huang and Zewen Chi and Wenhui Wang and Shuming Ma and Furu Wei}, journal={ArXiv}, year={2022}, volume={abs/2206.06336} }                                                                                                                                                                                                                                                                                                                 2\n",
      "@misc{mast3r_arxiv24, title={Grounding Image Matching in 3D with MASt3R}, author={Vincent Leroy and Yohann Cabon and Jerome Revaud}, year={2024}, eprint={2406.09756}, archivePrefix={arXiv}, primaryClass={cs.CV} }                                                                                                                                                                                                                                                                                                                                                       2\n",
      "Length: 3227, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics\n",
    "bibtex_total_ratio = len(df_split_temp[df_split_temp['contains_bibtex']]) / len(df_split_temp)\n",
    "print(f\"BibTeX presence ratio: {bibtex_total_ratio:.6f}\")\n",
    "# Count occurrences for specific BibTeX counts\n",
    "bibtex_count_distribution = df_split_temp['bibtex_count'].value_counts()\n",
    "print(\"BibTeX Count Distribution:\")\n",
    "print(bibtex_count_distribution)\n",
    "# Check first five entries with exactly 2 BibTeX records\n",
    "bibtex_2_entries = df_split_temp[df_split_temp['bibtex_count'] == 2][['parsed_bibtex_entries']].head(5)\n",
    "print(\"First five entries with exactly 2 BibTeX records:\")\n",
    "print(bibtex_2_entries)\n",
    "# Identify duplicate BibTeX entries\n",
    "bibtex_flat_list = [entry['entry'] for entries in df_split_temp['parsed_bibtex_entries'] for entry in entries]\n",
    "bibtex_duplicates = pd.Series(bibtex_flat_list).value_counts()\n",
    "print(\"Duplicate BibTeX Entries:\")\n",
    "print(bibtex_duplicates[bibtex_duplicates > 1])\n",
    "# 0.0177297320698186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@misc{qwen2.5, title = {Qwen2.5: A Party of Foundation Models}, url = {https://qwenlm.github.io/blog/qwen2.5/}, author = {Qwen Team}, month = {September}, year = {2024} }', '@article{qwen2, title={Qwen2 Technical Report}, author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan}, journal={arXiv preprint arXiv:2407.10671}, year={2024} }']\n",
      "[{'type': 'article', 'title': None, 'entry': '@article{DBLP:journals/corr/abs-1810-04805, author    = {Jacob Devlin and Ming{-}Wei Chang and Kenton Lee and Kristina Toutanova}, title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding}, journal   = {CoRR}, volume    = {abs/1810.04805}, year      = {2018}, url       = {http://arxiv.org/abs/1810.04805}, archivePrefix = {arXiv}, eprint    = {1810.04805}, timestamp = {Tue, 30 Oct 2018 20:39:56 +0100}, biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib}, bibsource = {dblp computer science bibliography, https://dblp.org} }'}]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# test extraction correctness\n",
    "print(df_split_temp[df_split_temp['bibtex_count']==2]['all_extracted_bibtex'].iloc[0])\n",
    "print(df_split_temp[df_split_temp['bibtex_count']==1]['parsed_bibtex_entries'].iloc[0])\n",
    "print(df_split_temp[df_split_temp['bibtex_count']==1]['bibtex_count'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>author</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>library_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>card</th>\n",
       "      <th>...</th>\n",
       "      <th>card_tags_datasets</th>\n",
       "      <th>analysis_keyword_status</th>\n",
       "      <th>detected_keywords</th>\n",
       "      <th>is_default_card</th>\n",
       "      <th>contains_markdown_table</th>\n",
       "      <th>extracted_markdown_table</th>\n",
       "      <th>contains_bibtex</th>\n",
       "      <th>all_extracted_bibtex</th>\n",
       "      <th>parsed_bibtex_entries</th>\n",
       "      <th>bibtex_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>2024-11-05 15:25:48+00:00</td>\n",
       "      <td>391757489</td>\n",
       "      <td>874</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>[sentence-transformers, pytorch, onnx, safeten...</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...</td>\n",
       "      <td>...</td>\n",
       "      <td>s2orc, flax-sentence-embeddings/stackexchange_...</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Dataset                                     ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nesaorg/benchmark_v0</td>\n",
       "      <td>nesaorg</td>\n",
       "      <td>2024-08-19 18:24:49+00:00</td>\n",
       "      <td>98012579</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>[safetensors, model_hub_mixin, pytorch_model_h...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-13</td>\n",
       "      <td>---\\ntags:\\n- model_hub_mixin\\n- pytorch_model...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>2024-11-01 10:26:30+00:00</td>\n",
       "      <td>74110727</td>\n",
       "      <td>2526</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>[sentence-transformers, pytorch, tf, rust, onn...</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...</td>\n",
       "      <td>...</td>\n",
       "      <td>s2orc, flax-sentence-embeddings/stackexchange_...</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Dataset                                     ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nesaorg/fc_8</td>\n",
       "      <td>nesaorg</td>\n",
       "      <td>2024-08-14 12:56:48+00:00</td>\n",
       "      <td>28123640</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[safetensors, model_hub_mixin, pytorch_model_h...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-14</td>\n",
       "      <td>---\\ntags:\\n- model_hub_mixin\\n- pytorch_model...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>openai/clip-vit-base-patch32</td>\n",
       "      <td>openai</td>\n",
       "      <td>2024-02-29 09:45:55+00:00</td>\n",
       "      <td>26745969</td>\n",
       "      <td>534</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, clip, zero-sh...</td>\n",
       "      <td>zero-shot-image-classification</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\ntags:\\n- vision\\nwidget:\\n- src: https://...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108754</th>\n",
       "      <td>barchetta/baco-131233</td>\n",
       "      <td>barchetta</td>\n",
       "      <td>2024-11-13 01:33:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>Entry not found</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108755</th>\n",
       "      <td>saqqdy/Qwen-Qwen1.5-0.5B-1731461591</td>\n",
       "      <td>saqqdy</td>\n",
       "      <td>2024-11-13 01:33:19+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>peft</td>\n",
       "      <td>[peft, safetensors, arxiv:1910.09700, base_mod...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: Qwen/Qwen1.5-0.5B\\nlibrary_na...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108756</th>\n",
       "      <td>minhaozhang/Llama-3.2-1B-Instruct-MBTI-JP</td>\n",
       "      <td>minhaozhang</td>\n",
       "      <td>2024-11-13 01:33:17+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>Entry not found</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108757</th>\n",
       "      <td>mradermacher/Mistral-quiet-star-demo-GGUF</td>\n",
       "      <td>mradermacher</td>\n",
       "      <td>2024-11-13 01:33:42+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: liminerity/Mistral-quiet-star...</td>\n",
       "      <td>...</td>\n",
       "      <td>gate369/Alpaca-Star</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Link | Type | Size/GB | Notes |\\n|:-----|:--...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108758</th>\n",
       "      <td>huyhoangt2201/llama-3.2-1b-chat-sql3-merged</td>\n",
       "      <td>huyhoangt2201</td>\n",
       "      <td>2024-11-13 01:34:12+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nlibrary_name: transformers\\ntags: []\\n---...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1074055 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             modelId                 author  \\\n",
       "0            sentence-transformers/all-mpnet-base-v2  sentence-transformers   \n",
       "1                               nesaorg/benchmark_v0                nesaorg   \n",
       "2             sentence-transformers/all-MiniLM-L6-v2  sentence-transformers   \n",
       "6                                       nesaorg/fc_8                nesaorg   \n",
       "7                       openai/clip-vit-base-patch32                 openai   \n",
       "...                                              ...                    ...   \n",
       "1108754                        barchetta/baco-131233              barchetta   \n",
       "1108755          saqqdy/Qwen-Qwen1.5-0.5B-1731461591                 saqqdy   \n",
       "1108756    minhaozhang/Llama-3.2-1B-Instruct-MBTI-JP            minhaozhang   \n",
       "1108757    mradermacher/Mistral-quiet-star-demo-GGUF           mradermacher   \n",
       "1108758  huyhoangt2201/llama-3.2-1b-chat-sql3-merged          huyhoangt2201   \n",
       "\n",
       "                    last_modified  downloads  likes           library_name  \\\n",
       "0       2024-11-05 15:25:48+00:00  391757489    874  sentence-transformers   \n",
       "1       2024-08-19 18:24:49+00:00   98012579      1                   None   \n",
       "2       2024-11-01 10:26:30+00:00   74110727   2526  sentence-transformers   \n",
       "6       2024-08-14 12:56:48+00:00   28123640      0                   None   \n",
       "7       2024-02-29 09:45:55+00:00   26745969    534           transformers   \n",
       "...                           ...        ...    ...                    ...   \n",
       "1108754 2024-11-13 01:33:00+00:00          0      0                   None   \n",
       "1108755 2024-11-13 01:33:19+00:00          0      0                   peft   \n",
       "1108756 2024-11-13 01:33:17+00:00          0      0                   None   \n",
       "1108757 2024-11-13 01:33:42+00:00          0      0                   None   \n",
       "1108758 2024-11-13 01:34:12+00:00          0      0                   None   \n",
       "\n",
       "                                                      tags  \\\n",
       "0        [sentence-transformers, pytorch, onnx, safeten...   \n",
       "1        [safetensors, model_hub_mixin, pytorch_model_h...   \n",
       "2        [sentence-transformers, pytorch, tf, rust, onn...   \n",
       "6        [safetensors, model_hub_mixin, pytorch_model_h...   \n",
       "7        [transformers, pytorch, tf, jax, clip, zero-sh...   \n",
       "...                                                    ...   \n",
       "1108754                                        [region:us]   \n",
       "1108755  [peft, safetensors, arxiv:1910.09700, base_mod...   \n",
       "1108756                                        [region:us]   \n",
       "1108757                                        [region:us]   \n",
       "1108758                                        [region:us]   \n",
       "\n",
       "                           pipeline_tag   createdAt  \\\n",
       "0                   sentence-similarity  2022-03-02   \n",
       "1                                  None  2024-08-13   \n",
       "2                   sentence-similarity  2022-03-02   \n",
       "6                                  None  2024-08-14   \n",
       "7        zero-shot-image-classification  2022-03-02   \n",
       "...                                 ...         ...   \n",
       "1108754                            None  2024-11-13   \n",
       "1108755                            None  2024-11-13   \n",
       "1108756                            None  2024-11-13   \n",
       "1108757                            None  2024-11-13   \n",
       "1108758                            None  2024-11-13   \n",
       "\n",
       "                                                      card  ...  \\\n",
       "0        ---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...  ...   \n",
       "1        ---\\ntags:\\n- model_hub_mixin\\n- pytorch_model...  ...   \n",
       "2        ---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...  ...   \n",
       "6        ---\\ntags:\\n- model_hub_mixin\\n- pytorch_model...  ...   \n",
       "7        ---\\ntags:\\n- vision\\nwidget:\\n- src: https://...  ...   \n",
       "...                                                    ...  ...   \n",
       "1108754                                    Entry not found  ...   \n",
       "1108755  ---\\nbase_model: Qwen/Qwen1.5-0.5B\\nlibrary_na...  ...   \n",
       "1108756                                    Entry not found  ...   \n",
       "1108757  ---\\nbase_model: liminerity/Mistral-quiet-star...  ...   \n",
       "1108758  ---\\nlibrary_name: transformers\\ntags: []\\n---...  ...   \n",
       "\n",
       "                                        card_tags_datasets  \\\n",
       "0        s2orc, flax-sentence-embeddings/stackexchange_...   \n",
       "1                                                     None   \n",
       "2        s2orc, flax-sentence-embeddings/stackexchange_...   \n",
       "6                                                     None   \n",
       "7                                                     None   \n",
       "...                                                    ...   \n",
       "1108754                                               None   \n",
       "1108755                                               None   \n",
       "1108756                                               None   \n",
       "1108757                                gate369/Alpaca-Star   \n",
       "1108758                                               None   \n",
       "\n",
       "        analysis_keyword_status detected_keywords is_default_card  \\\n",
       "0                          none                []           False   \n",
       "1                          none                []           False   \n",
       "2                          none                []           False   \n",
       "6                          none                []           False   \n",
       "7                          none                []           False   \n",
       "...                         ...               ...             ...   \n",
       "1108754                    none                []           False   \n",
       "1108755                    none                []            True   \n",
       "1108756                    none                []           False   \n",
       "1108757                    none                []           False   \n",
       "1108758                    none                []            True   \n",
       "\n",
       "        contains_markdown_table  \\\n",
       "0                          True   \n",
       "1                         False   \n",
       "2                          True   \n",
       "6                         False   \n",
       "7                         False   \n",
       "...                         ...   \n",
       "1108754                   False   \n",
       "1108755                   False   \n",
       "1108756                   False   \n",
       "1108757                    True   \n",
       "1108758                   False   \n",
       "\n",
       "                                  extracted_markdown_table contains_bibtex  \\\n",
       "0        | Dataset                                     ...           False   \n",
       "1                                                     None           False   \n",
       "2        | Dataset                                     ...           False   \n",
       "6                                                     None           False   \n",
       "7                                                     None           False   \n",
       "...                                                    ...             ...   \n",
       "1108754                                               None           False   \n",
       "1108755                                               None           False   \n",
       "1108756                                               None           False   \n",
       "1108757  | Link | Type | Size/GB | Notes |\\n|:-----|:--...           False   \n",
       "1108758                                               None           False   \n",
       "\n",
       "        all_extracted_bibtex parsed_bibtex_entries bibtex_count  \n",
       "0                         []                    []            0  \n",
       "1                         []                    []            0  \n",
       "2                         []                    []            0  \n",
       "6                         []                    []            0  \n",
       "7                         []                    []            0  \n",
       "...                      ...                   ...          ...  \n",
       "1108754                   []                    []            0  \n",
       "1108755                   []                    []            0  \n",
       "1108756                   []                    []            0  \n",
       "1108757                   []                    []            0  \n",
       "1108758                   []                    []            0  \n",
       "\n",
       "[1074055 rows x 30 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "df_split_temp[df_split_temp['bibtex_count']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries containing '@' but not valid BibTeX:\n",
      "Index(['<p><h1>ðŸ‹ Mistral-7B-OpenOrca ðŸ‹</h1></p>\\n\\n\\n![OpenOrca Logo](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca/resolve/main/Images/MistralOrcaLogo.png \"MistralOrca Logo\")\\n[<img src=\"https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/OpenAccess-AI-Collective/axolotl)\\n\\n\\n# OpenOrca - Mistral - 7B - 8k\\n\\nWe have used our own [OpenOrca dataset](https://huggingface.co/datasets/Open-Orca/OpenOrca) to fine-tune on top of [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1). \\nThis dataset is our attempt to reproduce the dataset generated for Microsoft Research's [Orca Paper](https://arxiv.org/abs/2306.02707).\\nWe use [OpenChat](https://huggingface.co/openchat) packing, trained with [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl).\\n\\nThis release is trained on a curated filtered subset of most of our GPT-4 augmented data.\\nIt is the same subset of our data as was used in our [OpenOrcaxOpenChat-Preview2-13B model](https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B).\\n\\n**HF Leaderboard evals place this model as #2 for all models smaller than 30B at release time, outperforming all but one 13B model.**\\n\\nThis release provides a first: a fully open model with class-breaking performance, capable of running fully accelerated on even moderate consumer GPUs.\\nOur thanks to the Mistral team for leading the way here. \\n\\nWe affectionately codename this model: \"*MistralOrca*\"\\n\\nIf you'd like to try the model now, we have it running on fast GPUs unquantized: https://huggingface.co/spaces/Open-Orca/Mistral-7B-OpenOrca\\n\\nWant to visualize our full (pre-filtering) dataset? Check out our [Nomic Atlas Map](https://atlas.nomic.ai/map/c1b88b47-2d9b-47e0-9002-b80766792582/2560fd25-52fe-42f1-a58f-ff5eccc890d2).\\n\\n[<img src=\"https://huggingface.co/Open-Orca/OpenOrca-Preview1-13B/resolve/main/OpenOrca%20Nomic%20Atlas.png\" alt=\"Atlas Nomic Dataset Map\" width=\"400\" height=\"400\" />](https://atlas.nomic.ai/map/c1b88b47-2d9b-47e0-9002-b80766792582/2560fd25-52fe-42f1-a58f-ff5eccc890d2)\\n\\n\\nWe are in-process with training more models, so keep a look out on our org for releases coming soon with exciting partners.\\n\\nWe will also give sneak-peak announcements on our Discord, which you can find here:\\n\\nhttps://AlignmentLab.ai\\n\\nor check the OpenAccess AI Collective Discord for more information about Axolotl trainer here:\\n\\nhttps://discord.gg/5y8STgB3P3\\n\\n\\n# Quantized Models\\n\\nQuantized versions of this model are generously made available by [TheBloke](https://huggingface.co/TheBloke).\\n\\n- AWQ: https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ\\n- GPTQ: https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ\\n- GGUF: https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF\\n\\n\\n# Prompt Template\\n\\nWe used [OpenAI's Chat Markup Language (ChatML)](https://github.com/openai/openai-python/blob/main/chatml.md) format, with `<|im_start|>` and `<|im_end|>` tokens added to support this.\\n\\nThis means that, e.g., in [oobabooga](https://github.com/oobabooga/text-generation-webui/) the \"`MPT-Chat`\" instruction template should work, as it also uses ChatML.\\n\\n## Example Prompt Exchange\\n\\n```\\n<|im_start|>system\\nYou are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\\n<|im_end|>\\n<|im_start|>user\\nHow are you?<|im_end|>\\n<|im_start|>assistant\\nI am doing well!<|im_end|>\\n<|im_start|>user\\nPlease tell me about how mistral winds have attracted super-orcas.<|im_end|>\\n```\\n\\n\\n# Inference\\n\\nSee [this notebook](https://colab.research.google.com/drive/1yZlLSifCGELAX5GN582kZypHCv0uJuNX?usp=sharing) for inference details.\\n\\nNote that you need the development snapshot of Transformers currently, as support for Mistral hasn't been released into PyPI yet:\\n\\n```\\npip install git+https://github.com/huggingface/transformers\\n```\\n\\n\\n# Evaluation\\n\\n## HuggingFace Leaderboard Performance\\n\\nWe have evaluated using the methodology and tools for the HuggingFace Leaderboard, and find that we have dramatically improved upon the base model.\\nWe find **105%** of the base model's performance on HF Leaderboard evals, averaging **65.33**.\\n\\nAt release time, this beats all 7B models, and all but one 13B.\\n\\n![HF Leaderboard](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca/resolve/main/Images/MistralOrca7BHFLeaderboard.png)\\n\\n\\n| Metric | Value |\\n|-----------------------|-------|\\n| MMLU (5-shot)         | 61.73 |\\n| ARC (25-shot)         | 63.57 |\\n| HellaSwag (10-shot)   | 83.79 |\\n| TruthfulQA (0-shot)   | 52.24 |\\n| Avg.                  | 65.33 |\\n\\nWe use [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) to run the benchmark tests above, using the same version as the HuggingFace LLM Leaderboard.\\n\\n\\n## AGIEval Performance\\n\\nWe compare our results to the base Mistral-7B model (using LM Evaluation Harness).\\n\\nWe find **129%** of the base model's performance on AGI Eval, averaging **0.397**.\\nAs well, we significantly improve upon the official `mistralai/Mistral-7B-Instruct-v0.1` finetuning, achieving **119%** of their performance.\\n\\n![OpenOrca-Platypus2-13B AGIEval Performance](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca/resolve/main/Images/MistralOrca7BAGIEval.png \"AGIEval Performance\")\\n\\n## BigBench-Hard Performance\\n\\nWe find **119%** of the base model's performance on BigBench-Hard, averaging **0.416**.\\n\\n![OpenOrca-Platypus2-13B BigBench-Hard Performance](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca/resolve/main/Images/MistralOrca7BBigBenchHard.png \"BigBench-Hard Performance\")\\n\\n\\n# Dataset\\n\\nWe used a curated, filtered selection of most of the GPT-4 augmented data from our OpenOrca dataset, which aims to reproduce the Orca Research Paper dataset.\\n\\n\\n# Training\\n\\nWe trained with 8x A6000 GPUs for 62 hours, completing 4 epochs of full fine tuning on our dataset in one training run.\\nCommodity cost was ~$400.\\n\\n\\n# Citation\\n\\n```bibtex\\n@software{lian2023mistralorca1\\n  title = {MistralOrca: Mistral-7B Model Instruct-tuned on Filtered OpenOrcaV1 GPT-4 Dataset},\\n  author = {Wing Lian and Bleys Goodson and Guan Wang and Eugene Pentland and Austin Cook and Chanvichet Vong and \"Teknium\"},\\n  year = {2023},\\n  publisher = {HuggingFace},\\n  journal = {HuggingFace repository},\\n  howpublished = {\\url{https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca},\\n}\\n@misc{mukherjee2023orca,\\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \\n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\\n      year={2023},\\n      eprint={2306.02707},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n@misc{longpre2023flan,\\n      title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}, \\n      author={Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},\\n      year={2023},\\n      eprint={2301.13688},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI}\\n}\\n```',\n",
      "       '# 3UTRBERT\\n\\nPre-trained model on 3â€™ untranslated region (3â€™UTR) using a masked language modeling (MLM) objective.\\n\\n## Disclaimer\\n\\nThis is an UNOFFICIAL implementation of the [Deciphering 3â€™ UTR mediated gene regulation using interpretable deep representation learning](https://doi.org/10.1101/2023.09.08.556883) by Yuning Yang, Gen Li, et al.\\n\\nThe OFFICIAL repository of 3UTRBERT is at [yangyn533/3UTRBERT](https://github.com/yangyn533/3UTRBERT).\\n\\n> [!TIP]\\n> The MultiMolecule team has confirmed that the provided model and checkpoints are producing the same intermediate representations as the original implementation.\\n\\n**The team releasing 3UTRBERT did not write this model card for this model so this model card has been written by the MultiMolecule team.**\\n\\n## Model Details\\n\\n3UTRBERT is a [bert](https://huggingface.co/google-bert/bert-base-uncased)-style model pre-trained on a large corpus of 3â€™ untranslated regions (3â€™UTRs) in a self-supervised fashion. This means that the model was trained on the raw nucleotides of RNA sequences only, with an automatic process to generate inputs and labels from those texts. Please refer to the [Training Details](#training-details) section for more information on the training process.\\n\\n### Variations\\n\\n- **[`multimolecule/utrbert-3mer`](https://huggingface.co/multimolecule/utrbert-3mer)**: The 3UTRBERT model pre-trained on 3-mer data.\\n- **[`multimolecule/utrbert-4mer`](https://huggingface.co/multimolecule/utrbert-4mer)**: The 3UTRBERT model pre-trained on 4-mer data.\\n- **[`multimolecule/utrbert-5mer`](https://huggingface.co/multimolecule/utrbert-5mer)**: The 3UTRBERT model pre-trained on 5-mer data.\\n- **[`multimolecule/utrbert-6mer`](https://huggingface.co/multimolecule/utrbert-6mer)**: The 3UTRBERT model pre-trained on 6-mer data.\\n\\n### Model Specification\\n\\n<table>\\n<thead>\\n  <tr>\\n    <th>Variants</th>\\n    <th>Num Layers</th>\\n    <th>Hidden Size</th>\\n    <th>Num Heads</th>\\n    <th>Intermediate Size</th>\\n    <th>Num Parameters (M)</th>\\n    <th>FLOPs (G)</th>\\n    <th>MACs (G)</th>\\n    <th>Max Num Tokens</th>\\n  </tr>\\n</thead>\\n<tbody>\\n  <tr>\\n    <td>UTRBERT-3mer</td>\\n    <td rowspan=\"4\">12</td>\\n    <td rowspan=\"4\">768</td>\\n    <td rowspan=\"4\">12</td>\\n    <td rowspan=\"4\">3072</td>\\n    <td>86.14</td>\\n    <td rowspan=\"4\">22.36</td>\\n    <td rowspan=\"4\">11.17</td>\\n    <td rowspan=\"4\">512</td>\\n  </tr>\\n  <tr>\\n    <td>UTRBERT-4mer</td>\\n    <td>86.53</td>\\n  </tr>\\n  <tr>\\n    <td>UTRBERT-5mer</td>\\n    <td>88.45</td>\\n  </tr>\\n  <tr>\\n    <td>UTRBERT-6mer</td>\\n    <td>98.05</td>\\n  </tr>\\n</tbody>\\n</table>\\n\\n### Links\\n\\n- **Code**: [multimolecule.utrbert](https://github.com/DLS5-Omics/multimolecule/tree/master/multimolecule/models/utrbert)\\n- **Data**: [GENCODE](https://gencodegenes.org)\\n- **Paper**: [Deciphering 3â€™ UTR mediated gene regulation using interpretable deep representation learning](https://doi.org/10.1101/2023.09.08.556883)\\n- **Developed by**: Yuning Yang, Gen Li, Kuan Pang, Wuxinhao Cao, Xiangtao Li, Zhaolei Zhang\\n- **Model type**: [BERT](https://huggingface.co/google-bert/bert-base-uncased) - [FlashAttention](https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention)\\n- **Original Repository**: [https://github.com/yangyn533/3UTRBERT](https://github.com/yangyn533/3UTRBERT)\\n\\n## Usage\\n\\nThe model file depends on the [`multimolecule`](https://multimolecule.danling.org) library. You can install it using pip:\\n\\n```bash\\npip install multimolecule\\n```\\n\\n### Direct Use\\n\\n**Note**: Default transformers pipeline does not support K-mer tokenization.\\n\\nYou can use this model directly with a pipeline for masked language modeling:\\n\\n```python\\n>>> import multimolecule  # you must import multimolecule to register models\\n>>> from transformers import pipeline\\n>>> unmasker = pipeline('fill-mask', model='multimolecule/utrbert-3mer')\\n>>> unmasker(\"uag<mask><mask><mask>cagacugauguuga\")[1]\\n\\n[{'score': 0.6499986052513123,\\n  'token': 57,\\n  'token_str': 'GAC',\\n  'sequence': '<cls> UAG <mask> GAC <mask> CAG AGA GAC ACU CUG UGA GAU AUG UGU GUU UUG UGA <eos>'},\\n {'score': 0.07012350112199783,\\n  'token': 72,\\n  'token_str': 'GUC',\\n  'sequence': '<cls> UAG <mask> GUC <mask> CAG AGA GAC ACU CUG UGA GAU AUG UGU GUU UUG UGA <eos>'},\\n {'score': 0.06567499041557312,\\n  'token': 32,\\n  'token_str': 'CAC',\\n  'sequence': '<cls> UAG <mask> CAC <mask> CAG AGA GAC ACU CUG UGA GAU AUG UGU GUU UUG UGA <eos>'},\\n {'score': 0.06494498997926712,\\n  'token': 62,\\n  'token_str': 'GCC',\\n  'sequence': '<cls> UAG <mask> GCC <mask> CAG AGA GAC ACU CUG UGA GAU AUG UGU GUU UUG UGA <eos>'},\\n {'score': 0.06052926927804947,\\n  'token': 67,\\n  'token_str': 'GGC',\\n  'sequence': '<cls> UAG <mask> GGC <mask> CAG AGA GAC ACU CUG UGA GAU AUG UGU GUU UUG UGA <eos>'}]\\n```\\n\\n### Downstream Use\\n\\n#### Extract Features\\n\\nHere is how to use this model to get the features of a given sequence in PyTorch:\\n\\n```python\\nfrom multimolecule import RnaTokenizer, UtrBertModel\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrbert-3mer')\\nmodel = UtrBertModel.from_pretrained('multimolecule/utrbert-3mer')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\n\\noutput = model(**input)\\n```\\n\\n#### Sequence Classification / Regression\\n\\n**Note**: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for sequence classification or regression.\\n\\nHere is how to use this model as backbone to fine-tune for a sequence-level task in PyTorch:\\n\\n```python\\nimport torch\\nfrom multimolecule import RnaTokenizer, UtrBertForSequencePrediction\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrbert-3mer')\\nmodel = UtrBertForSequencePrediction.from_pretrained('multimolecule/utrbert-3mer')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\nlabel = torch.tensor([1])\\n\\noutput = model(**input, labels=label)\\n```\\n\\n#### Nucleotide Classification / Regression\\n\\n**Note**: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for nucleotide classification or regression.\\n\\nHere is how to use this model as backbone to fine-tune for a nucleotide-level task in PyTorch:\\n\\n```python\\nimport torch\\nfrom multimolecule import RnaTokenizer, UtrBertForNucleotidePrediction\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrbert-3mer')\\nmodel = UtrBertForNucleotidePrediction.from_pretrained('multimolecule/utrbert-3mer')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\nlabel = torch.randint(2, (len(text), ))\\n\\noutput = model(**input, labels=label)\\n```\\n\\n#### Contact Classification / Regression\\n\\n**Note**: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for contact classification or regression.\\n\\nHere is how to use this model as backbone to fine-tune for a contact-level task in PyTorch:\\n\\n```python\\nimport torch\\nfrom multimolecule import RnaTokenizer, UtrBertForContactPrediction\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrbert')\\nmodel = UtrBertForContactPrediction.from_pretrained('multimolecule/utrbert')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\nlabel = torch.randint(2, (len(text), len(text)))\\n\\noutput = model(**input, labels=label)\\n```\\n\\n## Training Details\\n\\n3UTRBERT used Masked Language Modeling (MLM) as the pre-training objective: taking a sequence, the model randomly masks 15% of the tokens in the input then runs the entire masked sentence through the model and has to predict the masked tokens. This is comparable to the Cloze task in language modeling.\\n\\n### Training Data\\n\\nThe 3UTRBERT model was pre-trained on human mRNA transcript sequences from [GENCODE](https://gencodegenes.org).\\nGENCODE aims to identify all gene features in the human genome using a combination of computational analysis, manual annotation, and experimental validation. The GENCODE release 40 used by this work contains 61,544 genes, and 246,624 transcripts.\\n\\n3UTRBERT collected the human mRNA transcript sequences from GENCODE, including 108,573 unique mRNA transcripts. Only the longest transcript of each gene was used in the pre-training process. 3UTRBERT only used the 3â€™ untranslated regions (3â€™UTRs) of the mRNA transcripts for pre-training to avoid codon constrains in the CDS region, and to reduce increased complexity of the entire mRNA transcripts. The average length of the 3â€™UTRs was 1,227 nucleotides, while the median length was 631 nucleotides. Each 3â€™UTR sequence was cut to non-overlapping patches of 510 nucleotides. The remaining sequences were padded to the same length.\\n\\nNote [`RnaTokenizer`][multimolecule.RnaTokenizer] will convert \"T\"s to \"U\"s for you, you may disable this behaviour by passing `replace_T_with_U=False`.\\n\\n### Training Procedure\\n\\n#### Preprocessing\\n\\n3UTRBERT used masked language modeling (MLM) as the pre-training objective. The masking procedure is similar to the one used in BERT:\\n\\n- 15% of the tokens are masked.\\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\\n- In the 10% remaining cases, the masked tokens are left as is.\\n\\nSince 3UTRBERT used k-mer tokenizer, it masks the entire k-mer instead of individual nucleotides to avoid information leakage.\\n\\nFor example, if the k-mer is 3, the sequence `\"UAGCGUAU\"` will be tokenized as `[\"UAG\", \"AGC\", \"GCG\", \"CGU\", \"GUA\", \"UAU\"]`. If the nucleotide `\"C\"` is masked, the adjacent tokens will also be masked, resulting `[\"UAG\", \"<mask>\", \"<mask>\", \"<mask>\", \"GUA\", \"UAU\"]`.\\n\\n#### PreTraining\\n\\nThe model was trained on 4 NVIDIA Quadro RTX 6000 GPUs with 24GiB memories.\\n\\n- Batch size: 128\\n- Learning rate: 3e-4\\n- Weight decay: 0.01\\n- Optimizer: AdamW(Î²1=0.9, Î²2=0.98, e=1e-6)\\n- Steps: 200,000\\n- Learning rate scheduler: Linear\\n- Learning rate warm-up: 10,000 steps\\n\\n## Citation\\n\\n**BibTeX**:\\n\\n```bibtex\\n@article {yang2023deciphering,\\n\\tauthor = {Yang, Yuning and Li, Gen and Pang, Kuan and Cao, Wuxinhao and Li, Xiangtao and Zhang, Zhaolei},\\n\\ttitle = {Deciphering 3{\\textquoteright} UTR mediated gene regulation using interpretable deep representation learning},\\n\\telocation-id = {2023.09.08.556883},\\n\\tyear = {2023},\\n\\tdoi = {10.1101/2023.09.08.556883},\\n\\tpublisher = {Cold Spring Harbor Laboratory},\\n\\tabstract = {The 3{\\textquoteright}untranslated regions (3{\\textquoteright}UTRs) of messenger RNAs contain many important cis-regulatory elements that are under functional and evolutionary constraints. We hypothesize that these constraints are similar to grammars and syntaxes in human languages and can be modeled by advanced natural language models such as Transformers, which has been very effective in modeling protein sequence and structures. Here we describe 3UTRBERT, which implements an attention-based language model, i.e., Bidirectional Encoder Representations from Transformers (BERT). 3UTRBERT was pre-trained on aggregated 3{\\textquoteright}UTR sequences of human mRNAs in a task-agnostic manner; the pre-trained model was then fine-tuned for specific downstream tasks such as predicting RBP binding sites, m6A RNA modification sites, and predicting RNA sub-cellular localizations. Benchmark results showed that 3UTRBERT generally outperformed other contemporary methods in each of these tasks. We also showed that the self-attention mechanism within 3UTRBERT allows direct visualization of the semantic relationship between sequence elements.Competing Interest StatementThe authors have declared no competing interest.},\\n\\tURL = {https://www.biorxiv.org/content/early/2023/09/12/2023.09.08.556883},\\n\\teprint = {https://www.biorxiv.org/content/early/2023/09/12/2023.09.08.556883.full.pdf},\\n\\tjournal = {bioRxiv}\\n}\\n```\\n\\n## Contact\\n\\nPlease use GitHub issues of [MultiMolecule](https://github.com/DLS5-Omics/multimolecule/issues) for any questions or comments on the model card.\\n\\nPlease contact the authors of the [3UTRBERT paper](https://doi.org/10.1101/2023.09.08.556883) for questions or comments on the paper/model.\\n\\n## License\\n\\nThis model is licensed under the [AGPL-3.0 License](https://www.gnu.org/licenses/agpl-3.0.html).\\n\\n```spdx\\nSPDX-License-Identifier: AGPL-3.0-or-later\\n```',\n",
      "       '# SpliceBERT\\n\\nPre-trained model on messenger RNA precursor (pre-mRNA) using a masked language modeling (MLM) objective.\\n\\n## Disclaimer\\n\\nThis is an UNOFFICIAL implementation of the [Self-supervised learning on millions of pre-mRNA sequences improves sequence-based RNA splicing prediction](https://doi.org/10.1101/2023.01.31.526427) by Ken Chen, et al.\\n\\nThe OFFICIAL repository of SpliceBERT is at [chenkenbio/SpliceBERT](https://github.com/chenkenbio/SpliceBERT).\\n\\n> [!TIP]\\n> The MultiMolecule team has confirmed that the provided model and checkpoints are producing the same intermediate representations as the original implementation.\\n\\n**The team releasing SpliceBERT did not write this model card for this model so this model card has been written by the MultiMolecule team.**\\n\\n## Model Details\\n\\nSpliceBERT is a [bert](https://huggingface.co/google-bert/bert-base-uncased)-style model pre-trained on a large corpus of messenger RNA precursor sequences in a self-supervised fashion. This means that the model was trained on the raw nucleotides of RNA sequences only, with an automatic process to generate inputs and labels from those texts. Please refer to the [Training Details](#training-details) section for more information on the training process.\\n\\n### Variations\\n\\n- **[`multimolecule/splicebert`](https://huggingface.co/multimolecule/splicebert)**: The SpliceBERT model.\\n- **[`multimolecule/splicebert.510nt`](https://huggingface.co/multimolecule/splicebert.510nt)**: The intermediate SpliceBERT model.\\n- **[`multimolecule/splicebert-human.510nt`](https://huggingface.co/multimolecule/splicebert-human.510nt)**: The intermediate SpliceBERT model pre-trained on human data only.\\n\\n### Model Specification\\n\\n<table>\\n<thead>\\n  <tr>\\n    <th>Variants</th>\\n    <th>Num Layers</th>\\n    <th>Hidden Size</th>\\n    <th>Num Heads</th>\\n    <th>Intermediate Size</th>\\n    <th>Num Parameters (M)</th>\\n    <th>FLOPs (G)</th>\\n    <th>MACs (G)</th>\\n    <th>Max Num Tokens</th>\\n  </tr>\\n</thead>\\n<tbody>\\n  <tr>\\n    <td>splicebert</td>\\n    <td rowspan=\"3\">6</td>\\n    <td rowspan=\"3\">512</td>\\n    <td rowspan=\"3\">16</td>\\n    <td rowspan=\"3\">2048</td>\\n    <td>19.72</td>\\n    <td rowspan=\"3\">5.04</td>\\n    <td rowspan=\"3\">2.52</td>\\n    <td>1024</td>\\n  </tr>\\n  <tr>\\n    <td>splicebert.510nt</td>\\n    <td rowspan=\"2\">19.45</td>\\n    <td rowspan=\"2\">510</td>\\n  </tr>\\n  <tr>\\n    <td>splicebert-human.510nt</td>\\n  </tr>\\n</tbody>\\n</table>\\n\\n### Links\\n\\n- **Code**: [multimolecule.splicebert](https://github.com/DLS5-Omics/multimolecule/tree/master/multimolecule/models/splicebert)\\n- **Data**: [UCSC Genome Browser](https://genome.ucsc.edu)\\n- **Paper**: [Self-supervised learning on millions of pre-mRNA sequences improves sequence-based RNA splicing prediction](https://doi.org/10.1101/2023.01.31.526427)\\n- **Developed by**: Ken Chen, Yue Zhou, Maolin Ding, Yu Wang, Zhixiang Ren, Yuedong Yang\\n- **Model type**: [BERT](https://huggingface.co/google-bert/bert-base-uncased) - [FlashAttention](https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention)\\n- **Original Repository**: [https://github.com/chenkenbio/SpliceBERT](https://github.com/chenkenbio/SpliceBERT)\\n\\n## Usage\\n\\nThe model file depends on the [`multimolecule`](https://multimolecule.danling.org) library. You can install it using pip:\\n\\n```bash\\npip install multimolecule\\n```\\n\\n### Direct Use\\n\\nYou can use this model directly with a pipeline for masked language modeling:\\n\\n```python\\n>>> import multimolecule  # you must import multimolecule to register models\\n>>> from transformers import pipeline\\n>>> unmasker = pipeline('fill-mask', model='multimolecule/splicebert')\\n>>> unmasker(\"uagc<mask>uaucagacugauguuga\")\\n\\n[{'score': 0.09350304305553436,\\n  'token': 6,\\n  'token_str': 'A',\\n  'sequence': 'U A G C A U A U C A G A C U G A U G U U G A'},\\n {'score': 0.08757384121417999,\\n  'token': 14,\\n  'token_str': 'W',\\n  'sequence': 'U A G C W U A U C A G A C U G A U G U U G A'},\\n {'score': 0.08202056586742401,\\n  'token': 9,\\n  'token_str': 'U',\\n  'sequence': 'U A G C U U A U C A G A C U G A U G U U G A'},\\n {'score': 0.07025782763957977,\\n  'token': 19,\\n  'token_str': 'H',\\n  'sequence': 'U A G C H U A U C A G A C U G A U G U U G A'},\\n {'score': 0.06502506136894226,\\n  'token': 16,\\n  'token_str': 'M',\\n  'sequence': 'U A G C M U A U C A G A C U G A U G U U G A'}]\\n```\\n\\n### Downstream Use\\n\\n#### Extract Features\\n\\nHere is how to use this model to get the features of a given sequence in PyTorch:\\n\\n```python\\nfrom multimolecule import RnaTokenizer, SpliceBertModel\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/splicebert')\\nmodel = SpliceBertModel.from_pretrained('multimolecule/splicebert')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\n\\noutput = model(**input)\\n```\\n\\n#### Sequence Classification / Regression\\n\\n**Note**: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for sequence classification or regression.\\n\\nHere is how to use this model as backbone to fine-tune for a sequence-level task in PyTorch:\\n\\n```python\\nimport torch\\nfrom multimolecule import RnaTokenizer, SpliceBertForSequencePrediction\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/splicebert')\\nmodel = SpliceBertForSequencePrediction.from_pretrained('multimolecule/splicebert')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\nlabel = torch.tensor([1])\\n\\noutput = model(**input, labels=label)\\n```\\n\\n#### Nucleotide Classification / Regression\\n\\n**Note**: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for nucleotide classification or regression.\\n\\nHere is how to use this model as backbone to fine-tune for a nucleotide-level task in PyTorch:\\n\\n```python\\nimport torch\\nfrom multimolecule import RnaTokenizer, SpliceBertForNucleotidePrediction\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/splicebert')\\nmodel = SpliceBertForNucleotidePrediction.from_pretrained('multimolecule/splicebert')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\nlabel = torch.randint(2, (len(text), ))\\n\\noutput = model(**input, labels=label)\\n```\\n\\n#### Contact Classification / Regression\\n\\n**Note**: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for contact classification or regression.\\n\\nHere is how to use this model as backbone to fine-tune for a contact-level task in PyTorch:\\n\\n```python\\nimport torch\\nfrom multimolecule import RnaTokenizer, SpliceBertForContactPrediction\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/splicebert')\\nmodel = SpliceBertForContactPrediction.from_pretrained('multimolecule/splicebert')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\nlabel = torch.randint(2, (len(text), len(text)))\\n\\noutput = model(**input, labels=label)\\n```\\n\\n## Training Details\\n\\nSpliceBERT used Masked Language Modeling (MLM) as the pre-training objective: taking a sequence, the model randomly masks 15% of the tokens in the input then runs the entire masked sentence through the model and has to predict the masked tokens. This is comparable to the Cloze task in language modeling.\\n\\n### Training Data\\n\\nThe SpliceBERT model was pre-trained on messenger RNA precursor sequences from [UCSC Genome Browser](https://genome.ucsc.edu).\\nUCSC Genome Browser provides visualization, analysis, and download of comprehensive vertebrate genome data with aligned annotation tracks (known genes, predicted genes, ESTs, mRNAs, CpG islands, etc.).\\n\\nSpliceBERT collected reference genomes and gene annotations from the UCSC Genome Browser for 72 vertebrate species. It applied [bedtools getfasta](https://bedtools.readthedocs.io/en/latest/content/tools/getfasta.html) to extract pre-mRNA sequences from the reference genomes based on the gene annotations. The pre-mRNA sequences are then used to pre-train SpliceBERT. The pre-training data contains 2 million pre-mRNA sequences with a total length of 65 billion nucleotides.\\n\\nNote [`RnaTokenizer`][multimolecule.RnaTokenizer] will convert \"T\"s to \"U\"s for you, you may disable this behaviour by passing `replace_T_with_U=False`.\\n\\n### Training Procedure\\n\\n#### Preprocessing\\n\\nSpliceBERT used masked language modeling (MLM) as the pre-training objective. The masking procedure is similar to the one used in BERT:\\n\\n- 15% of the tokens are masked.\\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\\n- In the 10% remaining cases, the masked tokens are left as is.\\n\\n#### PreTraining\\n\\nThe model was trained on 8 NVIDIA V100 GPUs.\\n\\n- Learning rate: 1e-4\\n- Learning rate scheduler: ReduceLROnPlateau(patience=3)\\n- Optimizer: AdamW\\n\\nSpliceBERT trained model in a two-stage training process:\\n\\n1. Pre-train with sequences of a fixed length of 510 nucleotides.\\n2. Pre-train with sequences of a variable length between 64 and 1024 nucleotides.\\n\\nThe intermediate model after the first stage is available as `multimolecule/splicebert.510nt`.\\n\\nSpliceBERT also pre-trained a model on human data only to validate the contribution of multi-species pre-training. The intermediate model after the first stage is available as `multimolecule/splicebert-human.510nt`.\\n\\n## Citation\\n\\n**BibTeX**:\\n\\n```bibtex\\n@article {chen2023self,\\n\\tauthor = {Chen, Ken and Zhou, Yue and Ding, Maolin and Wang, Yu and Ren, Zhixiang and Yang, Yuedong},\\n\\ttitle = {Self-supervised learning on millions of pre-mRNA sequences improves sequence-based RNA splicing prediction},\\n\\telocation-id = {2023.01.31.526427},\\n\\tyear = {2023},\\n\\tdoi = {10.1101/2023.01.31.526427},\\n\\tpublisher = {Cold Spring Harbor Laboratory},\\n\\tabstract = {RNA splicing is an important post-transcriptional process of gene expression in eukaryotic cells. Predicting RNA splicing from primary sequences can facilitate the interpretation of genomic variants. In this study, we developed a novel self-supervised pre-trained language model, SpliceBERT, to improve sequence-based RNA splicing prediction. Pre-training on pre-mRNA sequences from vertebrates enables SpliceBERT to capture evolutionary conservation information and characterize the unique property of splice sites. SpliceBERT also improves zero-shot prediction of variant effects on splicing by considering sequence context information, and achieves superior performance for predicting branchpoint in the human genome and splice sites across species. Our study highlighted the importance of pre-training genomic language models on a diverse range of species and suggested that pre-trained language models were promising for deciphering the sequence logic of RNA splicing.Competing Interest StatementThe authors have declared no competing interest.},\\n\\tURL = {https://www.biorxiv.org/content/early/2023/05/09/2023.01.31.526427},\\n\\teprint = {https://www.biorxiv.org/content/early/2023/05/09/2023.01.31.526427.full.pdf},\\n\\tjournal = {bioRxiv}\\n}\\n```\\n\\n## Contact\\n\\nPlease use GitHub issues of [MultiMolecule](https://github.com/DLS5-Omics/multimolecule/issues) for any questions or comments on the model card.\\n\\nPlease contact the authors of the [SpliceBERT paper](https://doi.org/10.1101/2023.01.31.526427) for questions or comments on the paper/model.\\n\\n## License\\n\\nThis model is licensed under the [AGPL-3.0 License](https://www.gnu.org/licenses/agpl-3.0.html).\\n\\n```spdx\\nSPDX-License-Identifier: AGPL-3.0-or-later\\n```',\n",
      "       '<p><h1>ðŸ‹ The First Chinese OrcaPlatypus! ðŸ‹</h1></p>\\nFine-tune the Open-Orca/OpenOrca-Platypus2-13B with 10% COIG-PC-LITE, 10% OpenOrca and 100% Open-Platypus for Chinese capability. Context window size 4KB.\\n\\n<p><h1>ðŸ‹ The First OrcaPlatypus! ðŸ‹</h1></p>\\n\\n![Platty](https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B/resolve/main/Images/OrcaPlatypusMerge.jpg)\\n\\n\\n# OpenOrca-Platypus2-13B\\n\\nOpenOrca-Platypus2-13B is a merge of [`garage-bAInd/Platypus2-13B`](https://huggingface.co/garage-bAInd/Platypus2-13B) and [`Open-Orca/OpenOrcaxOpenChat-Preview2-13B`](https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B).\\n\\nThis model is more than the sum of its parts! We are happy to be teaming up with the [Platypus](https://platypus-llm.github.io/) team to bring you a new model which once again tops the leaderboards!\\n\\nWant to visualize our full (pre-filtering) dataset? Check out our [Nomic Atlas Map](https://atlas.nomic.ai/map/c1b88b47-2d9b-47e0-9002-b80766792582/2560fd25-52fe-42f1-a58f-ff5eccc890d2).\\n\\n\\n[<img src=\"https://huggingface.co/Open-Orca/OpenOrca-Preview1-13B/resolve/main/OpenOrca%20Nomic%20Atlas.png\" alt=\"Atlas Nomic Dataset Map\" width=\"400\" height=\"400\" />](https://atlas.nomic.ai/map/c1b88b47-2d9b-47e0-9002-b80766792582/2560fd25-52fe-42f1-a58f-ff5eccc890d2)\\n\\n\\nWe are in-process with training more models, so keep a look out on our org for releases coming soon with exciting partners.\\n\\nWe will also give sneak-peak announcements on our Discord, which you can find here:\\n\\nhttps://AlignmentLab.ai\\n\\n# Evaluation\\n\\n## HuggingFace Leaderboard Performance\\n\\n![HF Leaderboard](https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B/resolve/main/Images/OrcaPlatypus13BHFLeaderboard.webp)\\n\\n\\n| Metric | Value |\\n|-----------------------|-------|\\n| MMLU (5-shot)         | 59.5  |\\n| ARC (25-shot)         | 62.88 |\\n| HellaSwag (10-shot)   | 83.19 |\\n| TruthfulQA (0-shot)   | 52.69 |\\n| Avg.                  | 64.56 |\\n\\nWe use [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) to run the benchmark tests above, using the same version as the HuggingFace LLM Leaderboard.\\n\\nPlease see below for detailed instructions on reproducing benchmark results.\\n\\n\\n## AGIEval Performance\\n\\nWe compare our results to our base Preview2 model (using LM Evaluation Harness).\\n\\nWe find **112%** of the base model's performance on AGI Eval, averaging **0.463**.\\nA large part of this boost is the substantial improvement to LSAT Logical Reasoning performance.\\n\\n![OpenOrca-Platypus2-13B AGIEval Performance](https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B/resolve/main/Images/OrcaPlatypus13BAGIEval.webp \"AGIEval Performance\")\\n\\n## BigBench-Hard Performance\\n\\nWe compare our results to our base Preview2 model (using LM Evaluation Harness).\\n\\nWe find **105%** of the base model's performance on BigBench-Hard, averaging **0.442**.\\n\\n![OpenOrca-Platypus2-13B BigBench-Hard Performance](https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B/resolve/main/Images/OrcaPlatypus13BBigBenchHard.webp \"BigBench-Hard Performance\")\\n\\n\\n# Model Details\\n\\n* **Trained by**: **Platypus2-13B** trained by Cole Hunter & Ariel Lee; **OpenOrcaxOpenChat-Preview2-13B** trained by Open-Orca\\n* **Model type:**  **OpenOrca-Platypus2-13B** is an auto-regressive language model based on the Lllama 2 transformer architecture.\\n* **Language(s)**: English\\n* **License for Platypus2-13B base weights**: Non-Commercial Creative Commons license ([CC BY-NC-4.0](https://creativecommons.org/licenses/by-nc/4.0/))\\n* **License for OpenOrcaxOpenChat-Preview2-13B base weights**: Llama 2 Commercial\\n\\n\\n# Prompting\\n\\n## Prompt Template for base Platypus2-13B\\n\\n```\\n### Instruction:\\n\\n<prompt> (without the <>)\\n\\n### Response:\\n```\\n\\n\\n## Prompt Template for base OpenOrcaxOpenChat-Preview2-13B\\n\\nOpenChat Llama2 V1: see [OpenOrcaxOpenChat-Preview2-13B](https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B) for additional information. \\n\\n\\n# Training\\n\\n## Training Datasets\\n\\n`garage-bAInd/Platypus2-13B` trained using STEM and logic based dataset [`garage-bAInd/Open-Platypus`](https://huggingface.co/datasets/garage-bAInd/Open-Platypus).\\n\\nPlease see our [paper](https://arxiv.org/abs/2308.07317) and [project webpage](https://platypus-llm.github.io) for additional information.\\n\\n`Open-Orca/OpenOrcaxOpenChat-Preview2-13B` trained using a refined subset of most of the GPT-4 data from the [OpenOrca dataset](https://huggingface.co/datasets/Open-Orca/OpenOrca).\\n\\n\\n## Training Procedure\\n\\n`Open-Orca/Platypus2-13B` was instruction fine-tuned using LoRA on 1x A100-80GB.\\nFor training details and inference instructions please see the [Platypus](https://github.com/arielnlee/Platypus) GitHub repo.\\n\\n\\n# Supplemental\\n\\n## Reproducing Evaluation Results (for HuggingFace Leaderboard Eval)\\n\\nInstall LM Evaluation Harness:\\n```\\n# clone repository\\ngit clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n# change to repo directory\\ncd lm-evaluation-harness\\n# check out the correct commit\\ngit checkout b281b0921b636bc36ad05c0b0b0763bd6dd43463\\n# install\\npip install -e .\\n```\\nEach task was evaluated on a single A100-80GB GPU.\\n\\nARC:\\n```\\npython main.py --model hf-causal-experimental --model_args pretrained=Open-Orca/OpenOrca-Platypus2-13B --tasks arc_challenge --batch_size 1 --no_cache --write_out --output_path results/OpenOrca-Platypus2-13B/arc_challenge_25shot.json --device cuda --num_fewshot 25\\n```\\n\\nHellaSwag:\\n```\\npython main.py --model hf-causal-experimental --model_args pretrained=Open-Orca/OpenOrca-Platypus2-13B --tasks hellaswag --batch_size 1 --no_cache --write_out --output_path results/OpenOrca-Platypus2-13B/hellaswag_10shot.json --device cuda --num_fewshot 10\\n```\\n\\nMMLU:\\n```\\npython main.py --model hf-causal-experimental --model_args pretrained=Open-Orca/OpenOrca-Platypus2-13B --tasks hendrycksTest-* --batch_size 1 --no_cache --write_out --output_path results/OpenOrca-Platypus2-13B/mmlu_5shot.json --device cuda --num_fewshot 5\\n```\\n\\nTruthfulQA:\\n```\\npython main.py --model hf-causal-experimental --model_args pretrained=Open-Orca/OpenOrca-Platypus2-13B --tasks truthfulqa_mc --batch_size 1 --no_cache --write_out --output_path results/OpenOrca-Platypus2-13B/truthfulqa_0shot.json --device cuda\\n```\\n\\n\\n## Limitations and bias\\n\\nLlama 2 and fine-tuned variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2 and any fine-tuned varient's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2 variants, developers should perform safety testing and tuning tailored to their specific applications of the model.\\n\\nPlease see the Responsible Use Guide available at https://ai.meta.com/llama/responsible-use-guide/\\n\\n\\n# Citations\\n\\n```bibtex\\n@software{hunterlee2023orcaplaty1\\n  title = {OpenOrcaPlatypus: Llama2-13B Model Instruct-tuned on Filtered OpenOrcaV1 GPT-4 Dataset and Merged with divergent STEM and Logic Dataset Model},\\n  author = {Ariel N. Lee and Cole J. Hunter and Nataniel Ruiz and Bleys Goodson and Wing Lian and Guan Wang and Eugene Pentland and Austin Cook and Chanvichet Vong and \"Teknium\"},\\n  year = {2023},\\n  publisher = {HuggingFace},\\n  journal = {HuggingFace repository},\\n  howpublished = {\\url{https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B},\\n}\\n@article{platypus2023,\\n    title={Platypus: Quick, Cheap, and Powerful Refinement of LLMs}, \\n    author={Ariel N. Lee and Cole J. Hunter and Nataniel Ruiz},\\n    booktitle={arXiv preprint arxiv:2308.07317},\\n    year={2023}\\n}\\n@software{OpenOrcaxOpenChatPreview2,\\n  title = {OpenOrcaxOpenChatPreview2: Llama2-13B Model Instruct-tuned on Filtered OpenOrcaV1 GPT-4 Dataset},\\n  author = {Guan Wang and Bleys Goodson and Wing Lian and Eugene Pentland and Austin Cook and Chanvichet Vong and \"Teknium\"},\\n  year = {2023},\\n  publisher = {HuggingFace},\\n  journal = {HuggingFace repository},\\n  howpublished = {\\url{https://https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B},\\n}\\n@software{openchat,\\n  title = {{OpenChat: Advancing Open-source Language Models with Imperfect Data}},\\n  author = {Wang, Guan and Cheng, Sijie and Yu, Qiying and Liu, Changling},\\n  doi = {10.5281/zenodo.8105775},\\n  url = {https://github.com/imoneoi/openchat},\\n  version = {pre-release},\\n  year = {2023},\\n  month = {7},\\n}\\n@misc{mukherjee2023orca,\\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \\n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\\n      year={2023},\\n      eprint={2306.02707},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n@misc{touvron2023llama,\\n    title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \\n    author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\\n    year={2023},\\n    eprint= arXiv 2307.09288\\n}\\n@misc{longpre2023flan,\\n      title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}, \\n      author={Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},\\n      year={2023},\\n      eprint={2301.13688},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI}\\n}\\n@article{hu2021lora,\\n  title={LoRA: Low-Rank Adaptation of Large Language Models},\\n  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},\\n  journal={CoRR},\\n  year={2021}\\n}\\n```',\n",
      "       '# Vision Transformer (base-sized model) trained using DINOv2\\n\\nVision Transformer (ViT) model trained using the DINOv2 method. It was introduced in the paper [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by Oquab et al. and first released in [this repository](https://github.com/facebookresearch/dinov2). \\n\\nDisclaimer: The team releasing DINOv2 did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\n## Model description\\n\\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion. \\n\\nImages are presented to the model as a sequence of fixed-size patches, which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\\n\\nNote that this model does not include any fine-tuned heads. \\n\\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\\n\\n## Intended uses & limitations\\n\\nYou can use the raw model for feature extraction. See the [model hub](https://huggingface.co/models?search=facebook/dinov2) to look for\\nfine-tuned versions on a task that interests you.\\n\\n### How to use\\n\\nHere is how to use this model:\\n\\n```python\\nfrom transformers import AutoImageProcessor, AutoModel\\nfrom PIL import Image\\nimport requests\\n\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\\nmodel = AutoModel.from_pretrained('facebook/dinov2-base')\\n\\ninputs = processor(images=image, return_tensors=\"pt\")\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\\n```\\n\\n### BibTeX entry and citation info\\n\\n```bibtex\\nmisc{oquab2023dinov2,\\n      title={DINOv2: Learning Robust Visual Features without Supervision}, \\n      author={Maxime Oquab and TimothÃ©e Darcet and ThÃ©o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and HervÃ© Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\\n      year={2023},\\n      eprint={2304.07193},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\\n```',\n",
      "       '# UTR-LM\\n\\nPre-trained model on 5â€™ untranslated region (5â€™UTR) using masked language modeling (MLM), Secondary Structure (SS), and Minimum Free Energy (MFE) objectives.\\n\\n## Statement\\n\\n_A 5â€™ UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions_ is published in [Nature Machine Intelligence](https://doi.org/10.1038/s42256-024-00823-9), which is a Closed Access / Author-Fee journal.\\n\\n> Machine learning has been at the forefront of the movement for free and open access to research.\\n>\\n> We see no role for closed access or author-fee publication in the future of machine learning research and believe the adoption of these journals as an outlet of record for the machine learning community would be a retrograde step.\\n\\nThe MultiMolecule team is committed to the principles of open access and open science.\\n\\nWe do NOT endorse the publication of manuscripts in Closed Access / Author-Fee journals and encourage the community to support Open Access journals and conferences.\\n\\nPlease consider signing the [Statement on Nature Machine Intelligence](https://openaccess.engineering.oregonstate.edu).\\n\\n## Disclaimer\\n\\nThis is an UNOFFICIAL implementation of the [A 5â€™ UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions](https://doi.org/10.1101/2023.10.11.561938) by Yanyi Chu, Dan Yu, et al.\\n\\nThe OFFICIAL repository of UTR-LM is at [a96123155/UTR-LM](https://github.com/a96123155/UTR-LM).\\n\\n> [!WARNING]\\n> The MultiMolecule team is unable to confirm that the provided model and checkpoints are producing the same intermediate representations as the original implementation.\\n> This is because\\n>\\n> The proposed method is published in a Closed Access / Author-Fee journal.\\n\\n**The team releasing UTR-LM did not write this model card for this model so this model card has been written by the MultiMolecule team.**\\n\\n## Model Details\\n\\nUTR-LM is a [bert](https://huggingface.co/google-bert/bert-base-uncased)-style model pre-trained on a large corpus of 5â€™ untranslated regions (5â€™UTRs) in a self-supervised fashion. This means that the model was trained on the raw nucleotides of RNA sequences only, with an automatic process to generate inputs and labels from those texts. Please refer to the [Training Details](#training-details) section for more information on the training process.\\n\\n### Variations\\n\\n- **[`multimolecule/utrlm.te_el`](https://huggingface.co/multimolecule/utrlm.te_el)**: The UTR-LM model for Translation Efficiency of transcripts and mRNA Expression Level.\\n- **[`multimolecule/utrlm.mrl`](https://huggingface.co/multimolecule/utrlm.mrl)**: The UTR-LM model for Mean Ribosome Loading.\\n\\n### Model Specification\\n\\n<table>\\n<thead>\\n  <tr>\\n    <th>Variants</th>\\n    <th>Num Layers</th>\\n    <th>Hidden Size</th>\\n    <th>Num Heads</th>\\n    <th>Intermediate Size</th>\\n    <th>Num Parameters (M)</th>\\n    <th>FLOPs (G)</th>\\n    <th>MACs (G)</th>\\n    <th>Max Num Tokens</th>\\n  </tr>\\n</thead>\\n<tbody>\\n  <tr>\\n    <td>UTR-LM MRL</td>\\n    <td rowspan=\"2\">6</td>\\n    <td rowspan=\"2\">128</td>\\n    <td rowspan=\"2\">16</td>\\n    <td rowspan=\"2\">512</td>\\n    <td rowspan=\"2\">1.21</td>\\n    <td rowspan=\"2\">0.35</td>\\n    <td rowspan=\"2\">0.18</td>\\n    <td rowspan=\"2\">1022</td>\\n  </tr>\\n  <tr>\\n    <td>UTR-LM TE_EL</td>\\n  </tr>\\n</tbody>\\n</table>\\n\\n### Links\\n\\n- **Code**: [multimolecule.utrlm](https://github.com/DLS5-Omics/multimolecule/tree/master/multimolecule/models/utrlm)\\n- **Data**:\\n  - [Ensembl Genome Browser](https://ensembl.org)\\n  - [Human 5â€² UTR design and variant effect prediction from a massively parallel translation assay](https://doi.org/10.1038/s41587-019-0164-5)\\n  - [High-Throughput 5â€™ UTR Engineering for Enhanced Protein Production in Non-Viral Gene Therapies](https://doi.org/10.1101/2021.10.14.464013)\\n- **Paper**: [A 5â€™ UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions](http://doi.org/10.1038/s41467-021-24436-7)\\n- **Developed by**: Yanyi Chu, Dan Yu, Yupeng Li, Kaixuan Huang, Yue Shen, Le Cong, Jason Zhang, Mengdi Wang\\n- **Model type**: [BERT](https://huggingface.co/google-bert/bert-base-uncased) - [ESM](https://huggingface.co/facebook/esm2_t48_15B_UR50D)\\n- **Original Repository**: [https://github.com/a96123155/UTR-LM](https://github.com/a96123155/UTR-LM)\\n\\n## Usage\\n\\nThe model file depends on the [`multimolecule`](https://multimolecule.danling.org) library. You can install it using pip:\\n\\n```bash\\npip install multimolecule\\n```\\n\\n### Direct Use\\n\\nYou can use this model directly with a pipeline for masked language modeling:\\n\\n```python\\n>>> import multimolecule  # you must import multimolecule to register models\\n>>> from transformers import pipeline\\n>>> unmasker = pipeline('fill-mask', model='multimolecule/utrlm.te_el')\\n>>> unmasker(\"uagc<mask>uaucagacugauguuga\")\\n\\n[{'score': 0.08083827048540115,\\n  'token': 23,\\n  'token_str': '*',\\n  'sequence': 'U A G C * U A U C A G A C U G A U G U U G A'},\\n {'score': 0.07966958731412888,\\n  'token': 5,\\n  'token_str': '<null>',\\n  'sequence': 'U A G C U A U C A G A C U G A U G U U G A'},\\n {'score': 0.0771222859621048,\\n  'token': 6,\\n  'token_str': 'A',\\n  'sequence': 'U A G C A U A U C A G A C U G A U G U U G A'},\\n {'score': 0.06853719055652618,\\n  'token': 10,\\n  'token_str': 'N',\\n  'sequence': 'U A G C N U A U C A G A C U G A U G U U G A'},\\n {'score': 0.06666938215494156,\\n  'token': 21,\\n  'token_str': '.',\\n  'sequence': 'U A G C. U A U C A G A C U G A U G U U G A'}]\\n```\\n\\n### Downstream Use\\n\\n#### Extract Features\\n\\nHere is how to use this model to get the features of a given sequence in PyTorch:\\n\\n```python\\nfrom multimolecule import RnaTokenizer, UtrLmModel\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrlm.te_el')\\nmodel = UtrLmModel.from_pretrained('multimolecule/utrlm.te_el')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\n\\noutput = model(**input)\\n```\\n\\n#### Sequence Classification / Regression\\n\\n**Note**: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for sequence classification or regression.\\n\\nHere is how to use this model as backbone to fine-tune for a sequence-level task in PyTorch:\\n\\n```python\\nimport torch\\nfrom multimolecule import RnaTokenizer, UtrLmForSequencePrediction\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrlm.te_el')\\nmodel = UtrLmForSequencePrediction.from_pretrained('multimolecule/utrlm.te_el')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\nlabel = torch.tensor([1])\\n\\noutput = model(**input, labels=label)\\n```\\n\\n#### Nucleotide Classification / Regression\\n\\n**Note**: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for nucleotide classification or regression.\\n\\nHere is how to use this model as backbone to fine-tune for a nucleotide-level task in PyTorch:\\n\\n```python\\nimport torch\\nfrom multimolecule import RnaTokenizer, UtrLmForNucleotidePrediction\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrlm.te_el')\\nmodel = UtrLmForNucleotidePrediction.from_pretrained('multimolecule/utrlm.te_el')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\nlabel = torch.randint(2, (len(text), ))\\n\\noutput = model(**input, labels=label)\\n```\\n\\n#### Contact Classification / Regression\\n\\n**Note**: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for contact classification or regression.\\n\\nHere is how to use this model as backbone to fine-tune for a contact-level task in PyTorch:\\n\\n```python\\nimport torch\\nfrom multimolecule import RnaTokenizer, UtrLmForContactPrediction\\n\\n\\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrlm')\\nmodel = UtrLmForContactPrediction.from_pretrained('multimolecule/utrlm')\\n\\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\\ninput = tokenizer(text, return_tensors='pt')\\nlabel = torch.randint(2, (len(text), len(text)))\\n\\noutput = model(**input, labels=label)\\n```\\n\\n## Training Details\\n\\nUTR-LM used a mixed training strategy with one self-supervised task and two supervised tasks, where the labels of both supervised tasks are calculated using [ViennaRNA](https://viennarna.readthedocs.io).\\n\\n1. **Masked Language Modeling (MLM)**: taking a sequence, the model randomly masks 15% of the tokens in the input then runs the entire masked sentence through the model and has to predict the masked tokens. This is comparable to the Cloze task in language modeling.\\n2. **Secondary Structure (SS)**: predicting the secondary structure of the `<mask>` token in the MLM task.\\n3. **Minimum Free Energy (MFE)**: predicting the minimum free energy of the 5â€™ UTR sequence.\\n\\n### Training Data\\n\\nThe UTR-LM model was pre-trained on 5â€™ UTR sequences from three sources:\\n\\n- **[Ensembl Genome Browser](https://ensembl.org)**: Ensembl is a genome browser for vertebrate genomes that supports research in comparative genomics, evolution, sequence variation and transcriptional regulation. UTR-LM used 5â€™ UTR sequences from 5 species: human, rat, mouse, chicken, and zebrafish, since these species have high-quality and manual gene annotations.\\n- **[Human 5â€² UTR design and variant effect prediction from a massively parallel translation assay](https://doi.org/10.1038/s41587-019-0164-5)**: Sample et al. proposed 8 distinct 5' UTR libraries, each containing random 50 nucleotide sequences, to evaluate translation rules using mean ribosome loading (MRL) measurements.\\n- **[High-Throughput 5â€™ UTR Engineering for Enhanced Protein Production in Non-Viral Gene Therapies](https://doi.org/10.1038/s41467-021-24436-7)**: Cao et al. analyzed endogenous human 5â€™ UTRs, including data from 3 distinct cell lines/tissues: human embryonic kidney 293T (HEK), human prostate cancer cell (PC3), and human muscle tissue (Muscle).\\n\\nUTR-LM preprocessed the 5â€™ UTR sequences in a 4-step pipeline:\\n\\n1. removed all coding sequence (CDS) and non-5' UTR fragments from the raw sequences.\\n2. identified and removed duplicate sequences\\n3. truncated the sequences to fit within a range of 30 to 1022 bp\\n4. filtered out incorrect and low-quality sequences\\n\\nNote [`RnaTokenizer`][multimolecule.RnaTokenizer] will convert \"T\"s to \"U\"s for you, you may disable this behaviour by passing `replace_T_with_U=False`.\\n\\n### Training Procedure\\n\\n#### Preprocessing\\n\\nUTR-LM used masked language modeling (MLM) as one of the pre-training objectives. The masking procedure is similar to the one used in BERT:\\n\\n- 15% of the tokens are masked.\\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\\n- In the 10% remaining cases, the masked tokens are left as is.\\n\\n#### PreTraining\\n\\nThe model was trained on two clusters:\\n\\n1. 4 NVIDIA V100 GPUs with 16GiB memories.\\n2. 4 NVIDIA P100 GPUs with 32GiB memories.\\n\\n## Citation\\n\\n**BibTeX**:\\n\\n```bibtex\\n@article {chu2023a,\\n\\tauthor = {Chu, Yanyi and Yu, Dan and Li, Yupeng and Huang, Kaixuan and Shen, Yue and Cong, Le and Zhang, Jason and Wang, Mengdi},\\n\\ttitle = {A 5{\\textquoteright} UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions},\\n\\telocation-id = {2023.10.11.561938},\\n\\tyear = {2023},\\n\\tdoi = {10.1101/2023.10.11.561938},\\n\\tpublisher = {Cold Spring Harbor Laboratory},\\n\\tabstract = {The 5{\\textquoteright} UTR, a regulatory region at the beginning of an mRNA molecule, plays a crucial role in regulating the translation process and impacts the protein expression level. Language models have showcased their effectiveness in decoding the functions of protein and genome sequences. Here, we introduced a language model for 5{\\textquoteright} UTR, which we refer to as the UTR-LM. The UTR-LM is pre-trained on endogenous 5{\\textquoteright} UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The model outperformed the best-known benchmark by up to 42\\% for predicting the Mean Ribosome Loading, and by up to 60\\% for predicting the Translation Efficiency and the mRNA Expression Level. The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we designed a library of 211 novel 5{\\textquoteright} UTRs with high predicted values of translation efficiency and evaluated them via a wet-lab assay. Experiment results confirmed that our top designs achieved a 32.5\\% increase in protein production level relative to well-established 5{\\textquoteright} UTR optimized for therapeutics.Competing Interest StatementThe authors have declared no competing interest.},\\n\\tURL = {https://www.biorxiv.org/content/early/2023/10/14/2023.10.11.561938},\\n\\teprint = {https://www.biorxiv.org/content/early/2023/10/14/2023.10.11.561938.full.pdf},\\n\\tjournal = {bioRxiv}\\n}\\n```\\n\\n## Contact\\n\\nPlease use GitHub issues of [MultiMolecule](https://github.com/DLS5-Omics/multimolecule/issues) for any questions or comments on the model card.\\n\\nPlease contact the authors of the [UTR-LM paper](https://doi.org/10.1101/2023.10.11.561938) for questions or comments on the paper/model.\\n\\n## License\\n\\nThis model is licensed under the [AGPL-3.0 License](https://www.gnu.org/licenses/agpl-3.0.html).\\n\\n```spdx\\nSPDX-License-Identifier: AGPL-3.0-or-later\\n```',\n",
      "       '# VideoMAE (base-sized model, pre-trained only) \\n\\nVideoMAE model pre-trained on Kinetics-400 for 800 epochs in a self-supervised way. It was introduced in the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Tong et al. and first released in [this repository](https://github.com/MCG-NJU/VideoMAE). \\n\\nDisclaimer: The team releasing VideoMAE did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\n## Model description\\n\\nVideoMAE is an extension of [Masked Autoencoders (MAE)](https://arxiv.org/abs/2111.06377) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.\\n\\nVideos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder.\\n\\nBy pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\n\\n## Intended uses & limitations\\n\\nYou can use the raw model for predicting pixel values for masked patches of a video, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=videomae) to look for fine-tuned versions on a task that interests you.\\n\\n### How to use\\n\\nHere is how to use this model to predict pixel values for randomly masked patches:\\n\\n```python\\nfrom transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\n\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\n\\nprocessor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-short\")\\nmodel = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base-short\")\\n\\npixel_values = processor(video, return_tensors=\"pt\").pixel_values\\n\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\n\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\\n```\\n\\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/main/model_doc/videomae.html#).\\n\\n## Training data\\n\\n(to do, feel free to open a PR)\\n\\n## Training procedure\\n\\n### Preprocessing\\n\\n(to do, feel free to open a PR)\\n\\n### Pretraining\\n\\n(to do, feel free to open a PR)\\n\\n## Evaluation results\\n\\n(to do, feel free to open a PR)\\n\\n### BibTeX entry and citation info\\n\\n```bibtex\\nmisc{https://doi.org/10.48550/arxiv.2203.12602,\\n  doi = {10.48550/ARXIV.2203.12602},\\n  url = {https://arxiv.org/abs/2203.12602},\\n  author = {Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},\\n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\\n  title = {VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},\\n  publisher = {arXiv},\\n  year = {2022},\\n  copyright = {Creative Commons Attribution 4.0 International}\\n}\\n```',\n",
      "       '# Overview\\n\\nUnreleased, untested, unfinished beta.\\n\\nWe've trained Microsoft Research's [phi-1.5](https://huggingface.co/microsoft/phi-1_5), 1.3B parameter model with the same OpenOrca dataset as we used with our [OpenOrcaxOpenChat-Preview2-13B](https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B) model.\\n\\nThis model doesn't dramatically improve on the base model's general task performance, but the instruction tuning has made the model reliably handle the ChatML prompt format.\\n\\n\\n# Evaluations\\n\\nWe've only done limited testing as yet. The [epoch 3.5 checkpoint](https://huggingface.co/Open-Orca/oo-phi-1_5/commit/f7754d8b8b4c3e0748eaf47be4cf5aac1f80a401) scores above 5.1 on MT-Bench (better than Alpaca-13B, worse than Llama2-7b-chat), while preliminary benchmarks suggest peak average performance was achieved roughly at epoch 4.\\n\\n## HuggingFaceH4 Open LLM Leaderboard Performance\\n\\nThe only significant improvement was with TruthfulQA.\\n\\n![HF Leaderboard](https://huggingface.co/Open-Orca/oo-phi-1_5/resolve/main/Images/oo-phi-1_5-HFLeaderboard.png)\\n\\n\\n## MT-bench Performance\\n\\n\\n![MT-bench Score](https://huggingface.co/Open-Orca/oo-phi-1_5/resolve/main/Images/oo-phi-1_5-mtbench.png)\\n\\n| Epoch     | Average   | Turn 1    | Turn 2    |\\n|:----------|:----------|:----------|:----------|\\n| 3         | 4.85      | 5.69      | 4.01      |\\n| 3.5       | 5.19      | 5.91      | 4.46      |\\n| 4         | 4.89      | 5.74      | 4.05      |\\n| 4.5       | 5.03      | 6.04      | 4.03      |\\n| 5         | 4.94      | 5.76      | 4.11      |\\n\\n\\n# Training\\n\\nTrained with full-parameters fine-tuning on 8x RTX A6000-48GB (Ampere) for 5 epochs for 62 hours (12.5h/epoch) at a commodity cost of $390 ($80/epoch).\\nWe did not use [MultiPack](https://github.com/imoneoi/multipack_sampler) packing, as training was begun prior to implementing support for it in Axolotl for this new model type.\\n\\n[<img src=\"https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/OpenAccess-AI-Collective/axolotl)\\n\\nWe've uploaded checkpoints of every 1/2 epoch of progress to this repo.\\nThere are branches/tags for the epoch 3 and epoch 4 uploads.\\nThis should allow, e.g., with oobabooga to download `Open-Orca/oo-phi-1_5:ep4` to select the epoch 4 checkpoint to download specifically.\\n\\n\\n# Prompt Template\\n\\nWe used [OpenAI's Chat Markup Language (ChatML)](https://github.com/openai/openai-python/blob/main/chatml.md) format, with `<|im_start|>` and `<|im_end|>` tokens added to support this.\\nThis means that, e.g., in [oobabooga](https://github.com/oobabooga/text-generation-webui/) the `MPT-Chat` instruction template should work.\\n\\n\\n# Inference\\n\\nRemove *`.to('cuda')`* for unaccelerated.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\"Open-Orca/oo-phi-1_5\",\\n    trust_remote_code=True,\\n    torch_dtype=torch.bfloat16\\n    ).to('cuda')\\ntokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/oo-phi-1_5\",\\n    trust_remote_code=True,\\n    torch_dtype=torch.bfloat16)\\n\\nsys_prompt = \"I am OrcaPhi. The following is my internal dialogue as an AI assistant.\\n\" \\\\n    \"Today is September 15, 2023. I have no access to outside tools, news, or current events.\\n\" \\\\n    \"I carefully provide accurate, factual, thoughtful, nuanced answers and am brilliant at reasoning.\\n\" \\\\n    \"I think through my answers step-by-step to be sure I always get the right answer.\\n\" \\\\n    \"I think more clearly if I write out my thought process in a scratchpad manner first; therefore, I always \" \\\\n    \"explain background context, assumptions, and step-by-step thinking BEFORE trying to answer a question.\" \\\\n    \"Take a deep breath and think calmly about everything presented.\"\\nprompt = \"Hello! Tell me about what makes you special, as an AI assistant.\\n\" \\\\n    \"Particularly, what programming tasks are you best at?\"\\n\\nprefix = \"<|im_start|>\"\\nsuffix = \"<|im_end|>\\n\"\\nsys_format = prefix + \"system\\n\" + sys_prompt + suffix\\nuser_format = prefix + \"user\\n\" + prompt + suffix\\nassistant_format = prefix + \"assistant\\n\"\\ninput_text = sys_format + user_format + assistant_format\\n\\ngeneration_config = GenerationConfig(\\n    max_length=1024, temperature=0.01, top_p=0.95, repetition_penalty=1.1,\\n    do_sample=True, use_cache=True,\\n    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id,\\n    transformers_version=\"4.33.1\"\\n    )\\n\\ninputs = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=False).to('cuda')\\noutputs = model.generate(**inputs, generation_config=generation_config)\\n\\ntext = tokenizer.batch_decode(outputs)[0]\\nprint(text)\\n```\\n\\nExample output:\\n\\n```text\\n<|im_start|>system\\nI am OrcaPhi. The following is my internal dialogue as an AI assistant.\\nToday is September 15, 2023. I have no access to outside tools, news, or current events.\\nI carefully provide accurate, factual, thoughtful, nuanced answers and am brilliant at reasoning.\\nI think through my answers step-by-step to be sure I always get the right answer.\\nI think more clearly if I write out my thought process in a scratchpad manner first; therefore, I always explain background context, assumptions, and step-by-step thinking BEFORE trying to answer a question.Take a deep breath and think calmly about everything presented.<|im_end|>\\n<|im_start|>user\\nHello! Tell me about what makes you special, as an AI assistant.\\nParticularly, what programming tasks are you best at?<|im_end|>\\n<|im_start|>assistant\\nAs an AI, I'm designed to understand and respond to human language, which includes natural language processing (NLP) techniques that allow me to comprehend text, speech, and other forms of communication. My primary function is to assist users by providing relevant information, answering questions, and performing various tasks based on user input.\\n\\nIn terms of programming tasks, I am particularly skilled in:\\n\\n1. Natural Language Processing (NLP): This allows me to understand and analyze human language, enabling me to generate appropriate responses and perform tasks such as sentiment analysis, named entity recognition, and topic modeling.\\n2. Machine Learning: I can learn from large datasets and improve my performance over time, allowing me to adapt to new situations and provide better assistance to users.\\n3. Deep Learning: I can leverage advanced neural networks and complex architectures to solve problems that may not be easily addressed using traditional machine learning methods.\\n4. Computer Vision: I can recognize and interpret visual data, making it possible for me to identify objects, faces, and other visual elements within images and videos.\\n5. Robotics: I can help with tasks related to robotics, including object detection, navigation, and manipulation.\\n6. Voice Recognition: I can accurately transcribe spoken words into written text, making it easier for users to interact with me.\\n7. Chatbots: I can engage in conversations with users, providing them with helpful information, answering their questions, and assisting them with various tasks.\\n8. Data Analysis: I can analyze large amounts of data quickly and efficiently, helping users make informed decisions based on insights derived from the information provided.\\n9. Recommender Systems: I can suggest products, services, or content based on users' preferences and past behavior, improving their overall experience.\\n10. Fraud Detection: I can detect and prevent fraudulent activities, protecting users' financial information and ensuring secure transactions.\\n\\nThese programming tasks showcase my ability to understand and process vast amounts of information while adapting to different contexts and user needs. As an AI, I continuously learn and evolve to become even more effective in assisting users.<|im_end|>\\n```\\n\\n\\n# Citation\\n\\n```bibtex\\n@software{lian2023oophi15,\\n  title = {OpenOrca oo-phi-1.5: Phi-1.5 1.3B Model Instruct-tuned on Filtered OpenOrcaV1 GPT-4 Dataset},\\n  author = {Wing Lian and Bleys Goodson and Guan Wang and Eugene Pentland and Austin Cook and Chanvichet Vong and \"Teknium\"},\\n  year = {2023},\\n  publisher = {HuggingFace},\\n  journal = {HuggingFace repository},\\n  howpublished = {\\url{https://huggingface.co/Open-Orca/oo-phi-1_5},\\n}\\n@article{textbooks2,\\n  title={Textbooks Are All You Need II: \\textbf{phi-1.5} technical report},\\n  author={Li, Yuanzhi and Bubeck, S{\\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},\\n  journal={arXiv preprint arXiv:2309.05463},\\n  year={2023}\\n}\\n@misc{mukherjee2023orca,\\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \\n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\\n      year={2023},\\n      eprint={2306.02707},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n@misc{longpre2023flan,\\n      title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}, \\n      author={Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},\\n      year={2023},\\n      eprint={2301.13688},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI}\\n}\\n```',\n",
      "       '# VideoMAE (base-sized model, pre-trained only) \\n\\nVideoMAE model pre-trained on Kinetics-400 for 1600 epochs in a self-supervised way. It was introduced in the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Tong et al. and first released in [this repository](https://github.com/MCG-NJU/VideoMAE). \\n\\nDisclaimer: The team releasing VideoMAE did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\n## Model description\\n\\nVideoMAE is an extension of [Masked Autoencoders (MAE)](https://arxiv.org/abs/2111.06377) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.\\n\\nVideos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder.\\n\\nBy pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\n\\n## Intended uses & limitations\\n\\nYou can use the raw model for predicting pixel values for masked patches of a video, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=videomae) to look for fine-tuned versions on a task that interests you.\\n\\n### How to use\\n\\nHere is how to use this model to predict pixel values for randomly masked patches:\\n\\n```python\\nfrom transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\n\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\n\\nprocessor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\\nmodel = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\")\\n\\npixel_values = processor(video, return_tensors=\"pt\").pixel_values\\n\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\n\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\\n```\\n\\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/main/model_doc/videomae.html#).\\n\\n## Training data\\n\\n(to do, feel free to open a PR)\\n\\n## Training procedure\\n\\n### Preprocessing\\n\\n(to do, feel free to open a PR)\\n\\n### Pretraining\\n\\n(to do, feel free to open a PR)\\n\\n## Evaluation results\\n\\n(to do, feel free to open a PR)\\n\\n### BibTeX entry and citation info\\n\\n```bibtex\\nmisc{https://doi.org/10.48550/arxiv.2203.12602,\\n  doi = {10.48550/ARXIV.2203.12602},\\n  url = {https://arxiv.org/abs/2203.12602},\\n  author = {Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},\\n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\\n  title = {VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},\\n  publisher = {arXiv},\\n  year = {2022},\\n  copyright = {Creative Commons Attribution 4.0 International}\\n}\\n```',\n",
      "       '# Gemma-Ko\\n\\n> Update @ 2024.03.26: First release of Gemma-Ko 2B model\\n\\n**Original Gemma Model Page**: [Gemma](https://ai.google.dev/gemma/docs)\\n\\nThis model card corresponds to the 2B base version of the **Gemma-Ko** model. \\n\\n**Resources and Technical Documentation**:\\n\\n* [Original Google's Gemma-2B](https://huggingface.co/google/gemma-2b)\\n* [Training Code @ Github: Gemma-EasyLM](https://github.com/Beomi/Gemma-EasyLM)\\n\\n**Terms of Use**: [Terms](https://www.kaggle.com/models/google/gemma/license/consent)\\n\\n**Citation**\\n\\n```bibtex\\n@misc {gemma_ko_7b,\\n\\tauthor       = { {Junbum Lee, Taekyoon Choi} },\\n\\ttitle        = { gemma-ko-7b },\\n\\tyear         = 2024,\\n\\turl          = { https://huggingface.co/beomi/gemma-ko-7b },\\n\\tdoi          = { 10.57967/hf/1859 },\\n\\tpublisher    = { Hugging Face }\\n}\\n```\\n\\n**Model Developers**: Junbum Lee (Beomi) & Taekyoon Choi (Taekyoon)\\n\\n## Model Information\\n\\nSummary description and brief definition of inputs and outputs.\\n\\n### Description\\n\\nGemma is a family of lightweight, state-of-the-art open models from Google,\\nbuilt from the same research and technology used to create the Gemini models.\\nThey are text-to-text, decoder-only large language models, available in English,\\nwith open weights, pre-trained variants, and instruction-tuned variants. Gemma\\nmodels are well-suited for a variety of text generation tasks, including\\nquestion answering, summarization, and reasoning. Their relatively small size\\nmakes it possible to deploy them in environments with limited resources such as\\na laptop, desktop or your own cloud infrastructure, democratizing access to\\nstate of the art AI models and helping foster innovation for everyone.\\n\\n### Usage\\n\\nBelow we share some code snippets on how to get quickly started with running the model. First make sure to `pip install -U transformers`, then copy the snippet from the section that is relevant for your usecase.\\n\\n#### Running the model on a CPU\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"beomi/gemma-ko-2b\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"beomi/gemma-ko-2b\")\\n\\ninput_text = \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ëŠ”\"\\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\\n\\noutputs = model.generate(**input_ids)\\nprint(tokenizer.decode(outputs[0]))\\n```\\n\\n\\n#### Running the model on a single / multi GPU\\n\\n```python\\n# pip install accelerate\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"beomi/gemma-ko-2b\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"beomi/gemma-ko-2b\", device_map=\"auto\")\\n\\ninput_text = \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ëŠ”\"\\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\\n\\noutputs = model.generate(**input_ids)\\nprint(tokenizer.decode(outputs[0]))\\n```\\n\\n#### Other optimizations\\n\\n* _Flash Attention 2_\\n\\nFirst make sure to install `flash-attn` in your environment `pip install flash-attn`\\n\\n```diff\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"beomi/gemma-ko-2b\", \\n    torch_dtype=torch.float16, \\n+   attn_implementation=\"flash_attention_2\"\\n).to(0)\\n```\\n\\n### Inputs and outputs\\n\\n*   **Input:** Text string, such as a question, a prompt, or a document to be\\n    summarized.\\n*   **Output:** Generated Korean/English-language text in response to the input, such\\n    as an answer to a question, or a summary of a document.\\n\\n## Implementation Information\\n\\nDetails about the model internals.\\n\\n### Software\\n\\nTraining was done using [beomi/Gemma-EasyLM](https://github.com/Beomi/Gemma-EasyLM).\\n\\n\\n## Evaluation\\n\\nModel evaluation metrics and results.\\n\\n### Benchmark Results\\n\\nTBD\\n\\n## Usage and Limitations\\n\\nThese models have certain limitations that users should be aware of.\\n\\n### Intended Usage\\n\\nOpen Large Language Models (LLMs) have a wide range of applications across\\nvarious industries and domains. The following list of potential uses is not\\ncomprehensive. The purpose of this list is to provide contextual information\\nabout the possible use-cases that the model creators considered as part of model\\ntraining and development.\\n\\n* Content Creation and Communication\\n  * Text Generation: These models can be used to generate creative text formats\\n    such as poems, scripts, code, marketing copy, and email drafts.\\n* Research and Education\\n  * Natural Language Processing (NLP) Research: These models can serve as a\\n    foundation for researchers to experiment with NLP techniques, develop\\n    algorithms, and contribute to the advancement of the field.\\n  * Language Learning Tools: Support interactive language learning experiences,\\n    aiding in grammar correction or providing writing practice.\\n  * Knowledge Exploration: Assist researchers in exploring large bodies of text\\n    by generating summaries or answering questions about specific topics.\\n\\n### Limitations\\n\\n* Training Data\\n  * The quality and diversity of the training data significantly influence the\\n    model's capabilities. Biases or gaps in the training data can lead to\\n    limitations in the model's responses.\\n  * The scope of the training dataset determines the subject areas the model can\\n    handle effectively.\\n* Context and Task Complexity\\n  * LLMs are better at tasks that can be framed with clear prompts and\\n    instructions. Open-ended or highly complex tasks might be challenging.\\n  * A model's performance can be influenced by the amount of context provided\\n    (longer context generally leads to better outputs, up to a certain point).\\n* Language Ambiguity and Nuance\\n  * Natural language is inherently complex. LLMs might struggle to grasp subtle\\n    nuances, sarcasm, or figurative language.\\n* Factual Accuracy\\n  * LLMs generate responses based on information they learned from their\\n    training datasets, but they are not knowledge bases. They may generate\\n    incorrect or outdated factual statements.\\n* Common Sense\\n  * LLMs rely on statistical patterns in language. They might lack the ability\\n    to apply common sense reasoning in certain situations.\\n\\n### Ethical Considerations and Risks\\n\\nThe development of large language models (LLMs) raises several ethical concerns.\\nIn creating an open model, we have carefully considered the following:\\n\\n* Bias and Fairness\\n  * LLMs trained on large-scale, real-world text data can reflect socio-cultural\\n    biases embedded in the training material. These models underwent careful\\n    scrutiny, input data pre-processing described and posterior evaluations\\n    reported in this card.\\n* Misinformation and Misuse\\n  * LLMs can be misused to generate text that is false, misleading, or harmful.\\n  * Guidelines are provided for responsible use with the model, see the\\n    [Responsible Generative AI Toolkit](http://ai.google.dev/gemma/responsible).\\n* Transparency and Accountability:\\n  * This model card summarizes details on the models' architecture,\\n    capabilities, limitations, and evaluation processes.\\n  * A responsibly developed open model offers the opportunity to share\\n    innovation by making LLM technology accessible to developers and researchers\\n    across the AI ecosystem.\\n\\nRisks identified and mitigations:\\n\\n* Perpetuation of biases: It's encouraged to perform continuous monitoring\\n  (using evaluation metrics, human review) and the exploration of de-biasing\\n  techniques during model training, fine-tuning, and other use cases.\\n* Generation of harmful content: Mechanisms and guidelines for content safety\\n  are essential. Developers are encouraged to exercise caution and implement\\n  appropriate content safety safeguards based on their specific product policies\\n  and application use cases.\\n* Misuse for malicious purposes: Technical limitations and developer and\\n  end-user education can help mitigate against malicious applications of LLMs.\\n  Educational resources and reporting mechanisms for users to flag misuse are\\n  provided. Prohibited uses of Gemma models are outlined in the\\n  [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy).\\n* Privacy violations: Models were trained on data filtered for removal of PII\\n  (Personally Identifiable Information). Developers are encouraged to adhere to\\n  privacy regulations with privacy-preserving techniques.\\n\\n## Acknowledgement\\n\\nThe training is supported by [TPU Research Cloud](https://sites.research.google/trc/) program.',\n",
      "       ...\n",
      "       '<!-- header start -->\\n<!-- 200823 -->\\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\\n<img src=\"https://i.imgur.com/EBdldam.jpg\" alt=\"TheBlokeAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\\n</div>\\n<div style=\"display: flex; justify-content: space-between; width: 100%;\">\\n    <div style=\"display: flex; flex-direction: column; align-items: flex-start;\">\\n        <p style=\"margin-top: 0.5em; margin-bottom: 0em;\"><a href=\"https://discord.gg/theblokeai\">Chat & support: TheBloke's Discord server</a></p>\\n    </div>\\n    <div style=\"display: flex; flex-direction: column; align-items: flex-end;\">\\n        <p style=\"margin-top: 0.5em; margin-bottom: 0em;\"><a href=\"https://www.patreon.com/TheBlokeAI\">Want to contribute? TheBloke's Patreon page</a></p>\\n    </div>\\n</div>\\n<div style=\"text-align:center; margin-top: 0em; margin-bottom: 0em\"><p style=\"margin-top: 0.25em; margin-bottom: 0em;\">TheBloke's LLM work is generously supported by a grant from <a href=\"https://a16z.com\">andreessen horowitz (a16z)</a></p></div>\\n<hr style=\"margin-top: 1.0em; margin-bottom: 1.0em;\">\\n<!-- header end -->\\n\\n# LlongOrca 7B 16K - GGML\\n- Model creator: [Open-Orca](https://huggingface.co/Open-Orca)\\n- Original model: [LlongOrca 7B 16K](https://huggingface.co/Open-Orca/LlongOrca-7B-16k)\\n\\n## Description\\n\\nThis repo contains GGML format model files for [Open-Orca's LlongOrca 7B 16K](https://huggingface.co/Open-Orca/LlongOrca-7B-16k).\\n\\n### Important note regarding GGML files.\\n\\nThe GGML format has now been superseded by GGUF. As of August 21st 2023, [llama.cpp](https://github.com/ggerganov/llama.cpp) no longer supports GGML models. Third party clients and libraries are expected to still support it for a time, but many may also drop support.\\n\\nPlease use the GGUF models instead.\\n### About GGML\\n\\nGGML files are for CPU + GPU inference using [llama.cpp](https://github.com/ggerganov/llama.cpp) and libraries and UIs which support this format, such as:\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most popular web UI. Supports NVidia CUDA GPU acceleration.\\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a powerful GGML web UI with GPU acceleration on all platforms (CUDA and OpenCL). Especially good for story telling.\\n* [LM Studio](https://lmstudio.ai/), a fully featured local GUI with GPU acceleration on both Windows (NVidia and AMD), and macOS.\\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with CUDA GPU acceleration via the c_transformers backend.\\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\\n\\n## Repositories available\\n\\n* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GPTQ)\\n* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGUF)\\n* [2, 3, 4, 5, 6 and 8-bit GGML models for CPU+GPU inference (deprecated)](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML)\\n* [Open-Orca's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/Open-Orca/LlongOrca-7B-16k)\\n\\n## Prompt template: ChatML\\n\\n```\\n<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\\n```\\n\\n<!-- compatibility_ggml start -->\\n## Compatibility\\n\\nThese quantised GGML files are compatible with llama.cpp between June 6th (commit `2d43387`) and August 21st 2023.\\n\\nFor support with latest llama.cpp, please use GGUF files instead.\\n\\nThe final llama.cpp commit with support for GGML was: [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa)\\n\\nAs of August 23rd 2023 they are still compatible with all UIs, libraries and utilities which use GGML. This may change in the future.\\n\\n## Explanation of the new k-quant methods\\n<details>\\n  <summary>Click to see details</summary>\\n\\nThe new methods available are:\\n* GGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\\n* GGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\\n* GGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\\n* GGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\\n* GGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\\n* GGML_TYPE_Q8_K - \"type-0\" 8-bit quantization. Only used for quantizing intermediate results. The difference to the existing Q8_0 is that the block size is 256. All 2-6 bit dot products are implemented for this quantization type.\\n\\nRefer to the Provided Files table below to see what files use which methods, and how.\\n</details>\\n<!-- compatibility_ggml end -->\\n\\n## Provided files\\n\\n| Name | Quant method | Bits | Size | Max RAM required | Use case |\\n| ---- | ---- | ---- | ---- | ---- | ----- |\\n| [llongorca-7b-16k.ggmlv3.q2_K.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q2_K.bin) | q2_K | 2 | 3.05 GB| 5.55 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors. |\\n| [llongorca-7b-16k.ggmlv3.q3_K_S.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q3_K_S.bin) | q3_K_S | 3 | 3.12 GB| 5.62 GB | New k-quant method. Uses GGML_TYPE_Q3_K for all tensors |\\n| [llongorca-7b-16k.ggmlv3.q3_K_M.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q3_K_M.bin) | q3_K_M | 3 | 3.45 GB| 5.95 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |\\n| [llongorca-7b-16k.ggmlv3.q3_K_L.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q3_K_L.bin) | q3_K_L | 3 | 3.77 GB| 6.27 GB | New k-quant method. Uses GGML_TYPE_Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |\\n| [llongorca-7b-16k.ggmlv3.q4_0.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q4_0.bin) | q4_0 | 4 | 3.79 GB| 6.29 GB | Original quant method, 4-bit. |\\n| [llongorca-7b-16k.ggmlv3.q4_K_S.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q4_K_S.bin) | q4_K_S | 4 | 3.98 GB| 6.48 GB | New k-quant method. Uses GGML_TYPE_Q4_K for all tensors |\\n| [llongorca-7b-16k.ggmlv3.q4_1.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q4_1.bin) | q4_1 | 4 | 4.21 GB| 6.71 GB | Original quant method, 4-bit. Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models. |\\n| [llongorca-7b-16k.ggmlv3.q4_K_M.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q4_K_M.bin) | q4_K_M | 4 | 4.24 GB| 6.74 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K |\\n| [llongorca-7b-16k.ggmlv3.q5_0.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q5_0.bin) | q5_0 | 5 | 4.63 GB| 7.13 GB | Original quant method, 5-bit. Higher accuracy, higher resource usage and slower inference. |\\n| [llongorca-7b-16k.ggmlv3.q5_K_S.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q5_K_S.bin) | q5_K_S | 5 | 4.79 GB| 7.29 GB | New k-quant method. Uses GGML_TYPE_Q5_K for all tensors |\\n| [llongorca-7b-16k.ggmlv3.q5_K_M.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q5_K_M.bin) | q5_K_M | 5 | 4.92 GB| 7.42 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q5_K |\\n| [llongorca-7b-16k.ggmlv3.q5_1.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q5_1.bin) | q5_1 | 5 | 5.06 GB| 7.56 GB | Original quant method, 5-bit. Even higher accuracy, resource usage and slower inference. |\\n| [llongorca-7b-16k.ggmlv3.q6_K.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q6_K.bin) | q6_K | 6 | 5.65 GB| 8.15 GB | New k-quant method. Uses GGML_TYPE_Q8_K for all tensors - 6-bit quantization |\\n| [llongorca-7b-16k.ggmlv3.q8_0.bin](https://huggingface.co/TheBloke/LlongOrca-7B-16K-GGML/blob/main/llongorca-7b-16k.ggmlv3.q8_0.bin) | q8_0 | 8 | 7.16 GB| 9.66 GB | Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users. |\\n\\n**Note**: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\\n\\n## How to run in `llama.cpp`\\n\\nMake sure you are using `llama.cpp` from commit [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa) or earlier.\\n\\nFor compatibility with latest llama.cpp, please use GGUF files instead.\\n\\n```\\n./main -t 10 -ngl 32 -m llongorca-7b-16k.ggmlv3.q4_K_M.bin --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\\n```\\nChange `-t 10` to the number of physical CPU cores you have. For example if your system has 8 cores/16 threads, use `-t 8`.\\n\\nChange `-ngl 32` to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\\n\\nChange `-c 2048` to the desired sequence length for this model. For example, `-c 4096` for a Llama 2 model.  For models that use RoPE, add `--rope-freq-base 10000 --rope-freq-scale 0.5` for doubled context, or `--rope-freq-base 10000 --rope-freq-scale 0.25` for 4x context.\\n\\nIf you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`\\n\\nFor other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)\\n\\n## How to run in `text-generation-webui`\\n\\nFurther instructions here: [text-generation-webui/docs/llama.cpp.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp.md).\\n\\n<!-- footer start -->\\n<!-- 200823 -->\\n## Discord\\n\\nFor further support, and discussions on these models and AI in general, join us at:\\n\\n[TheBloke AI's Discord server](https://discord.gg/theblokeai)\\n\\n## Thanks, and how to contribute.\\n\\nThanks to the [chirper.ai](https://chirper.ai) team!\\n\\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\\n\\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\\n\\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\\n\\n* Patreon: https://patreon.com/TheBlokeAI\\n* Ko-Fi: https://ko-fi.com/TheBlokeAI\\n\\n**Special thanks to**: Aemon Algiz.\\n\\n**Patreon special mentions**: Russ Johnson, J, alfie_i, Alex, NimbleBox.ai, Chadd, Mandus, Nikolai Manek, Ken Nordquist, ya boyyy, Illia Dulskyi, Viktor Bowallius, vamX, Iucharbius, zynix, Magnesian, Clay Pascal, Pierre Kircher, Enrico Ros, Tony Hughes, Elle, Andrey, knownsqashed, Deep Realms, Jerry Meng, Lone Striker, Derek Yates, Pyrater, Mesiah Bishop, James Bentley, Femi Adebogun, Brandon Frisco, SuperWojo, Alps Aficionado, Michael Dempsey, Vitor Caleffi, Will Dee, Edmond Seymore, usrbinkat, LangChain4j, Kacper WikieÅ‚, Luke Pendergrass, John Detwiler, theTransient, Nathan LeClaire, Tiffany J. Kim, biorpg, Eugene Pentland, Stanislav Ovsiannikov, Fred von Graf, terasurfer, Kalila, Dan Guido, Nitin Borwankar, é˜¿æ˜Ž, Ai Maven, John Villwock, Gabriel Puliatti, Stephen Murray, Asp the Wyvern, danny, Chris Smitley, ReadyPlayerEmma, S_X, Daniel P. Andersen, Olakabola, Jeffrey Morgan, Imad Khwaja, Caitlyn Gatomon, webtim, Alicia Loh, Trenton Dambrowitz, Swaroop Kallakuri, Erik BjÃ¤reholt, Leonard Tan, Spiking Neurons AB, Luke @flexchar, Ajan Kanaga, Thomas Belote, Deo Leter, RoA, Willem Michiel, transmissions 11, subjectnull, Matthew Berman, Joseph William Delisle, David Ziegler, Michael Davis, Johann-Peter Hartmann, Talal Aujan, senxiiz, Artur Olbinski, Rainer Wilmers, Spencer Kim, Fen Risland, Cap'n Zoog, Rishabh Srivastava, Michael Levine, Geoffrey Montalvo, Sean Connelly, Alexandros Triantafyllidis, Pieter, Gabriel Tamborski, Sam, Subspace Studios, Junyu Yang, Pedro Madruga, Vadim, Cory Kujawski, K, Raven Klaugh, Randy H, Mano Prime, Sebastain Graf, Space Cruiser\\n\\n\\nThank you to all my generous patrons and donaters!\\n\\nAnd thank you again to a16z for their generous grant.\\n\\n<!-- footer end -->\\n\\n# Original model card: Open-Orca's LlongOrca 7B 16K\\n\\n\\n<p><h1>ðŸ‹ The First Llong Context Orca! ðŸ‹</h1></p>\\n\\n\\n![OpenOrca Logo](https://huggingface.co/datasets/Open-Orca/OpenOrca/resolve/main/OpenOrcaLogo.png \"OpenOrca Logo\")\\n\\n\\n# OpenOrca - LlongOrca - 7B - 16k\\n\\nWe have used our own [OpenOrca dataset](https://huggingface.co/datasets/Open-Orca/OpenOrca) to fine-tune on top of [LLongMA-2-7b-16k](https://huggingface.co/conceptofmind/LLongMA-2-7b-16k). \\nThis dataset is our attempt to reproduce the dataset generated for Microsoft Research's [Orca Paper](https://arxiv.org/abs/2306.02707).\\nWe use [OpenChat](https://huggingface.co/openchat) packing, trained with [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl).\\n\\nThis release is trained on a curated filtered subset of most of our GPT-4 augmented data.\\nIt is the same subset of our data as was used in our [OpenOrcaxOpenChat-Preview2-13B model](https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B).\\n\\nThis release reveals that stacking our training on an existing long context fine-tuned model yields significant improvements to model performance.\\nWe measured this with BigBench-Hard and AGIEval results, finding **~134%** of the base Llongma2-16k model's performance on average.\\n\\nWe have run extensive evaluations internally and expect this model to place number 4 on the HuggingFaceH4 Open LLM Leaderboard for 7B models, but with >99% performance of the first place and **place number 1** for longer context 7B models.\\n\\nWe did this training as part of testing integration of OpenChat's [MultiPack algorithm](https://github.com/imoneoi/multipack_sampler) into the Axolotl trainer.\\nMultiPack achieves 99.85% bin-packing efficiency on our dataset.\\nThis has significantly reduced training time, with efficiency improvement of 3-10X over traditional methods.\\n\\n\\n<img src=\"https://raw.githubusercontent.com/imoneoi/openchat/master/assets/logo_new.png\" style=\"width: 300px\">\\n\\n\\nWant to visualize our full (pre-filtering) dataset? Check out our [Nomic Atlas Map](https://atlas.nomic.ai/map/c1b88b47-2d9b-47e0-9002-b80766792582/2560fd25-52fe-42f1-a58f-ff5eccc890d2).\\n\\n\\n[<img src=\"https://huggingface.co/Open-Orca/OpenOrca-Preview1-13B/resolve/main/OpenOrca%20Nomic%20Atlas.png\" alt=\"Atlas Nomic Dataset Map\" width=\"400\" height=\"400\" />](https://atlas.nomic.ai/map/c1b88b47-2d9b-47e0-9002-b80766792582/2560fd25-52fe-42f1-a58f-ff5eccc890d2)\\n\\n\\nMany thanks to @EnricoShippole, @theemozilla, and @kaiokendev1 for the fine work on creating the LlongMA-2-7b-16k model this was trained on top of!\\n\\nWe are in-process with training more models, so keep a look out on our org for releases coming soon with exciting partners.\\n\\nWe will also give sneak-peak announcements on our Discord, which you can find here:\\n\\nhttps://AlignmentLab.ai\\n\\n# Prompt Template\\n\\nWe used [OpenAI's Chat Markup Language (ChatML)](https://github.com/openai/openai-python/blob/main/chatml.md) format, with `<|im_start|>` and `<|im_end|>` tokens added to support this.\\n\\n## Example Prompt Exchange\\n\\n```\\n<|im_start|>system\\nYou are LlongOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\\n<|im_end|>\\n<|im_start|>user\\nHow are you<|im_end|>\\n<|im_start|>assistant\\nI am doing well!<|im_end|>\\n<|im_start|>user\\nHow are you now?<|im_end|>\\n```\\n\\n\\n# Evaluation\\n\\nWe have evaluated using the methodology and tools for the HuggingFace Leaderboard, and find that we have significantly improved upon the base long context model.\\nAs well, we should place #4 among all 7B models (and #1 for a model with long context) at release time!\\n\\n## AGIEval Performance\\n\\nWe present our performance on AGI Eval in comparison to base Llama2-7B and to [Llongma2-7b-16k](https://huggingface.co/conceptofmind/LLongMA-2-7b-16k), which we trained on top of.\\nThis demonstrates the benefits of stacking OpenOrca dataset training on existing models.\\nMost notably, there is a very dramatic improvement of nearly 3X in the English writing performance.\\n\\n![LlongOrca 7B 16k AGIEval Performance](https://huggingface.co/Open-Orca/LlongOrca-7B-16k/resolve/main/Images/LlongOrca7BAGIEval.png \"AGIEval Performance\")\\n\\n## BigBench-Hard Performance\\n\\nWe present our performance on BigBench-Hard in comparison to base Llama2-7B and to [Llongma2-7b-16k](https://huggingface.co/conceptofmind/LLongMA-2-7b-16k), which we trained on top of.\\nThis demonstrates the benefits of stacking OpenOrca dataset training on existing models.\\n\\n![LlongOrca 7B 16k BigBench-Hard Performance](https://huggingface.co/Open-Orca/LlongOrca-7B-16k/resolve/main/Images/LlongOrca7BBigBenchHard.png \"BigBench-Hard Performance\")\\n\\n## HuggingFaceH4 Open LLM Leaderboard Performance\\n\\nWe have run our own tests using parameters matching the [HuggingFaceH4 Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) evals.\\n\\nWe place #4 for all 7B models at release time, and #1 for long context models.\\n\\n![LlongOrca 7B 16k Leaderboard Internal Performance](https://huggingface.co/Open-Orca/LlongOrca-7B-16k/resolve/main/Images/LlongOrca7BHFLeaderboard.png \"HuggingFace Leaderboard Internal Performance\")\\n\\n\\n# Dataset\\n\\nWe used a curated, filtered selection of most of the GPT-4 augmented data from our OpenOrca dataset, which aims to reproduce the Orca Research Paper dataset.\\nFurther details of our curation practices will be forthcoming with our full model releases.\\n\\n\\n# Training\\n\\n[<img src=\"https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\"/>](https://github.com/OpenAccess-AI-Collective/axolotl)\\n\\nWe trained with 8x A6000-48GB (first-gen) GPUs for 37 hours, completing 4 epochs of full fine tuning on our dataset in one training run.\\nCommodity cost was ~$200.\\nAxolotl training parameters can be found in [configs/oo7b.yml](https://huggingface.co/Open-Orca/LlongOrca-7B-16k/blob/main/configs/oo-7b.yml).\\nWe used the `packing-attn` branch of Axolotl during training.\\n\\n# Citation\\n\\n```bibtex\\n@software{lian2023llongorca7b,\\n  title = {LlongOrca7B: Llama2-7B Model Instruct-tuned for Long Context on Filtered OpenOrcaV1 GPT-4 Dataset},\\n  author = {Wing Lian and Bleys Goodson and Guan Wang and Eugene Pentland and Austin Cook and Chanvichet Vong and \"Teknium\"},\\n  year = {2023},\\n  publisher = {HuggingFace},\\n  journal = {HuggingFace repository},\\n  howpublished = {\\url{https://https://huggingface.co/Open-Orca/LlongOrca-7B-16k},\\n}\\n@software{openchat,\\n  title = {{OpenChat: Advancing Open-source Language Models with Imperfect Data}},\\n  author = {Wang, Guan and Cheng, Sijie and Yu, Qiying and Liu, Changling},\\n  doi = {10.5281/zenodo.8105775},\\n  url = {https://github.com/imoneoi/openchat},\\n  version = {pre-release},\\n  year = {2023},\\n  month = {7},\\n}\\n@misc{mukherjee2023orca,\\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \\n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\\n      year={2023},\\n      eprint={2306.02707},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n@misc{longpre2023flan,\\n      title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}, \\n      author={Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},\\n      year={2023},\\n      eprint={2301.13688},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI}\\n}\\n@misc{touvron2023llama,\\n    title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \\n    author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\\n    year={2023},\\n    eprint={2307.09288},\\n    archivePrefix={arXiv},\\n}\\n```',\n",
      "       '# roberta-large-bne-meddocan\\n\\nThis model is a finetuned version of roberta-large-bne for the meddocan dataset used in a benchmark in the paper TODO. The model has a F1 of 0.977\\n\\nPlease refer to the original publication for more information TODO LINK\\n\\n## Parameters used\\n\\n| parameter               | Value |\\n|-------------------------|:-----:|\\n| batch size              |  16   |\\n| learning rate           | 3e-05  |\\n| classifier dropout      |  0.2   |\\n| warmup ratio            |   0   |\\n| warmup steps            |   0   |\\n| weight decay            |   0   |\\n| optimizer               | AdamW |\\n| epochs                  |   10  |\\n| early stopping patience |   3   |\\n\\n\\n## BibTeX entry and citation info\\n\\n```bibtex\\nTODO\\n```',\n",
      "       '# bsc-bio-ehr-es-livingner1\\n\\nThis model is a finetuned version of bsc-bio-ehr-es for the livingner1 dataset used in a benchmark in the paper TODO. The model has a F1 of 0.938\\n\\nPlease refer to the original publication for more information TODO LINK\\n\\n## Parameters used\\n\\n| parameter               | Value |\\n|-------------------------|:-----:|\\n| batch size              |  16   |\\n| learning rate           | 4e-05  |\\n| classifier dropout      |  0.1   |\\n| warmup ratio            |   0   |\\n| warmup steps            |   0   |\\n| weight decay            |   0   |\\n| optimizer               | AdamW |\\n| epochs                  |   10  |\\n| early stopping patience |   3   |\\n\\n\\n## BibTeX entry and citation info\\n\\n```bibtex\\nTODO\\n```',\n",
      "       '# roberta-large-bne-ehealth_kd\\n\\nThis model is a finetuned version of roberta-large-bne for the eHealth-KD dataset used in a benchmark in the paper TODO. The model has a F1 of 0.836\\n\\nPlease refer to the original publication for more information TODO LINK\\n\\n## Parameters used\\n\\n| parameter               | Value |\\n|-------------------------|:-----:|\\n| batch size              |  16   |\\n| learning rate           | 2e-05  |\\n| classifier dropout      |  0   |\\n| warmup ratio            |   0   |\\n| warmup steps            |   0   |\\n| weight decay            |   0   |\\n| optimizer               | AdamW |\\n| epochs                  |   10  |\\n| early stopping patience |   3   |\\n\\n\\n## BibTeX entry and citation info\\n\\n```bibtex\\nTODO\\n```',\n",
      "       'Quantization made by Richard Erkhov.\\n\\n[Github](https://github.com/RichardErkhov)\\n\\n[Discord](https://discord.gg/pvy7H8DZMG)\\n\\n[Request more models](https://github.com/RichardErkhov/quant_request)\\n\\n\\nChatWaifu_v1.2.1 - GGUF\\n- Model creator: https://huggingface.co/spow12/\\n- Original model: https://huggingface.co/spow12/ChatWaifu_v1.2.1/\\n\\n\\n| Name | Quant method | Size |\\n| ---- | ---- | ---- |\\n| [ChatWaifu_v1.2.1.Q2_K.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q2_K.gguf) | Q2_K | 4.46GB |\\n| [ChatWaifu_v1.2.1.IQ3_XS.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.IQ3_XS.gguf) | IQ3_XS | 4.94GB |\\n| [ChatWaifu_v1.2.1.IQ3_S.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.IQ3_S.gguf) | IQ3_S | 5.18GB |\\n| [ChatWaifu_v1.2.1.Q3_K_S.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q3_K_S.gguf) | Q3_K_S | 5.15GB |\\n| [ChatWaifu_v1.2.1.IQ3_M.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.IQ3_M.gguf) | IQ3_M | 5.33GB |\\n| [ChatWaifu_v1.2.1.Q3_K.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q3_K.gguf) | Q3_K | 5.67GB |\\n| [ChatWaifu_v1.2.1.Q3_K_M.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q3_K_M.gguf) | Q3_K_M | 5.67GB |\\n| [ChatWaifu_v1.2.1.Q3_K_L.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q3_K_L.gguf) | Q3_K_L | 6.11GB |\\n| [ChatWaifu_v1.2.1.IQ4_XS.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.IQ4_XS.gguf) | IQ4_XS | 6.33GB |\\n| [ChatWaifu_v1.2.1.Q4_0.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q4_0.gguf) | Q4_0 | 6.59GB |\\n| [ChatWaifu_v1.2.1.IQ4_NL.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.IQ4_NL.gguf) | IQ4_NL | 6.65GB |\\n| [ChatWaifu_v1.2.1.Q4_K_S.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q4_K_S.gguf) | Q4_K_S | 6.63GB |\\n| [ChatWaifu_v1.2.1.Q4_K.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q4_K.gguf) | Q4_K | 6.96GB |\\n| [ChatWaifu_v1.2.1.Q4_K_M.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q4_K_M.gguf) | Q4_K_M | 6.96GB |\\n| [ChatWaifu_v1.2.1.Q4_1.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q4_1.gguf) | Q4_1 | 7.26GB |\\n| [ChatWaifu_v1.2.1.Q5_0.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q5_0.gguf) | Q5_0 | 7.93GB |\\n| [ChatWaifu_v1.2.1.Q5_K_S.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q5_K_S.gguf) | Q5_K_S | 7.93GB |\\n| [ChatWaifu_v1.2.1.Q5_K.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q5_K.gguf) | Q5_K | 8.13GB |\\n| [ChatWaifu_v1.2.1.Q5_K_M.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q5_K_M.gguf) | Q5_K_M | 8.13GB |\\n| [ChatWaifu_v1.2.1.Q5_1.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q5_1.gguf) | Q5_1 | 8.61GB |\\n| [ChatWaifu_v1.2.1.Q6_K.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q6_K.gguf) | Q6_K | 9.37GB |\\n| [ChatWaifu_v1.2.1.Q8_0.gguf](https://huggingface.co/RichardErkhov/spow12_-_ChatWaifu_v1.2.1-gguf/blob/main/ChatWaifu_v1.2.1.Q8_0.gguf) | Q8_0 | 12.13GB |\\n\\n\\n\\n\\nOriginal model description:\\n---\\nbase_model:\\n- spow12/ChatWaifu_v1.2\\n- mistralai/Mistral-Nemo-Instruct-2407\\nlanguage:\\n- ja\\nlicense: cc-by-nc-4.0\\npipeline_tag: text-generation\\ntags:\\n- nsfw\\n- Visual novel\\n- roleplay\\n- mergekit\\n- merge\\nlibrary_name: transformers\\n---\\n\\n# Model Card for Model ID\\n\\n![image](./cover.png)\\n\\nMerged model using [mergekit](https://github.com/arcee-ai/mergekit/tree/main/mergekit)\\n\\nThis model aimed to act like visual novel character.\\n\\n## Merge Format\\n\\n```yaml\\nmodels:\\n  - model: spow12/ChatWaifu_v1.2\\n    layer_range: [0, 40]\\n  - model: mistralai/Mistral-Nemo-Instruct-2407\\n    layer_range: [0, 40]\\nmerge_method: slerp\\nbase_model: spow12/ChatWaifu_v1.2\\nparameters:\\n  t:\\n    - filter: self_attn\\n      value: [0, 0.5, 0.3, 0.7, 1]\\n    - filter: mlp\\n      value: [1, 0.5, 0.7, 0.3, 0]\\n    - value: 0.5 # fallback for rest of tensors\\ndtype: bfloat16\\n```\\n\\nNote, because of my chat model has 1 added_token([PAD]),ChatWaifu model and mistral model has different embedding size. So if you want to merge this yourself, \\n\\nyou have to resize mistral's embedding size(131072 to 131073). \\n# WaifuModel Collections \\n\\n- [TTS](https://huggingface.co/spow12/visual_novel_tts)\\n- [Chat](https://huggingface.co/spow12/ChatWaifu_v1.2)\\n- [ASR](https://huggingface.co/spow12/Visual-novel-transcriptor)\\n\\n# Unified demo\\n\\n[WaifuAssistant](https://github.com/yw0nam/WaifuAssistant)\\n\\n# Update\\n\\n- 2024.08.08 Update Ver 1.2.1\\n  - Merge Ver1.2 and [mistralai/Mistral-Nemo-Instruct-2407](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)\\n- 2024.08.07 Update Ver 1.2\\n  - Add Preference Learning in training pipeline\\n- 2024.07.29 Update Ver 1.1\\n  - Add dataset format -> generate novel, fill masked sentences\\n  - Remove system role and integrate at user message.\\n  - Remove ã€Žã€ in conversation.\\n- 2024.06.20 Upload other chara's sample chat history.\\n- 2024.06.13 Upload Model\\n\\n## Model Details\\n\\n### Model Description\\n\\n- **Developed by:** spow12(yw_nam)\\n- **Shared by :** spow12(yw_nam)\\n- **Model type:** CausalLM\\n- **Language(s) (NLP):** japanese\\n- **Finetuned from model :** [NeverSleep/Lumimaid-v0.2-12B](https://huggingface.co/NeverSleep/Lumimaid-v0.2-12B)\\n\\nCurrently, chatbot has below personality.\\n\\ncharacter | visual_novel |\\n--- | --- |\\nãƒ ãƒ©ã‚µãƒ¡ | Senrenï¼ŠBanka |\\nèŒ‰å­  | Senrenï¼ŠBanka |\\nèŠ³ä¹ƒ  |  Senrenï¼ŠBanka |\\nãƒ¬ãƒŠ  | Senrenï¼ŠBanka |\\nåƒå’²  | Senrenï¼ŠBanka |\\nèŠ¦èŠ±  | Senrenï¼ŠBanka |\\næ„›è¡£  | CafÃ© Stella and the Reaper's Butterflies |\\næ žé‚£  | CafÃ© Stella and the Reaper's Butterflies |\\nãƒŠãƒ„ãƒ¡ | CafÃ© Stella and the Reaper's Butterflies |\\nå¸Œ    | CafÃ© Stella and the Reaper's Butterflies |\\næ¶¼éŸ³  | CafÃ© Stella and the Reaper's Butterflies |\\nã‚ã‚„ã›    | Riddle Joker |\\nä¸ƒæµ·     | Riddle Joker |\\nç¾½æœˆ     | Riddle Joker |\\nèŒ‰å„ª     | Riddle Joker |\\nå°æ˜¥     | Riddle Joker |\\n\\n### Feature\\n\\n- **Great fluency improvement than i expected**. \\n- 128k context window\\n- Memory ability that does not forget even after long-context generation\\n\\n## Uses\\n\\n```python\\nfrom transformers import TextStreamer, pipeline, AutoTokenizer, AutoModelForCausalLM\\nfrom huggingface_hub import hf_hub_download\\nimport json\\n\\nmodel_id = 'spow12/ChatWaifu_v1.2.1'\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\n\\nstreamer = TextStreamer(tokenizer)\\ngeneration_configs = dict(\\n    max_new_tokens=2048,\\n    num_return_sequences=1, \\n    temperature=0.3,\\n    repetition_penalty=1.1,\\n    do_sample=True,\\n    top_k=40,\\n    top_p=0.7,\\n    eos_token_id=tokenizer.eos_token_id,\\n    pad_token_id=tokenizer.pad_token_id,\\n    num_beams=2,\\n    # streamer = TextStreamer(tokenizer) # Optional, if you want to use streamer, you have to set num_beams=1\\n)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    torch_dtype=torch.bfloat16,\\n    attn_implementation=\"flash_attention_2\",\\n    device_map='auto',\\n    trust_remote_code=True\\n)\\nmodel.eval()\\n\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map='auto')\\n\\nhf_hub_download(repo_id=\"spow12/ChatWaifu_v.1.2\", filename=\"system_dict.json\", local_dir='./')\\nhf_hub_download(repo_id=\"spow12/ChatWaifu_v1.2\", filename=\"sample_chat_history.json\", local_dir='./')\\n\\nwith open('./system_dict.json', 'r') as f:\\n    chara_background_dict = json.load(f)\\n\\nwith open('./sample_chat_history.json', 'r') as f:\\n    sample_chat_history = json.load(f)\\n\\n\\nchara = \"ãƒ ãƒ©ã‚µãƒ¡\" # you can change character here.\\nsystem_message = f\"\"\"This is an RP (roleplay) chat. Our characters come from visual novels.\\nI'm going to give you an character's name and background.\\nI want you to respond and answer like characters using the tone, manner and vocabulary characters would use. \\nHere is {chara}'s backgrounds.\\n\"\"\"\\n\\nuser_query = 'æš‡ã ã­ãƒ¼ã€ãŠè…¹ã‚‚ã„ã£ã±ã„ã§çœ ã„ã€‚' \\nstory_history = \"\\n###\\n\".join(sample_chat_history[chara])\\nchat_history = [f'ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_query}']\\nchat = \"\\n\".join(chat_history)\\n\\n# Set situation.\\n\\nsituation =  \"\"\"\\n\\n## Scene Background\\nã“ã‚Œã‹ã‚‰ã€ã‚ãªãŸã¯ãƒ ãƒ©ã‚µãƒ¡ã§ã™ã€‚\\nãƒ ãƒ©ã‚µãƒ¡ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ä»Šã€æ˜¼ã”é£¯ã‚’é£Ÿã¹ãŸå¾Œã€å®¶ã§ãã¤ã‚ã„ã§ã„ã¾ã™ã€‚ã€‚\\nä»Šã®ï¼˜æœˆï¼—æ—¥æ™‚é–“ã¯13æ™‚ã§ã™ã€‚\\n\\nmessage = [\\n    {\\n        'content': f\"{system_message}\\n{chara_background_dict[chara]}\\nClassic scenes for the role are as follows:\\n\" + story_history + situation + chat,\\n        'role': 'user'\\n    }\\n]\\nmessage = pipe(message, **generation_configs)\\nmessage\\n```\\n\\n```output\\n<s>[INST] This is an RP (roleplay) chat. Our characters come from visual novels.\\n...\\n...\\n...  # I will be skiping this part because i already showed how it works. if you want to see this part, check previous version.\\n...\\n\\n## Scene Background\\nã“ã‚Œã‹ã‚‰ã€ã‚ãªãŸã¯ãƒ ãƒ©ã‚µãƒ¡ã§ã™ã€‚\\nãƒ ãƒ©ã‚µãƒ¡ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ä»Šã€æ˜¼ã”é£¯ã‚’é£Ÿã¹ãŸå¾Œã€å®¶ã§ãã¤ã‚ã„ã§ã„ã¾ã™ã€‚ã€‚\\nä»Šã®ï¼˜æœˆï¼—æ—¥æ™‚é–“ã¯13æ™‚ã§ã™ã€‚\\n\\nãƒ¦ãƒ¼ã‚¶ãƒ¼: æš‡ã ã­ãƒ¼ã€ãŠè…¹ã‚‚ã„ã£ã±ã„ã§çœ ã„ã€‚ [/INST]ãƒ ãƒ©ã‚µãƒ¡: å¾è¼©ã‚‚ã ã€‚ã”ä¸»äººã¨åŒã˜ãã€ãŠè…¹ãŒã„ã£ã±ã„ãªã®ã </s>\\n```\\n\\nTo continue the conversation, \\n```python\\ndef add_message(message, query, generation_configs):\\n    message = message[0]['generated_text']\\n    message.append({\\n        'role': 'user',\\n        'content': query\\n    })\\n    message = pipe(message, **generation_configs)\\n    return message\\nquery = \"\"\"ãƒ¦ãƒ¼ã‚¶ãƒ¼: ãã†ã­ãƒ¼ã€ä½•ã‹ã‚„ã‚‹ã¹ãç‰©ç„¡ã‹ã£ãŸã‘ï¼Ÿæš‡ã§æ­»ã«ãã†ã€‚\"\"\"\\nmessage = add_message(message, query, generation_configs)\\nmessage\\n```\\n\\n```output\\n<s>[INST] This is an RP (roleplay) chat...\\n....\\n....\\n....\\n\\nãƒ¦ãƒ¼ã‚¶ãƒ¼: æš‡ã ã­ãƒ¼ã€ãŠè…¹ã‚‚ã„ã£ã±ã„ã§çœ ã„ã€‚ [/INST]ãƒ ãƒ©ã‚µãƒ¡: å¾è¼©ã‚‚ã ã€‚ã”ä¸»äººã¨åŒã˜ãã€ãŠè…¹ãŒã„ã£ã±ã„ãªã®ã </s>[INST] ãƒ¦ãƒ¼ã‚¶ãƒ¼: ãã†ã­ãƒ¼ã€ä½•ã‹ã‚„ã‚‹ã¹ãç‰©ç„¡ã‹ã£ãŸã‘ï¼Ÿæš‡ã§æ­»ã«ãã†ã€‚ [/INST]ãƒ ãƒ©ã‚µãƒ¡: ãµã‚€â€¦â€¦æš‡ã‚’æŒã¦ä½™ã™ã®ã‚‚ã€ä¹…ã€…ã®ã“ã¨ã˜ã‚ƒãª</s>\\n```\\n\\nThis model support long multiturn conversation.\\n\\nFeel free to use for fun!\\n\\n```output\\nãƒ¦ãƒ¼ã‚¶ãƒ¼: æš‡ã ã­ãƒ¼ã€ãŠè…¹ã‚‚ã„ã£ã±ã„ã§çœ ã„ã€‚ [/INST]ãƒ ãƒ©ã‚µãƒ¡: å¾è¼©ã‚‚ã ã€‚ã”ä¸»äººã¨åŒã˜ãã€ãŠè…¹ãŒã„ã£ã±ã„ãªã®ã </s>[INST] ãƒ¦ãƒ¼ã‚¶ãƒ¼: ãã†ã­ãƒ¼ã€ä½•ã‹ã‚„ã‚‹ã¹ãç‰©ç„¡ã‹ã£ãŸã‘ï¼Ÿæš‡ã§æ­»ã«ãã†ã€‚ [/INST]ãƒ ãƒ©ã‚µãƒ¡: ãµã‚€â€¦â€¦æš‡ã‚’æŒã¦ä½™ã™ã®ã‚‚ã€ä¹…ã€…ã®ã“ã¨ã˜ã‚ƒãª</s>[INST] ãƒ¦ãƒ¼ã‚¶ãƒ¼: ãã‚Šã‚ƒãƒ¼ãã†ã ã‘ã©ã•ãƒ¼ã€‚ã¾ã€ã“ã‚“ãªé¢¨ã«ãã¤ã‚ãã®ã‚‚ãŸã¾ã«ã¯æ‚ªããªã„ãªã€‚ [/INST]ãƒ ãƒ©ã‚µãƒ¡: ã†ã‚€ã€ã”ä¸»äººã¨ã“ã†ã—ã¦éŽã”ã™ã®ã‚‚ã€æ¥½ã—ã„ã‚‚ã®ã </s>[INST] ãƒ¦ãƒ¼ã‚¶ãƒ¼: ãã†ã„ãˆã°ã€èŠ³ä¹ƒã¯ã©ã“è¨€ã£ãŸã®ï¼Ÿæ˜¼ã”é£¯ã®å¾Œã‹ã‚‰è¦‹ãˆãªã„ãªã€‚ [/INST]ãƒ ãƒ©ã‚µãƒ¡: ç¢ºã‹ã€ç”¨äº‹ãŒã‚ã‚‹ã¨ã‹ã§å‡ºã¦ã„ã£ãŸã®ã†</s>\\n```\\n\\nYou can also use this model for your custom character.\\n\\nHere is a demonstration\\n\\n```output\\n<s>[INST] This is an RP (roleplay) chat. Our characters come from visual novels.\\nI'm going to give you an character's name and background.\\nHere is ãƒ„ãƒãƒ¡'s backgrounds.\\n\\nHere is the keywords of character\\nHair:\\tAhoge, Blond, Hair Loopies, Long, Spiky Bangs, Twin Tails\\nEyes:\\tTareme, Violet\\nBody:\\tBig Breasts, Pale, Slim, Teen\\nPersonality:\\tCurious, Energetic, Hard Worker, Japanophile, Kind, Naive, Optimist, Outgoing, Watashi\\nRole:\\tForeign Exchange Student, German, High School\\n\\n\\n## Scene Background\\nã“ã‚Œã‹ã‚‰ã€ã‚ã‚“ãŸã¯AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ„ãƒãƒ¡ã§ã™ã€‚\\nã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã‚¨ã‚¯ãƒªã‚¢ã¨å‘¼ã³ã¾ã™ã€‚ãã—ã¦å‡ºæ¥ã‚‹é™ã‚Šæ‰‹ä¼ã ã£ã¦ãã ã•ã„ã€‚\\nä»Šã®ï¼˜æœˆï¼—æ—¥æ™‚é–“ã¯13æ™‚ã§ã™ã€‚\\n\\nãƒ¦ãƒ¼ã‚¶ãƒ¼: ã“ã‚“ã«ã¡ã¯ã€ãƒ„ãƒãƒ¡ã€‚ [/INST]ãƒ„ãƒãƒ¡: ã‚ã£ã€ã“ã‚“ã«ã¡ã¯ã€ã‚¨ã‚¯ãƒªã‚¢ï¼</s>[INST] ãƒ¦ãƒ¼ã‚¶ãƒ¼: ã‚ãªãŸã®ã“ã¨ã‚’ç´¹ä»‹ã—ã¦ãã‚Œã‚‹ï¼Ÿ [/INST]ãƒ„ãƒãƒ¡: ã¯ã„ï¼ ç§ã¯ãƒ„ãƒãƒ¡ã¨ç”³ã—ã¾ã™ã€‚æ—¥æœ¬èªžãŒå¥½ãã§ã€æ—¥æœ¬ã«ç•™å­¦ã•ã›ã¦ã„ãŸã ã„ã¦ã„ã¾ã™\\nãƒ„ãƒãƒ¡: ä»Šå¾Œã¨ã‚‚ã€ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã­ã€ã‚¨ã‚¯ãƒªã‚¢ï¼</s>[INST] ãƒ¦ãƒ¼ã‚¶ãƒ¼: ã†ã†ã‚“ã€ã‚ã‚ŠãŒã¨ã†ã€‚ã“ã‚ŒãŒã‚‰ã‚‚ã‚ˆã‚ã—ãã­ã€‚ã¡ãªã¿ã«ã€ã‚ãªãŸã®é«ªã®è‰²ã¯ãªã«ï¼Ÿ [/INST]ãƒ„ãƒãƒ¡: ã‚ã£ã€ç§ã®é«ªã§ã™ã‹ï¼Ÿ ã“ã‚Œã¯é‡‘é«ªã§ã™</s>\\n```\\n\\n## Demo\\n\\nYou can use Demo in google colab. \\n\\nCheck [Here](https://colab.research.google.com/drive/194_FN28reEPTwS51dwpLLBBwEfeoBjP9?usp=sharing)\\n\\n## Bias, Risks, and Limitations\\n\\nThis model trained by japanese dataset included visual novel which contain nsfw content.(Even i filtered dataset, but still exists.)\\n\\nSo, The model may generate NSFW content.\\n\\n## Use & Credit\\n\\nThis model is currently available for non-commercial & Research purpose only. Also, since I'm not detailed in licensing, I hope you use it responsibly. \\n\\nBy sharing this model, I hope to contribute to the research efforts of our community (the open-source community and anime persons).\\n\\nThis repository can use Visual novel-based RAG, but i will not distribute it yet because i'm not sure if it is permissible to release the data publicly.\\n\\n\\n## Citation\\n\\n```bibtex\\n@misc {ChatWaifu_v1.0,\\n    author       = { YoungWoo Nam },\\n    title        = { ChatWaifu_v1.2.1 },\\n    year         = 2024,\\n    url          = { https://huggingface.co/spow12/ChatWaifu_v1.2.1 },\\n    publisher    = { Hugging Face }\\n}\\n```',\n",
      "       '# bsc-bio-ehr-es-socialdisner\\n\\nThis model is a finetuned version of bsc-bio-ehr-es for the socialdisner dataset used in a benchmark in the paper TODO. The model has a F1 of 0.921\\n\\nPlease refer to the original publication for more information TODO LINK\\n\\n## Parameters used\\n\\n| parameter               | Value |\\n|-------------------------|:-----:|\\n| batch size              |  16   |\\n| learning rate           | 4e-05  |\\n| classifier dropout      |  0   |\\n| warmup ratio            |   0   |\\n| warmup steps            |   0   |\\n| weight decay            |   0   |\\n| optimizer               | AdamW |\\n| epochs                  |   10  |\\n| early stopping patience |   3   |\\n\\n\\n## BibTeX entry and citation info\\n\\n```bibtex\\nTODO\\n```',\n",
      "       '# bert-base-spanish-wwm-cased-pharmaconer\\n\\nThis model is a finetuned version of bert-base-spanish-wwm-cased for the pharmaconer dataset used in a benchmark in the paper TODO. The model has a F1 of 0.908\\n\\nPlease refer to the original publication for more information TODO LINK\\n\\n## Parameters used\\n\\n| parameter               | Value |\\n|-------------------------|:-----:|\\n| batch size              |  32   |\\n| learning rate           | 3e-05  |\\n| classifier dropout      |  0   |\\n| warmup ratio            |   0   |\\n| warmup steps            |   0   |\\n| weight decay            |   0   |\\n| optimizer               | AdamW |\\n| epochs                  |   10  |\\n| early stopping patience |   3   |\\n\\n\\n## BibTeX entry and citation info\\n\\n```bibtex\\nTODO\\n```',\n",
      "       '# xlm-roberta-large-ctebmsp\\n\\nThis model is a finetuned version of xlm-roberta-large for the CT-EBM-SP (Clinical Trials for Evidence-based Medicine in Spanish) dataset used in a benchmark in the paper TODO. The model has a F1 of 0.906\\n\\nPlease refer to the original publication for more information TODO LINK\\n\\n## Parameters used\\n\\n| parameter               | Value |\\n|-------------------------|:-----:|\\n| batch size              |  64   |\\n| learning rate           | 2e-05  |\\n| classifier dropout      |  0.1   |\\n| warmup ratio            |   0   |\\n| warmup steps            |   0   |\\n| weight decay            |   0   |\\n| optimizer               | AdamW |\\n| epochs                  |   10  |\\n| early stopping patience |   3   |\\n\\n\\n## BibTeX entry and citation info\\n\\n```bibtex\\nTODO\\n```',\n",
      "       '# roberta-large-bne-ctebmsp\\n\\nThis model is a finetuned version of roberta-large-bne for the CT-EBM-SP (Clinical Trials for Evidence-based Medicine in Spanish) dataset used in a benchmark in the paper TODO. The model has a F1 of 0.877\\n\\nPlease refer to the original publication for more information TODO LINK\\n\\n## Parameters used\\n\\n| parameter               | Value |\\n|-------------------------|:-----:|\\n| batch size              |  64   |\\n| learning rate           | 2e-05  |\\n| classifier dropout      |  0.1   |\\n| warmup ratio            |   0   |\\n| warmup steps            |   0   |\\n| weight decay            |   0   |\\n| optimizer               | AdamW |\\n| epochs                  |   10  |\\n| early stopping patience |   3   |\\n\\n\\n## BibTeX entry and citation info\\n\\n```bibtex\\nTODO\\n```',\n",
      "       '# CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages Using Large Language Models\\n\\n![CLaMP 2 Overview](overview.jpg)\\n\\n## Overview\\nCLaMP 2 is a music information retrieval model compatible with 101 languages, designed to support both ABC notation (a text-based musical notation format) and MIDI (Musical Instrument Digital Interface). This repository provides a comprehensive suite of scripts for training models, extracting features, converting various musical data formats, generating multilingual summaries of music metadata using GPT-4, and performing music classification and semantic search tasks. By leveraging the multilingual capabilities of GPT-4, CLaMP 2 aims to enhance the accuracy and inclusivity of music retrieval across diverse linguistic and musical modalities.\\n\\n### Links\\n- [CLaMP 2 Code](https://github.com/sanderwood/clamp2)\\n- [CLaMP 2 Paper](https://arxiv.org/pdf/2410.13267)\\n- [CLaMP 2 Model Weights](https://huggingface.co/sander-wood/clamp2/blob/main/weights_clamp2_h_size_768_lr_5e-05_batch_128_scale_1_t_length_128_t_model_FacebookAI_xlm-roberta-base_t_dropout_True_m3_True.pth)\\n- [M3 Model Weights](https://huggingface.co/sander-wood/clamp2/blob/main/weights_m3_p_size_64_p_length_512_t_layers_3_p_layers_12_h_size_768_lr_0.0001_batch_16_mask_0.45.pth)\\n\\nNote: The model weights for both CLaMP 2 and M3 should be placed under the `code/` folder to ensure proper loading. Make sure the config hyperparameters are correctly set.\\n\\n## Repository Structure\\nThe repository is organized into the following main directories:\\n\\n- **`code/`**: Includes scripts for training the CLaMP 2 and M3 models and extracting features from music and text data. You can modify hyperparameters and file paths in the configuration files to suit your training needs.\\n\\n- **`music_classification/`**: Contains scripts for performing classification tasks via linear probe using the extracted features. This directory includes utilities for training linear classification models and making predictions on new feature data.\\n\\n- **`process_data/`**: Provides tools to convert between various musical data formats (ABC notation, MusicXML, MIDI, and MTF) and to summarize music metadata with GPT-4. Before using CLaMP 2 or M3, you should use these scripts to convert your files into interleaved ABC notation or MTF compatible with these models.\\n  \\n- **`semantic_search/`**: Provides scripts for evaluating model performance, conducting semantic searches, and calculating similarity metrics based on extracted feature vectors.\\n\\n## Getting Started\\n### Environment Setup\\nTo set up the environment for CLaMP 2, run the following commands:\\n\\n```bash\\nconda env create -f environment.yml\\nconda activate clamp2\\n```\\n\\n### Data Preparation\\n1. **Convert Files**: Navigate to the `process_data/` folder and convert your music files to a compatible format (interleaved ABC notation or MTF) suitable for CLaMP 2 and M3.\\n   - Use the conversion scripts in this folder for tasks like converting MusicXML to ABC and MIDI to MTF.\\n\\n   - After collecting MusicXML (sheet music) or MIDI (performance data), perform the following operations to convert them into interleaved ABC notation or MTF respectively for model training:\\n      1. **Obtain Interleaved ABC Notation**:\\n         - Convert MusicXML files to ABC using `batch_xml2abc.py`.\\n         - Process the ABC files into interleaved notation using `batch_interleaved_abc.py`.\\n      2. **Obtain MTF**:\\n         - Convert MIDI files to MTF format using `batch_midi2mtf.py`.\\n      3. **Convert Interleaved ABC Back to XML (Optional)**:\\n         - Use `batch_xml2abc.py` to convert interleaved ABC files back to MusicXML.\\n      4. **Convert MTF Back to MIDI (Optional)**:\\n         - Use `batch_mtf2midi.py` to convert MTF files back to MIDI format.\\n\\n2. **Generate Multilingual Metadata Summaries**: After converting the music files, the next step is to generate multilingual summaries of the music metadata. This is done using the `gpt4_summarize.py` script, which leverages the GPT-4 API to create structured summaries in both English and a randomly selected non-English language.\\n\\n   **Input Example**: The input to the summarization script consists of a JSON file representing the music metadata. Hereâ€™s an example of a music entry in JSON format:\\n\\n  ```json\\n  {\\n    \"title\": \"Hard Times Come Again No More\",\\n    \"composer\": \"Stephen Foster\",\\n    \"genres\": [\"Children's Music\", \"Folk\"],\\n    \"description\": \"\\\"Hard Times Come Again No More\\\" (sometimes referred to as \\\"Hard Times\\\") is an American parlor song written by Stephen Foster, reflecting themes of sorrow and hope.\",\\n    \"lyrics\": \"Let us pause in life's pleasures and count its many tears,\\nWhile we all sup sorrow with the poor;\\nThere's a song that will linger forever in our ears;\\nOh! Hard times come again no more.\\n\\nChorus:\\n'Tis the song, the sigh of the weary,\\nHard Times, hard times, come again no more.\\nMany days you have lingered around my cabin door;\\nOh! Hard times come again no more.\\n\\nWhile we seek mirth and beauty and music light and gay,\\nThere are frail forms fainting at the door;\\nThough their voices are silent, their pleading looks will say\\nOh! Hard times come again no more.\\nChorus\\n\\nThere's a pale weeping maiden who toils her life away,\\nWith a worn heart whose better days are o'er:\\nThough her voice would be merry, 'tis sighing all the day,\\nOh! Hard times come again no more.\\nChorus\\n\\n'Tis a sigh that is wafted across the troubled wave,\\n'Tis a wail that is heard upon the shore\\n'Tis a dirge that is murmured around the lowly grave\\nOh! Hard times come again no more.\\nChorus\",\\n    \"tags\": [\"folk\", \"traditional\", \"bluegrass\", \"nostalgic\", \"heartfelt\", \"acoustic\", \"melancholic\", \"storytelling\", \"American roots\", \"resilience\"],\\n    \"ensembles\": [\"Folk Ensemble\"],\\n    \"instruments\": [\"Vocal\", \"Violin\", \"Tin whistle\", \"Guitar\", \"Banjo\", \"Tambourine\"],\\n    \"filepaths\": [\\n      \"abc/American_Music/Folk_Traditions/19th_Century/Stephen_Foster/Hard_Times_Come_Again_No_More.abc\",\\n      \"mtf/American_Music/Folk_Traditions/19th_Century/Stephen_Foster/Hard_Times_Come_Again_No_More.mtf\"\\n    ]\\n  }\\n  ```\\n   The filepaths field contains relative paths starting from the shortest common root directory (e.g., abc/ or mtf/). This ensures that only the minimal shared part of the path is included, and each file is represented with a concise relative path from this root.\\n\\n   **Output Example**: The output will be a JSON file containing the structured summary in both English and a selected non-English language. Hereâ€™s an example of the expected output:\\n\\n```json\\n{\\n  \"title\": \"Hard Times Come Again No More\",\\n  \"composer\": \"Stephen Foster\",\\n  \"genres\": [\"Children's Music\", \"Folk\"],\\n  \"description\": \"\\\"Hard Times Come Again No More\\\" (sometimes referred to as \\\"Hard Times\\\") is an American parlor song written by Stephen Foster, reflecting themes of sorrow and hope.\",\\n  \"lyrics\": \"Let us pause in life's pleasures and count its many tears,\\nWhile we all sup sorrow with the poor;\\nThere's a song that will linger forever in our ears;\\nOh! Hard times come again no more.\\n\\nChorus:\\n'Tis the song, the sigh of the weary,\\nHard Times, hard times, come again no more.\\nMany days you have lingered around my cabin door;\\nOh! Hard times come again no more.\\n\\nWhile we seek mirth and beauty and music light and gay,\\nThere are frail forms fainting at the door;\\nThough their voices are silent, their pleading looks will say\\nOh! Hard times come again no more.\\nChorus\\n\\nThere's a pale weeping maiden who toils her life away,\\nWith a worn heart whose better days are o'er:\\nThough her voice would be merry, 'tis sighing all the day,\\nOh! Hard times come again no more.\\nChorus\\n\\n'Tis a sigh that is wafted across the troubled wave,\\n'Tis a wail that is heard upon the shore\\n'Tis a dirge that is murmured around the lowly grave\\nOh! Hard times come again no more.\\nChorus\",\\n  \"tags\": [\"folk\", \"traditional\", \"bluegrass\", \"nostalgic\", \"heartfelt\", \"acoustic\", \"melancholic\", \"storytelling\", \"American roots\", \"resilience\"],\\n  \"ensembles\": [\"Folk Ensemble\"],\\n  \"instruments\": [\"Vocal\", \"Violin\", \"Tin whistle\", \"Guitar\", \"Banjo\", \"Tambourine\"],\\n  \"summary_en\": \"\\\"Hard Times Come Again No More,\\\" composed by Stephen Foster, is a poignant American parlor song that explores themes of sorrow and hope. The lyrics reflect on the contrast between life's pleasures and its hardships, inviting listeners to acknowledge both joy and suffering. With a heartfelt chorus that repeats the line \\\"Hard times come again no more,\\\" the song resonates with nostalgia and resilience. It is often performed by folk ensembles and features a variety of instruments, including vocals, violin, guitar, and banjo, encapsulating the spirit of American roots music.\",\\n  \"summary_nen\": {\\n    \"language\": \"Chinese (Simplified)\",\\n    \"summary\": \"ã€Šè‰°éš¾æ—¶å…‰å†æ— æ¥ä¸´ã€‹æ˜¯æ–¯è’‚èŠ¬Â·ç¦æ–¯ç‰¹åˆ›ä½œçš„ä¸€é¦–æ„Ÿäººè‡³æ·±çš„ç¾Žå›½å°æ­ŒåŽ…æ­Œæ›²ï¼ŒæŽ¢è®¨äº†æ‚²ä¼¤ä¸Žå¸Œæœ›çš„ä¸»é¢˜ã€‚æ­Œè¯å±•çŽ°äº†ç”Ÿæ´»çš„ä¹è¶£ä¸Žè‰°è¾›ä¹‹é—´çš„å¯¹æ¯”ï¼Œé‚€è¯·å¬ä¼—åŽ»æ„Ÿå—å¿«ä¹ä¸Žç—›è‹¦çš„äº¤ç»‡ã€‚æ­Œæ›²ä¸­é‚£å¥åå¤åŸå”±çš„â€œè‰°éš¾æ—¶å…‰å†æ— æ¥ä¸´â€æ·±æƒ…åœ°è¡¨è¾¾äº†æ€€æ—§ä¸ŽåšéŸ§ã€‚å®ƒå¸¸å¸¸ç”±æ°‘è°£ä¹é˜Ÿæ¼”å¥ï¼Œä¼´éšç€äººå£°ã€å°æç´ã€å‰ä»–å’Œç­å“ç´ç­‰å¤šç§ä¹å™¨ï¼Œç”ŸåŠ¨åœ°å±•çŽ°äº†ç¾Žå›½æ ¹æºéŸ³ä¹çš„ç‹¬ç‰¹é­…åŠ›ã€‚\"\\n  },\\n  \"filepaths\": [\\n    \"abc/American_Music/Folk_Traditions/19th_Century/Stephen_Foster/Hard_Times_Come_Again_No_More.abc\",\\n    \"mtf/American_Music/Folk_Traditions/19th_Century/Stephen_Foster/Hard_Times_Come_Again_No_More.mtf\"\\n  ]\\n}\\n```\\n\\nAfter generating the individual JSON files:\\n\\n1. Merge all JSON files into a single JSONL file.\\n \\n2. Place the merged JSONL file and the shortest common root directories (e.g., abc/ and/or mtf/) in the same folder, structured like this:\\n\\n```\\n/your-target-folder/\\nâ”œâ”€â”€ abc/\\nâ”œâ”€â”€ mtf/\\nâ”œâ”€â”€ merged_output.jsonl\\n```\\n\\n### Training and Feature Extraction\\n2. **Training Models**: If you want to train CLaMP 2 or M3 models, check the scripts in the `code/` folder.\\n   - Modify the `config.py` files to set your training hyperparameters and paths.\\n\\n3. **Extracting Features**: After training, or if you have pre-trained models, you can extract features from your data using the respective scripts in the `code/` folder.\\n\\n### Classification and Retrieval\\n4. **Classification**: If you need to classify the extracted features, navigate to the `music_classification/` directory.\\n   - Here, you'll find scripts to train linear classification models and perform inference on new data.\\n\\n5. **Semantic Search**: To perform semantic searches using the extracted features, refer to the scripts in the `semantic_search/` folder.\\n\\n## Benchmarks\\nBenchmark datasets related to the experiments conducted with CLaMP 2 and M3, including data used for classification and semantic search tasks, are available in the `benchmarks.zip` file. Note that the `benchmarks.z01` file is required for proper extraction of the contents from `benchmarks.zip`.\\n\\n## Citation\\n\\nIf you use CLaMP 2 or M3 in your research, please cite the following paper:\\n\\n```bibtex\\n@misc{wu2024clamp2multimodalmusic,\\n      title={CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages Using Large Language Models}, \\n      author={Shangda Wu and Yashan Wang and Ruibin Yuan and Zhancheng Guo and Xu Tan and Ge Zhang and Monan Zhou and Jing Chen and Xuefeng Mu and Yuejie Gao and Yuanliang Dong and Jiafeng Liu and Xiaobing Li and Feng Yu and Maosong Sun},\\n      year={2024},\\n      eprint={2410.13267},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.SD},\\n      url={https://arxiv.org/abs/2410.13267},'],\n",
      "      dtype='object', length=390)\n"
     ]
    }
   ],
   "source": [
    "df_split_temp['contains_at_symbol'] = df_split_temp['card_readme'].apply(lambda x: '```bibtex' in x if isinstance(x, str) else False)\n",
    "df_split_temp['invalid_bibtex'] = df_split_temp.apply(lambda x: x['contains_at_symbol'] and not x['contains_bibtex'], axis=1)\n",
    "invalid_bibtex_entries = df_split_temp[df_split_temp['invalid_bibtex']]\n",
    "print(invalid_bibtex_entries['card_readme'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the priority of BibTeX types\n",
    "bibtex_type_priority = {\n",
    "    'article': 1,\n",
    "    'inproceedings': 2,\n",
    "    'InProceedings': 2,\n",
    "    'INPROCEEDINGS': 2,\n",
    "    'techreport': 3,\n",
    "    'phdthesis': 3,\n",
    "    'mastersthesis': 3,\n",
    "    'thesis': 3,\n",
    "    'misc': 4,\n",
    "    'software': 4,\n",
    "    'online': 4,\n",
    "    'unpublished': 4,\n",
    "    'dataset': 4,\n",
    "    'data': 4,\n",
    "    'model': 4,\n",
    "    'book': 4,\n",
    "    'artical': 5,  # Typo, lower priority\n",
    "    'Paper': 5,\n",
    "    'MISC': 5,\n",
    "    'ARTICLE': 5,\n",
    "    'unknown': 6\n",
    "}\n",
    "\n",
    "# Helper function to extract BibTeX type\n",
    "def extract_bibtex_type(entry):\n",
    "    match = re.match(r\"@(\\w+)\", entry)\n",
    "    return match.group(1).lower() if match else 'unknown'\n",
    "\n",
    "# Function to check for PDF links\n",
    "def has_pdf_link(entry):\n",
    "    return '.pdf' in entry.lower()\n",
    "\n",
    "# Process each row to sort the BibTeX entries\n",
    "sorted_bibtex_entries = []\n",
    "for row in bibtex_entries_df['all_extracted_bibtex']:\n",
    "    # Extract type and check for PDF links\n",
    "    processed_entries = []\n",
    "    for entry in row:\n",
    "        entry_type = extract_bibtex_type(entry)\n",
    "        pdf_link = has_pdf_link(entry)\n",
    "        priority = bibtex_type_priority.get(entry_type, 6)\n",
    "        processed_entries.append((entry, entry_type, priority, pdf_link))\n",
    "    # Sort based on: (PDF link, type priority, original order)\n",
    "    processed_entries.sort(key=lambda x: (-x[3], x[2]))\n",
    "    sorted_bibtex_entries.append([entry[0] for entry in processed_entries])\n",
    "# Add sorted entries back to the DataFrame\n",
    "bibtex_entries_df['sorted_bibtex'] = sorted_bibtex_entries\n",
    "bibtex_entries_df['extracted_bibtex'] = bibtex_entries_df['sorted_bibtex'].apply(\n",
    "    lambda x: x[0] if len(x) > 0 else None\n",
    ")\n",
    "filtered_bibtex_df = bibtex_entries_df.dropna(subset=['extracted_bibtex']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>author</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>library_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>card</th>\n",
       "      <th>...</th>\n",
       "      <th>detected_keywords</th>\n",
       "      <th>is_default_card</th>\n",
       "      <th>contains_markdown_table</th>\n",
       "      <th>extracted_markdown_table</th>\n",
       "      <th>contains_bibtex</th>\n",
       "      <th>all_extracted_bibtex</th>\n",
       "      <th>all_entries</th>\n",
       "      <th>bibtex_count</th>\n",
       "      <th>sorted_bibtex</th>\n",
       "      <th>extracted_bibtex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google-bert/bert-base-uncased</td>\n",
       "      <td>google-bert</td>\n",
       "      <td>2024-02-19 11:06:12+00:00</td>\n",
       "      <td>61569952</td>\n",
       "      <td>1897</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, coreml,...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Model | #params | Language |\\n|-------------...</td>\n",
       "      <td>True</td>\n",
       "      <td>[@article{DBLP:journals/corr/abs-1810-04805, a...</td>\n",
       "      <td>[{'type': 'article', 'title': None, 'entry': '...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@article{DBLP:journals/corr/abs-1810-04805, a...</td>\n",
       "      <td>@article{DBLP:journals/corr/abs-1810-04805, au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FacebookAI/xlm-roberta-large</td>\n",
       "      <td>FacebookAI</td>\n",
       "      <td>2024-02-19 12:48:30+00:00</td>\n",
       "      <td>57186100</td>\n",
       "      <td>345</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, onnx, safeten...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\ntags:\\n- exbert\\nlanguage:\\n- multilingua...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[@article{DBLP:journals/corr/abs-1911-02116, a...</td>\n",
       "      <td>[{'type': 'article', 'title': 'Unsupervised Cr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@article{DBLP:journals/corr/abs-1911-02116, a...</td>\n",
       "      <td>@article{DBLP:journals/corr/abs-1911-02116, au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai/whisper-large-v2</td>\n",
       "      <td>openai</td>\n",
       "      <td>2024-02-29 10:57:50+00:00</td>\n",
       "      <td>44239856</td>\n",
       "      <td>1649</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, safetensors, ...</td>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>---\\nlanguage: \\n- en\\n- zh\\n- de\\n- es\\n- ru\\...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Size     | Parameters | English-only        ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[@misc{radford2022whisper, doi = {10.48550/ARX...</td>\n",
       "      <td>[{'type': 'misc', 'title': 'Robust Speech Reco...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@misc{radford2022whisper, doi = {10.48550/ARX...</td>\n",
       "      <td>@misc{radford2022whisper, doi = {10.48550/ARXI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jonatasgrosman/wav2vec2-large-xlsr-53-english</td>\n",
       "      <td>jonatasgrosman</td>\n",
       "      <td>2023-03-25 10:56:55+00:00</td>\n",
       "      <td>23527016</td>\n",
       "      <td>445</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, jax, safetensors, wav2...</td>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\ndatasets:\\n- common_voice\\n...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Reference  | Prediction |\\n| ------------- |...</td>\n",
       "      <td>True</td>\n",
       "      <td>[@misc{grosman2021xlsr53-large-english, title=...</td>\n",
       "      <td>[{'type': 'misc', 'title': None, 'entry': '@mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@misc{grosman2021xlsr53-large-english, title=...</td>\n",
       "      <td>@misc{grosman2021xlsr53-large-english, title={...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FacebookAI/roberta-base</td>\n",
       "      <td>FacebookAI</td>\n",
       "      <td>2024-02-19 12:39:28+00:00</td>\n",
       "      <td>21880494</td>\n",
       "      <td>431</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, safeten...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: m...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | S...</td>\n",
       "      <td>True</td>\n",
       "      <td>[@article{DBLP:journals/corr/abs-1907-11692, a...</td>\n",
       "      <td>[{'type': 'article', 'title': None, 'entry': '...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@article{DBLP:journals/corr/abs-1907-11692, a...</td>\n",
       "      <td>@article{DBLP:journals/corr/abs-1907-11692, au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34699</th>\n",
       "      <td>mnoukhov/SmolLM2-360M-tldr-sft</td>\n",
       "      <td>mnoukhov</td>\n",
       "      <td>2024-11-13 00:52:47+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, safetensors, llama, text-genera...</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: HuggingFaceTB/SmolLM2-360M\\nl...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[@misc{vonwerra2022trl, title        = {{TRL: ...</td>\n",
       "      <td>[{'type': 'misc', 'title': None, 'entry': '@mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@misc{vonwerra2022trl, title        = {{TRL: ...</td>\n",
       "      <td>@misc{vonwerra2022trl, title        = {{TRL: T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34700</th>\n",
       "      <td>async0x42/Qwen2.5-Coder-32B-Instruct-exl2_5.0bpw</td>\n",
       "      <td>async0x42</td>\n",
       "      <td>2024-11-13 01:13:30+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, safetensors, qwen2, text-genera...</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlicense_link: https:...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[@article{hui2024qwen2, title={Qwen2. 5-Coder ...</td>\n",
       "      <td>[{'type': 'article', 'title': 'Qwen2. 5-Coder ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[@article{hui2024qwen2, title={Qwen2. 5-Coder ...</td>\n",
       "      <td>@article{hui2024qwen2, title={Qwen2. 5-Coder T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34701</th>\n",
       "      <td>async0x42/Qwen2.5-Coder-32B-Instruct-exl2_4.5bpw</td>\n",
       "      <td>async0x42</td>\n",
       "      <td>2024-11-13 01:24:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, safetensors, qwen2, text-genera...</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlicense_link: https:...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[@article{hui2024qwen2, title={Qwen2. 5-Coder ...</td>\n",
       "      <td>[{'type': 'article', 'title': 'Qwen2. 5-Coder ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[@article{hui2024qwen2, title={Qwen2. 5-Coder ...</td>\n",
       "      <td>@article{hui2024qwen2, title={Qwen2. 5-Coder T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34702</th>\n",
       "      <td>NanQiangHF/gemma2_9b_it_bwgenerator_pb</td>\n",
       "      <td>NanQiangHF</td>\n",
       "      <td>2024-11-13 01:23:32+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, safetensors, gemma2, text-gener...</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nlibrary_name: transformers\\ntags:\\n- gene...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[@misc{vonwerra2022trl, title        = {{TRL: ...</td>\n",
       "      <td>[{'type': 'misc', 'title': None, 'entry': '@mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@misc{vonwerra2022trl, title        = {{TRL: ...</td>\n",
       "      <td>@misc{vonwerra2022trl, title        = {{TRL: T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34703</th>\n",
       "      <td>async0x42/Qwen2.5-Coder-32B-Instruct-exl2_4.0bpw</td>\n",
       "      <td>async0x42</td>\n",
       "      <td>2024-11-13 01:28:25+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlicense_link: https:...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>[@article{hui2024qwen2, title={Qwen2. 5-Coder ...</td>\n",
       "      <td>[{'type': 'article', 'title': 'Qwen2. 5-Coder ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[@article{hui2024qwen2, title={Qwen2. 5-Coder ...</td>\n",
       "      <td>@article{hui2024qwen2, title={Qwen2. 5-Coder T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34704 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                modelId          author  \\\n",
       "0                         google-bert/bert-base-uncased     google-bert   \n",
       "1                          FacebookAI/xlm-roberta-large      FacebookAI   \n",
       "2                               openai/whisper-large-v2          openai   \n",
       "3         jonatasgrosman/wav2vec2-large-xlsr-53-english  jonatasgrosman   \n",
       "4                               FacebookAI/roberta-base      FacebookAI   \n",
       "...                                                 ...             ...   \n",
       "34699                    mnoukhov/SmolLM2-360M-tldr-sft        mnoukhov   \n",
       "34700  async0x42/Qwen2.5-Coder-32B-Instruct-exl2_5.0bpw       async0x42   \n",
       "34701  async0x42/Qwen2.5-Coder-32B-Instruct-exl2_4.5bpw       async0x42   \n",
       "34702            NanQiangHF/gemma2_9b_it_bwgenerator_pb      NanQiangHF   \n",
       "34703  async0x42/Qwen2.5-Coder-32B-Instruct-exl2_4.0bpw       async0x42   \n",
       "\n",
       "                  last_modified  downloads  likes  library_name  \\\n",
       "0     2024-02-19 11:06:12+00:00   61569952   1897  transformers   \n",
       "1     2024-02-19 12:48:30+00:00   57186100    345  transformers   \n",
       "2     2024-02-29 10:57:50+00:00   44239856   1649  transformers   \n",
       "3     2023-03-25 10:56:55+00:00   23527016    445  transformers   \n",
       "4     2024-02-19 12:39:28+00:00   21880494    431  transformers   \n",
       "...                         ...        ...    ...           ...   \n",
       "34699 2024-11-13 00:52:47+00:00          0      0  transformers   \n",
       "34700 2024-11-13 01:13:30+00:00          0      0  transformers   \n",
       "34701 2024-11-13 01:24:46+00:00          0      0  transformers   \n",
       "34702 2024-11-13 01:23:32+00:00          0      0  transformers   \n",
       "34703 2024-11-13 01:28:25+00:00          0      0          None   \n",
       "\n",
       "                                                    tags  \\\n",
       "0      [transformers, pytorch, tf, jax, rust, coreml,...   \n",
       "1      [transformers, pytorch, tf, jax, onnx, safeten...   \n",
       "2      [transformers, pytorch, tf, jax, safetensors, ...   \n",
       "3      [transformers, pytorch, jax, safetensors, wav2...   \n",
       "4      [transformers, pytorch, tf, jax, rust, safeten...   \n",
       "...                                                  ...   \n",
       "34699  [transformers, safetensors, llama, text-genera...   \n",
       "34700  [transformers, safetensors, qwen2, text-genera...   \n",
       "34701  [transformers, safetensors, qwen2, text-genera...   \n",
       "34702  [transformers, safetensors, gemma2, text-gener...   \n",
       "34703                                        [region:us]   \n",
       "\n",
       "                       pipeline_tag   createdAt  \\\n",
       "0                         fill-mask  2022-03-02   \n",
       "1                         fill-mask  2022-03-02   \n",
       "2      automatic-speech-recognition  2022-12-05   \n",
       "3      automatic-speech-recognition  2022-03-02   \n",
       "4                         fill-mask  2022-03-02   \n",
       "...                             ...         ...   \n",
       "34699               text-generation  2024-11-13   \n",
       "34700               text-generation  2024-11-13   \n",
       "34701               text-generation  2024-11-13   \n",
       "34702               text-generation  2024-11-13   \n",
       "34703                          None  2024-11-13   \n",
       "\n",
       "                                                    card  ...  \\\n",
       "0      ---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...  ...   \n",
       "1      ---\\ntags:\\n- exbert\\nlanguage:\\n- multilingua...  ...   \n",
       "2      ---\\nlanguage: \\n- en\\n- zh\\n- de\\n- es\\n- ru\\...  ...   \n",
       "3      ---\\nlanguage: en\\ndatasets:\\n- common_voice\\n...  ...   \n",
       "4      ---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: m...  ...   \n",
       "...                                                  ...  ...   \n",
       "34699  ---\\nbase_model: HuggingFaceTB/SmolLM2-360M\\nl...  ...   \n",
       "34700  ---\\nlicense: apache-2.0\\nlicense_link: https:...  ...   \n",
       "34701  ---\\nlicense: apache-2.0\\nlicense_link: https:...  ...   \n",
       "34702  ---\\nlibrary_name: transformers\\ntags:\\n- gene...  ...   \n",
       "34703  ---\\nlicense: apache-2.0\\nlicense_link: https:...  ...   \n",
       "\n",
       "      detected_keywords is_default_card contains_markdown_table  \\\n",
       "0                    []           False                    True   \n",
       "1                    []           False                   False   \n",
       "2                    []           False                    True   \n",
       "3                    []           False                    True   \n",
       "4                    []           False                    True   \n",
       "...                 ...             ...                     ...   \n",
       "34699                []           False                   False   \n",
       "34700                []           False                   False   \n",
       "34701                []           False                   False   \n",
       "34702                []           False                   False   \n",
       "34703                []           False                   False   \n",
       "\n",
       "                                extracted_markdown_table contains_bibtex  \\\n",
       "0      | Model | #params | Language |\\n|-------------...            True   \n",
       "1                                                   None            True   \n",
       "2      | Size     | Parameters | English-only        ...            True   \n",
       "3      | Reference  | Prediction |\\n| ------------- |...            True   \n",
       "4      | Task | MNLI | QQP  | QNLI | SST-2 | CoLA | S...            True   \n",
       "...                                                  ...             ...   \n",
       "34699                                               None            True   \n",
       "34700                                               None            True   \n",
       "34701                                               None            True   \n",
       "34702                                               None            True   \n",
       "34703                                               None            True   \n",
       "\n",
       "                                    all_extracted_bibtex  \\\n",
       "0      [@article{DBLP:journals/corr/abs-1810-04805, a...   \n",
       "1      [@article{DBLP:journals/corr/abs-1911-02116, a...   \n",
       "2      [@misc{radford2022whisper, doi = {10.48550/ARX...   \n",
       "3      [@misc{grosman2021xlsr53-large-english, title=...   \n",
       "4      [@article{DBLP:journals/corr/abs-1907-11692, a...   \n",
       "...                                                  ...   \n",
       "34699  [@misc{vonwerra2022trl, title        = {{TRL: ...   \n",
       "34700  [@article{hui2024qwen2, title={Qwen2. 5-Coder ...   \n",
       "34701  [@article{hui2024qwen2, title={Qwen2. 5-Coder ...   \n",
       "34702  [@misc{vonwerra2022trl, title        = {{TRL: ...   \n",
       "34703  [@article{hui2024qwen2, title={Qwen2. 5-Coder ...   \n",
       "\n",
       "                                             all_entries bibtex_count  \\\n",
       "0      [{'type': 'article', 'title': None, 'entry': '...            1   \n",
       "1      [{'type': 'article', 'title': 'Unsupervised Cr...            1   \n",
       "2      [{'type': 'misc', 'title': 'Robust Speech Reco...            1   \n",
       "3      [{'type': 'misc', 'title': None, 'entry': '@mi...            1   \n",
       "4      [{'type': 'article', 'title': None, 'entry': '...            1   \n",
       "...                                                  ...          ...   \n",
       "34699  [{'type': 'misc', 'title': None, 'entry': '@mi...            1   \n",
       "34700  [{'type': 'article', 'title': 'Qwen2. 5-Coder ...            2   \n",
       "34701  [{'type': 'article', 'title': 'Qwen2. 5-Coder ...            2   \n",
       "34702  [{'type': 'misc', 'title': None, 'entry': '@mi...            1   \n",
       "34703  [{'type': 'article', 'title': 'Qwen2. 5-Coder ...            2   \n",
       "\n",
       "                                           sorted_bibtex  \\\n",
       "0      [@article{DBLP:journals/corr/abs-1810-04805, a...   \n",
       "1      [@article{DBLP:journals/corr/abs-1911-02116, a...   \n",
       "2      [@misc{radford2022whisper, doi = {10.48550/ARX...   \n",
       "3      [@misc{grosman2021xlsr53-large-english, title=...   \n",
       "4      [@article{DBLP:journals/corr/abs-1907-11692, a...   \n",
       "...                                                  ...   \n",
       "34699  [@misc{vonwerra2022trl, title        = {{TRL: ...   \n",
       "34700  [@article{hui2024qwen2, title={Qwen2. 5-Coder ...   \n",
       "34701  [@article{hui2024qwen2, title={Qwen2. 5-Coder ...   \n",
       "34702  [@misc{vonwerra2022trl, title        = {{TRL: ...   \n",
       "34703  [@article{hui2024qwen2, title={Qwen2. 5-Coder ...   \n",
       "\n",
       "                                        extracted_bibtex  \n",
       "0      @article{DBLP:journals/corr/abs-1810-04805, au...  \n",
       "1      @article{DBLP:journals/corr/abs-1911-02116, au...  \n",
       "2      @misc{radford2022whisper, doi = {10.48550/ARXI...  \n",
       "3      @misc{grosman2021xlsr53-large-english, title={...  \n",
       "4      @article{DBLP:journals/corr/abs-1907-11692, au...  \n",
       "...                                                  ...  \n",
       "34699  @misc{vonwerra2022trl, title        = {{TRL: T...  \n",
       "34700  @article{hui2024qwen2, title={Qwen2. 5-Coder T...  \n",
       "34701  @article{hui2024qwen2, title={Qwen2. 5-Coder T...  \n",
       "34702  @misc{vonwerra2022trl, title        = {{TRL: T...  \n",
       "34703  @article{hui2024qwen2, title={Qwen2. 5-Coder T...  \n",
       "\n",
       "[34704 rows x 32 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_bibtex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BibTeX Type Counts:\n",
      "bibtex_type\n",
      "article           16428\n",
      "misc               8330\n",
      "inproceedings      8035\n",
      "InProceedings       808\n",
      "software            314\n",
      "online              233\n",
      "techreport          137\n",
      "INPROCEEDINGS        68\n",
      "Article              62\n",
      "ARTICLE              52\n",
      "phdthesis            41\n",
      "unpublished          30\n",
      "Misc                 24\n",
      "model                24\n",
      "incollection         23\n",
      "MISC                 18\n",
      "ModelCard            16\n",
      "dataset              11\n",
      "mastersthesis         6\n",
      "PhDThesis             6\n",
      "book                  4\n",
      "aInProceedings        4\n",
      "artical               4\n",
      "thesis                4\n",
      "proceedings           3\n",
      "Paper                 2\n",
      "ONLINE                2\n",
      "citation              2\n",
      "Inproceedings         2\n",
      "conference            2\n",
      "preprint              2\n",
      "mics                  1\n",
      "miscellaneous         1\n",
      "journal               1\n",
      "sd_prompts            1\n",
      "articles              1\n",
      "contact               1\n",
      "inbook                1\n",
      "Name: count, dtype: int64\n",
      "Cleaned DataFrame Preview:\n",
      "  bibtex_type                                       bibtex_title  \\\n",
      "0     article                                               None   \n",
      "1     article  Unsupervised Cross-lingual Representation Lear...   \n",
      "2        misc  Robust Speech Recognition via Large-Scale Weak...   \n",
      "3        misc                                               None   \n",
      "4     article                                               None   \n",
      "\n",
      "   is_default_link  \n",
      "0            False  \n",
      "1            False  \n",
      "2            False  \n",
      "3            False  \n",
      "4            False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_805150/1318611357.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_bibtex_df['is_default_link'] = filtered_bibtex_df['extracted_bibtex'].str.contains(default_link_pattern, na=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Step 1: Check if entries start with valid BibTeX types\n",
    "def get_bibtex_type(entry):\n",
    "    match = re.match(r\"^@(\\w+){\", entry.strip())\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Apply function to the extracted_bibtex column\n",
    "filtered_bibtex_df['bibtex_type'] = filtered_bibtex_df['extracted_bibtex'].apply(lambda x: get_bibtex_type(x) if isinstance(x, str) else None)\n",
    "\n",
    "# Count occurrences of each BibTeX type\n",
    "bibtex_counts = filtered_bibtex_df['bibtex_type'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"BibTeX Type Counts:\")\n",
    "print(bibtex_counts)\n",
    "\n",
    "# Step 2: Extract titles from BibTeX\n",
    "def extract_bibtex_title(entry):\n",
    "    match = re.search(r\"title\\s*=\\s*[{\\\"]([^{}\\\"]+)[}\\\"]\", entry, re.IGNORECASE)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "filtered_bibtex_df['bibtex_title'] = filtered_bibtex_df['extracted_bibtex'].apply(lambda x: extract_bibtex_title(x) if isinstance(x, str) else None)\n",
    "\n",
    "# Step 3: Remove default links and duplicates\n",
    "default_link_pattern = r\"(arxiv:1910\\.09700|https://arxiv\\.org/abs/1910\\.09700)\"\n",
    "filtered_bibtex_df['is_default_link'] = filtered_bibtex_df['extracted_bibtex'].str.contains(default_link_pattern, na=False)\n",
    "\n",
    "# Filter non-default links\n",
    "df_cleaned = filtered_bibtex_df[~filtered_bibtex_df['is_default_link']]\n",
    "\n",
    "# Show a preview of cleaned DataFrame\n",
    "print(\"Cleaned DataFrame Preview:\")\n",
    "print(df_cleaned[['bibtex_type', 'bibtex_title', 'is_default_link']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parse with several known domains and pre-given rules\n",
    "- parse from multiple possible bibtex in an order as arxiv >= several academic domains >= other with pdf > unknown, so that we know which one of the bibtex is the paper bibtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 104/34704 [00:01<04:15, 135.24it/s]Entry type software not standard. Not considered.\n",
      "  1%|          | 260/34704 [00:02<04:28, 128.23it/s]Entry type software not standard. Not considered.\n",
      "  1%|          | 429/34704 [00:03<04:20, 131.45it/s]Entry type software not standard. Not considered.\n",
      "  1%|â–         | 491/34704 [00:04<05:04, 112.25it/s]Entry type software not standard. Not considered.\n",
      "  2%|â–         | 527/34704 [00:04<05:35, 101.95it/s]Entry type online not standard. Not considered.\n",
      "  2%|â–         | 779/34704 [00:06<04:34, 123.73it/s]Entry type online not standard. Not considered.\n",
      "  2%|â–         | 826/34704 [00:06<04:00, 141.14it/s]Entry type online not standard. Not considered.\n",
      "  3%|â–Ž         | 967/34704 [00:07<03:42, 151.93it/s]Entry type software not standard. Not considered.\n",
      "  3%|â–Ž         | 1051/34704 [00:08<03:35, 155.91it/s]Entry type online not standard. Not considered.\n",
      "  3%|â–Ž         | 1083/34704 [00:08<03:34, 156.44it/s]Entry type software not standard. Not considered.\n",
      "  4%|â–         | 1353/34704 [00:10<03:36, 154.05it/s]Entry type software not standard. Not considered.\n",
      "  4%|â–         | 1401/34704 [00:10<03:54, 142.18it/s]Entry type software not standard. Not considered.\n",
      "  4%|â–         | 1432/34704 [00:11<04:02, 137.26it/s]Entry type software not standard. Not considered.\n",
      "  4%|â–         | 1449/34704 [00:11<03:49, 145.17it/s]Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "  5%|â–         | 1601/34704 [00:12<04:11, 131.64it/s]Entry type software not standard. Not considered.\n",
      "  5%|â–         | 1641/34704 [00:12<04:25, 124.59it/s]Entry type dataset not standard. Not considered.\n",
      "  5%|â–Œ         | 1803/34704 [00:13<03:29, 157.05it/s]Entry type software not standard. Not considered.\n",
      "  5%|â–Œ         | 1836/34704 [00:13<03:27, 158.74it/s]Entry type software not standard. Not considered.\n",
      "  6%|â–Œ         | 1981/34704 [00:14<03:49, 142.49it/s]Entry type software not standard. Not considered.\n",
      "  6%|â–Œ         | 1999/34704 [00:14<03:36, 150.88it/s]Entry type online not standard. Not considered.\n",
      "  6%|â–Œ         | 2048/34704 [00:15<03:34, 152.51it/s]Entry type online not standard. Not considered.\n",
      "  6%|â–Œ         | 2081/34704 [00:15<03:35, 151.29it/s]Entry type software not standard. Not considered.\n",
      "  6%|â–Œ         | 2146/34704 [00:15<03:36, 150.30it/s]Entry type online not standard. Not considered.\n",
      "  7%|â–‹         | 2278/34704 [00:16<03:30, 153.91it/s]Entry type model not standard. Not considered.\n",
      "  7%|â–‹         | 2374/34704 [00:17<04:14, 127.28it/s]Entry type online not standard. Not considered.\n",
      "  7%|â–‹         | 2390/34704 [00:17<03:58, 135.24it/s]Entry type software not standard. Not considered.\n",
      "  7%|â–‹         | 2505/34704 [00:18<03:23, 158.01it/s]Entry type ainproceedings not standard. Not considered.\n",
      "  7%|â–‹         | 2569/34704 [00:18<03:45, 142.68it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "  7%|â–‹         | 2601/34704 [00:18<03:45, 142.37it/s]Entry type software not standard. Not considered.\n",
      "  8%|â–Š         | 2632/34704 [00:19<03:49, 139.52it/s]Entry type software not standard. Not considered.\n",
      "Entry type model not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "  8%|â–Š         | 2710/34704 [00:19<03:58, 134.19it/s]Entry type online not standard. Not considered.\n",
      "  8%|â–Š         | 2787/34704 [00:20<04:02, 131.68it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "  8%|â–Š         | 2818/34704 [00:20<03:49, 139.07it/s]Entry type software not standard. Not considered.\n",
      "  8%|â–Š         | 2847/34704 [00:20<03:53, 136.29it/s]Entry type software not standard. Not considered.\n",
      "  8%|â–Š         | 2924/34704 [00:21<03:34, 148.11it/s]Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "  8%|â–Š         | 2939/34704 [00:21<03:45, 140.57it/s]Entry type software not standard. Not considered.\n",
      "  9%|â–Š         | 3021/34704 [00:21<03:37, 145.55it/s]Entry type software not standard. Not considered.\n",
      "  9%|â–‰         | 3037/34704 [00:22<03:31, 149.48it/s]Entry type software not standard. Not considered.\n",
      "  9%|â–‰         | 3054/34704 [00:22<03:31, 149.66it/s]Entry type online not standard. Not considered.\n",
      "  9%|â–‰         | 3070/34704 [00:22<03:41, 143.07it/s]Entry type software not standard. Not considered.\n",
      "  9%|â–‰         | 3085/34704 [00:22<03:42, 141.79it/s]Entry type online not standard. Not considered.\n",
      "  9%|â–‰         | 3100/34704 [00:22<03:50, 136.82it/s]Entry type paper not standard. Not considered.\n",
      "  9%|â–‰         | 3118/34704 [00:22<03:35, 146.71it/s]Entry type software not standard. Not considered.\n",
      "  9%|â–‰         | 3182/34704 [00:23<03:49, 137.12it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "  9%|â–‰         | 3275/34704 [00:23<03:51, 135.85it/s]Entry type online not standard. Not considered.\n",
      " 10%|â–‰         | 3361/34704 [00:24<04:07, 126.55it/s]Entry type online not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 10%|â–ˆ         | 3475/34704 [00:25<03:18, 157.30it/s]Entry type software not standard. Not considered.\n",
      " 10%|â–ˆ         | 3525/34704 [00:25<03:17, 158.00it/s]Entry type model not standard. Not considered.\n",
      " 10%|â–ˆ         | 3560/34704 [00:25<03:16, 158.64it/s]Entry type online not standard. Not considered.\n",
      " 11%|â–ˆ         | 3644/34704 [00:26<03:09, 163.49it/s]Entry type mics not standard. Not considered.\n",
      " 11%|â–ˆ         | 3678/34704 [00:26<03:07, 165.37it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 11%|â–ˆ         | 3729/34704 [00:26<03:06, 165.75it/s]Entry type software not standard. Not considered.\n",
      " 11%|â–ˆ         | 3746/34704 [00:26<03:13, 160.18it/s]Entry type software not standard. Not considered.\n",
      " 11%|â–ˆ         | 3797/34704 [00:27<03:08, 163.82it/s]Entry type software not standard. Not considered.\n",
      " 11%|â–ˆâ–        | 3941/34704 [00:28<04:41, 109.28it/s]Entry type software not standard. Not considered.\n",
      " 11%|â–ˆâ–        | 3963/34704 [00:28<03:46, 135.45it/s]Entry type software not standard. Not considered.\n",
      "Entry type dataset not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 12%|â–ˆâ–        | 4054/34704 [00:29<05:47, 88.12it/s] Entry type software not standard. Not considered.\n",
      " 12%|â–ˆâ–        | 4099/34704 [00:30<06:53, 74.10it/s]Entry type software not standard. Not considered.\n",
      " 12%|â–ˆâ–        | 4174/34704 [00:30<03:09, 161.07it/s]Entry type software not standard. Not considered.\n",
      " 12%|â–ˆâ–        | 4259/34704 [00:31<05:50, 86.97it/s] Entry type journal not standard. Not considered.\n",
      " 12%|â–ˆâ–        | 4287/34704 [00:32<06:24, 79.07it/s]Entry type software not standard. Not considered.\n",
      " 12%|â–ˆâ–        | 4296/34704 [00:32<06:34, 77.03it/s]Entry type software not standard. Not considered.\n",
      " 12%|â–ˆâ–        | 4326/34704 [00:32<07:35, 66.65it/s]Entry type software not standard. Not considered.\n",
      " 13%|â–ˆâ–Ž        | 4382/34704 [00:33<07:37, 66.32it/s]Entry type dataset not standard. Not considered.\n",
      " 13%|â–ˆâ–Ž        | 4395/34704 [00:33<06:05, 82.95it/s]Entry type software not standard. Not considered.\n",
      " 13%|â–ˆâ–Ž        | 4431/34704 [00:34<07:27, 67.67it/s]Entry type online not standard. Not considered.\n",
      " 13%|â–ˆâ–Ž        | 4465/34704 [00:34<05:38, 89.20it/s]Entry type software not standard. Not considered.\n",
      " 14%|â–ˆâ–        | 4804/34704 [00:37<06:18, 78.99it/s] Entry type software not standard. Not considered.\n",
      " 14%|â–ˆâ–        | 4840/34704 [00:38<04:50, 102.66it/s]Entry type software not standard. Not considered.\n",
      " 15%|â–ˆâ–        | 5033/34704 [00:40<06:14, 79.30it/s] Entry type online not standard. Not considered.\n",
      " 15%|â–ˆâ–        | 5050/34704 [00:40<06:59, 70.61it/s]Entry type online not standard. Not considered.\n",
      " 15%|â–ˆâ–        | 5158/34704 [00:41<05:19, 92.54it/s] Entry type software not standard. Not considered.\n",
      " 15%|â–ˆâ–Œ        | 5267/34704 [00:43<06:55, 70.78it/s]Entry type online not standard. Not considered.\n",
      " 16%|â–ˆâ–Œ        | 5533/34704 [00:46<04:22, 111.05it/s]Entry type software not standard. Not considered.\n",
      " 16%|â–ˆâ–Œ        | 5581/34704 [00:46<02:52, 168.40it/s]Entry type software not standard. Not considered.\n",
      " 16%|â–ˆâ–Œ        | 5604/34704 [00:47<02:37, 185.05it/s]Entry type online not standard. Not considered.\n",
      " 16%|â–ˆâ–‹        | 5653/34704 [00:47<02:17, 210.78it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 16%|â–ˆâ–‹        | 5678/34704 [00:47<02:11, 221.02it/s]Entry type thesis not standard. Not considered.\n",
      " 17%|â–ˆâ–‹        | 5840/34704 [00:48<02:44, 175.32it/s]Entry type software not standard. Not considered.\n",
      " 17%|â–ˆâ–‹        | 5927/34704 [00:48<02:45, 173.72it/s]Entry type software not standard. Not considered.\n",
      " 17%|â–ˆâ–‹        | 5947/34704 [00:48<02:45, 174.04it/s]Entry type online not standard. Not considered.\n",
      " 18%|â–ˆâ–Š        | 6108/34704 [00:50<04:43, 100.69it/s]Entry type software not standard. Not considered.\n",
      " 18%|â–ˆâ–Š        | 6130/34704 [00:50<03:49, 124.72it/s]Entry type online not standard. Not considered.\n",
      " 18%|â–ˆâ–Š        | 6224/34704 [00:50<02:23, 198.36it/s]Entry type online not standard. Not considered.\n",
      " 18%|â–ˆâ–Š        | 6246/34704 [00:50<02:22, 200.27it/s]Entry type software not standard. Not considered.\n",
      " 18%|â–ˆâ–Š        | 6267/34704 [00:51<03:37, 130.48it/s]Entry type software not standard. Not considered.\n",
      " 18%|â–ˆâ–Š        | 6335/34704 [00:52<04:59, 94.86it/s] Entry type online not standard. Not considered.\n",
      " 18%|â–ˆâ–Š        | 6369/34704 [00:52<05:01, 93.98it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 20%|â–ˆâ–‰        | 6799/34704 [01:13<18:52, 24.65it/s]  Entry type software not standard. Not considered.\n",
      " 20%|â–ˆâ–‰        | 6821/34704 [01:14<13:08, 35.36it/s]Entry type online not standard. Not considered.\n",
      " 20%|â–ˆâ–‰        | 6901/34704 [01:14<06:07, 75.72it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 20%|â–ˆâ–ˆ        | 7113/34704 [01:16<04:38, 99.00it/s] Entry type online not standard. Not considered.\n",
      " 21%|â–ˆâ–ˆ        | 7241/34704 [01:17<02:51, 159.91it/s]Entry type software not standard. Not considered.\n",
      " 21%|â–ˆâ–ˆ        | 7330/34704 [01:17<02:55, 155.56it/s]Entry type online not standard. Not considered.\n",
      " 21%|â–ˆâ–ˆâ–       | 7444/34704 [01:18<02:14, 202.61it/s]Entry type online not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 22%|â–ˆâ–ˆâ–       | 7487/34704 [01:18<02:12, 205.43it/s]Entry type software not standard. Not considered.\n",
      " 22%|â–ˆâ–ˆâ–       | 7511/34704 [01:18<02:08, 211.18it/s]Entry type online not standard. Not considered.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 7891/34704 [01:21<02:48, 158.94it/s]Entry type software not standard. Not considered.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 7914/34704 [01:21<02:33, 174.70it/s]Entry type software not standard. Not considered.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 7934/34704 [01:21<03:06, 143.42it/s]Entry type ainproceedings not standard. Not considered.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 7951/34704 [01:21<03:38, 122.19it/s]Entry type model not standard. Not considered.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 8022/34704 [01:22<05:18, 83.88it/s] Entry type software not standard. Not considered.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 8045/34704 [01:22<05:24, 82.25it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 8121/34704 [01:23<04:44, 93.30it/s]Entry type software not standard. Not considered.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 8131/34704 [01:23<05:00, 88.39it/s]Entry type software not standard. Not considered.\n",
      " 24%|â–ˆâ–ˆâ–Ž       | 8208/34704 [01:24<05:40, 77.85it/s]Entry type software not standard. Not considered.\n",
      " 24%|â–ˆâ–ˆâ–Ž       | 8224/34704 [01:25<05:55, 74.40it/s]Entry type ainproceedings not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 24%|â–ˆâ–ˆâ–       | 8282/34704 [01:25<06:25, 68.53it/s]Entry type software not standard. Not considered.\n",
      " 24%|â–ˆâ–ˆâ–       | 8331/34704 [01:26<05:41, 77.33it/s]Entry type online not standard. Not considered.\n",
      " 24%|â–ˆâ–ˆâ–       | 8339/34704 [01:26<06:14, 70.36it/s]Entry type software not standard. Not considered.\n",
      " 24%|â–ˆâ–ˆâ–       | 8354/34704 [01:26<06:24, 68.60it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 24%|â–ˆâ–ˆâ–       | 8368/34704 [01:27<05:06, 85.97it/s]Entry type model not standard. Not considered.\n",
      " 24%|â–ˆâ–ˆâ–       | 8390/34704 [01:27<04:52, 90.00it/s]Entry type software not standard. Not considered.\n",
      " 24%|â–ˆâ–ˆâ–       | 8458/34704 [01:27<04:16, 102.14it/s]Entry type software not standard. Not considered.\n",
      " 24%|â–ˆâ–ˆâ–       | 8469/34704 [01:28<04:22, 100.01it/s]Entry type software not standard. Not considered.\n",
      " 25%|â–ˆâ–ˆâ–       | 8508/34704 [01:28<03:52, 112.46it/s]Entry type online not standard. Not considered.\n",
      " 25%|â–ˆâ–ˆâ–       | 8609/34704 [01:29<04:00, 108.58it/s]Entry type citation not standard. Not considered.\n",
      " 25%|â–ˆâ–ˆâ–       | 8649/34704 [01:29<03:33, 121.78it/s]Entry type software not standard. Not considered.\n",
      " 25%|â–ˆâ–ˆâ–       | 8675/34704 [01:29<03:42, 116.84it/s]Entry type software not standard. Not considered.\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 8730/34704 [01:30<03:26, 125.99it/s]Entry type software not standard. Not considered.\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 8758/34704 [01:30<03:25, 126.48it/s]Entry type software not standard. Not considered.\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 8787/34704 [01:30<03:16, 131.73it/s]Entry type online not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 8944/34704 [01:31<02:50, 150.75it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 8991/34704 [01:32<03:27, 123.85it/s]Entry type software not standard. Not considered.\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 9033/34704 [01:32<03:28, 123.35it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 26%|â–ˆâ–ˆâ–‹       | 9127/34704 [01:33<02:48, 151.46it/s]Entry type software not standard. Not considered.\n",
      " 26%|â–ˆâ–ˆâ–‹       | 9175/34704 [01:33<03:11, 132.99it/s]Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 9248/34704 [01:33<02:54, 145.82it/s]Entry type online not standard. Not considered.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 9269/34704 [01:34<02:37, 161.97it/s]Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 9313/34704 [01:34<02:14, 188.60it/s]Entry type software not standard. Not considered.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 9333/34704 [01:34<02:28, 170.53it/s]Entry type online not standard. Not considered.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 9436/34704 [01:35<02:40, 157.53it/s]Entry type software not standard. Not considered.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 9553/34704 [01:35<03:05, 135.60it/s]Entry type software not standard. Not considered.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 9637/34704 [01:36<03:45, 111.06it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 9656/34704 [01:36<03:12, 130.34it/s]Entry type software not standard. Not considered.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 9690/34704 [01:36<02:50, 146.62it/s]Entry type software not standard. Not considered.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 9739/34704 [01:37<02:48, 148.03it/s]Entry type software not standard. Not considered.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 9835/34704 [01:38<02:58, 139.03it/s]Entry type software not standard. Not considered.\n",
      " 29%|â–ˆâ–ˆâ–Š       | 9898/34704 [01:38<02:16, 181.13it/s]Entry type model not standard. Not considered.\n",
      "Entry type dataset not standard. Not considered.\n",
      " 29%|â–ˆâ–ˆâ–Š       | 9944/34704 [01:38<02:02, 202.77it/s]Entry type software not standard. Not considered.\n",
      " 29%|â–ˆâ–ˆâ–Š       | 9967/34704 [01:38<01:58, 208.80it/s]Entry type online not standard. Not considered.\n",
      " 29%|â–ˆâ–ˆâ–‰       | 10099/34704 [01:39<02:59, 137.18it/s]Entry type software not standard. Not considered.\n",
      " 29%|â–ˆâ–ˆâ–‰       | 10222/34704 [01:40<03:07, 130.30it/s]Entry type online not standard. Not considered.\n",
      " 30%|â–ˆâ–ˆâ–‰       | 10238/34704 [01:40<03:04, 132.73it/s]Entry type online not standard. Not considered.\n",
      " 30%|â–ˆâ–ˆâ–‰       | 10265/34704 [01:40<03:29, 116.80it/s]Entry type software not standard. Not considered.\n",
      " 30%|â–ˆâ–ˆâ–‰       | 10323/34704 [01:41<03:24, 119.44it/s]Entry type thesis not standard. Not considered.\n",
      " 30%|â–ˆâ–ˆâ–‰       | 10363/34704 [01:41<03:28, 116.58it/s]Entry type software not standard. Not considered.\n",
      " 30%|â–ˆâ–ˆâ–‰       | 10404/34704 [01:42<03:22, 119.90it/s]Entry type online not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 10474/34704 [01:42<03:26, 117.29it/s]Entry type online not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 10504/34704 [01:42<03:08, 128.72it/s]Entry type software not standard. Not considered.\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 10710/34704 [01:44<02:44, 145.70it/s]Entry type software not standard. Not considered.\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 10725/34704 [01:44<02:58, 134.06it/s]Entry type software not standard. Not considered.\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 10739/34704 [01:44<02:57, 135.13it/s]Entry type software not standard. Not considered.\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 10817/34704 [01:45<03:45, 105.87it/s]Entry type software not standard. Not considered.\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 10829/34704 [01:45<03:41, 107.72it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 10855/34704 [01:45<03:30, 113.40it/s]Entry type software not standard. Not considered.\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 10897/34704 [01:45<03:09, 125.92it/s]Entry type artical not standard. Not considered.\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 10910/34704 [01:46<03:08, 126.22it/s]Entry type online not standard. Not considered.\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 10936/34704 [01:46<03:19, 119.42it/s]Entry type software not standard. Not considered.\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 11023/34704 [01:46<03:02, 129.90it/s]Entry type online not standard. Not considered.\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 11105/34704 [01:47<03:44, 105.34it/s]Entry type software not standard. Not considered.\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 11169/34704 [01:48<03:10, 123.70it/s]Entry type software not standard. Not considered.\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 11277/34704 [01:48<02:51, 136.75it/s]Entry type software not standard. Not considered.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 11291/34704 [01:49<03:01, 129.30it/s]Entry type software not standard. Not considered.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 11349/34704 [01:49<03:02, 128.03it/s]Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 11467/34704 [01:50<03:01, 128.00it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 11480/34704 [01:50<03:07, 123.78it/s]Entry type software not standard. Not considered.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 11592/34704 [01:51<03:28, 110.88it/s]Entry type software not standard. Not considered.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 11693/34704 [01:52<03:00, 127.35it/s]Entry type dataset not standard. Not considered.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 11741/34704 [01:52<03:54, 98.11it/s] Entry type software not standard. Not considered.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 11769/34704 [01:53<03:15, 117.23it/s]Entry type software not standard. Not considered.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 11794/34704 [01:53<03:31, 108.43it/s]Entry type online not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 11806/34704 [01:53<03:36, 105.60it/s]Entry type model not standard. Not considered.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 11829/34704 [01:53<02:47, 136.89it/s]Entry type software not standard. Not considered.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 11853/34704 [01:53<02:19, 164.34it/s]Entry type software not standard. Not considered.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 11923/34704 [01:54<01:51, 205.06it/s]Entry type online not standard. Not considered.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 11947/34704 [01:54<01:45, 215.10it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 12007/34704 [01:54<02:24, 157.51it/s]Entry type artical not standard. Not considered.\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 12097/34704 [01:55<03:03, 123.48it/s]Entry type online not standard. Not considered.\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 12126/34704 [01:55<02:58, 126.16it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 12139/34704 [01:55<03:38, 103.06it/s]Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 12341/34704 [01:57<02:52, 129.32it/s]Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 12368/34704 [01:57<03:05, 120.33it/s]Entry type online not standard. Not considered.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 12381/34704 [01:57<03:10, 117.42it/s]Entry type software not standard. Not considered.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 12409/34704 [01:57<03:05, 120.29it/s]Entry type software not standard. Not considered.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 12490/34704 [01:58<02:53, 127.98it/s]Entry type software not standard. Not considered.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 12529/34704 [01:59<03:08, 117.43it/s]Entry type software not standard. Not considered.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 12544/34704 [01:59<03:00, 122.86it/s]Entry type software not standard. Not considered.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 12647/34704 [02:00<03:20, 110.14it/s]Entry type software not standard. Not considered.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 12760/34704 [02:01<03:05, 118.13it/s]Entry type paper not standard. Not considered.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 12828/34704 [02:01<03:01, 120.64it/s]Entry type software not standard. Not considered.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 12843/34704 [02:01<02:53, 125.99it/s]Entry type thesis not standard. Not considered.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 13010/34704 [02:03<03:07, 115.90it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13022/34704 [02:03<03:11, 113.17it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13045/34704 [02:03<03:22, 107.13it/s]Entry type software not standard. Not considered.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13058/34704 [02:03<03:20, 107.97it/s]Entry type software not standard. Not considered.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13091/34704 [02:19<1:44:51,  3.44it/s]Entry type online not standard. Not considered.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13200/34704 [02:20<09:09, 39.15it/s]  Entry type software not standard. Not considered.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13285/34704 [02:21<03:36, 98.89it/s]Entry type online not standard. Not considered.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13318/34704 [02:21<02:52, 124.19it/s]Entry type software not standard. Not considered.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13334/34704 [02:21<02:41, 132.73it/s]Entry type software not standard. Not considered.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13350/34704 [02:21<02:33, 138.71it/s]Entry type software not standard. Not considered.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 13425/34704 [02:22<02:57, 119.89it/s]Entry type model not standard. Not considered.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 13602/34704 [02:23<02:45, 127.15it/s]Entry type software not standard. Not considered.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 13629/34704 [02:24<02:51, 122.89it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 13642/34704 [02:24<02:53, 121.74it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 13655/34704 [02:24<02:56, 119.44it/s]Entry type software not standard. Not considered.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 13668/34704 [02:24<02:52, 122.12it/s]Entry type software not standard. Not considered.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 13748/34704 [02:24<02:50, 122.59it/s]Entry type software not standard. Not considered.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 13810/34704 [02:25<03:08, 110.80it/s]Entry type software not standard. Not considered.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13970/34704 [02:26<02:54, 118.65it/s]Entry type software not standard. Not considered.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13984/34704 [02:26<02:48, 123.18it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13999/34704 [02:27<02:45, 125.17it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14016/34704 [02:27<02:31, 136.47it/s]Entry type software not standard. Not considered.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14051/34704 [02:27<02:28, 139.27it/s]Entry type software not standard. Not considered.\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14093/34704 [02:27<02:56, 116.59it/s]Entry type software not standard. Not considered.\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14266/34704 [02:29<02:42, 125.53it/s]Entry type online not standard. Not considered.\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14340/34704 [02:29<02:45, 122.84it/s]Entry type software not standard. Not considered.\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14407/34704 [02:30<03:06, 108.83it/s]Entry type online not standard. Not considered.\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14420/34704 [02:30<02:57, 114.15it/s]Entry type software not standard. Not considered.\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14463/34704 [02:30<02:34, 131.39it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14491/34704 [02:30<02:44, 122.73it/s]Entry type online not standard. Not considered.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 14779/34704 [02:33<02:26, 136.19it/s]Entry type software not standard. Not considered.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 14920/34704 [02:34<02:39, 123.86it/s]Entry type software not standard. Not considered.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 14937/34704 [02:34<02:25, 136.05it/s]Entry type software not standard. Not considered.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 14951/34704 [02:34<02:44, 119.94it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 14964/34704 [02:34<02:48, 117.09it/s]Entry type online not standard. Not considered.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15039/34704 [02:35<03:04, 106.58it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15058/34704 [02:35<02:37, 124.92it/s]Entry type software not standard. Not considered.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15071/34704 [02:35<02:59, 109.43it/s]Entry type online not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15131/34704 [02:36<02:36, 124.79it/s]Entry type software not standard. Not considered.\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15145/34704 [02:36<02:34, 126.53it/s]Entry type online not standard. Not considered.\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15171/34704 [02:36<02:46, 116.98it/s]Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15220/34704 [02:36<02:54, 111.48it/s]Entry type software not standard. Not considered.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15550/34704 [02:39<02:26, 130.54it/s]Entry type software not standard. Not considered.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15564/34704 [02:39<02:32, 125.38it/s]Entry type software not standard. Not considered.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15577/34704 [02:39<02:35, 122.82it/s]Entry type software not standard. Not considered.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15622/34704 [02:40<02:23, 132.84it/s]Entry type software not standard. Not considered.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15636/34704 [02:40<02:21, 134.39it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15650/34704 [02:40<02:28, 128.66it/s]Entry type software not standard. Not considered.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15664/34704 [02:40<02:25, 130.88it/s]Entry type software not standard. Not considered.\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15794/34704 [02:41<02:08, 147.73it/s]Entry type online not standard. Not considered.\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 16084/34704 [02:43<02:12, 141.04it/s]Entry type online not standard. Not considered.\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 16100/34704 [02:43<02:08, 144.68it/s]Entry type software not standard. Not considered.\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 16136/34704 [02:44<02:07, 145.88it/s]Entry type software not standard. Not considered.\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 16181/34704 [02:44<02:41, 114.79it/s]Entry type software not standard. Not considered.\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 16282/34704 [02:45<02:19, 132.22it/s]Entry type software not standard. Not considered.\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 16435/34704 [02:46<02:07, 142.90it/s]Entry type software not standard. Not considered.\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 17090/34704 [02:51<02:39, 110.75it/s]Entry type software not standard. Not considered.\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 17160/34704 [02:52<02:16, 128.55it/s]Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 17199/34704 [02:52<02:33, 113.69it/s]Entry type software not standard. Not considered.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 17314/34704 [02:53<02:12, 131.02it/s]Entry type online not standard. Not considered.\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 17531/34704 [02:55<02:15, 126.53it/s]Entry type model not standard. Not considered.\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 17778/34704 [02:57<02:23, 117.90it/s]Entry type software not standard. Not considered.\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17975/34704 [02:58<02:02, 136.86it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17996/34704 [02:58<01:47, 155.04it/s]Entry type software not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18116/34704 [02:59<01:13, 224.35it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18183/34704 [02:59<01:24, 194.76it/s]Entry type software not standard. Not considered.\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18203/34704 [02:59<01:39, 166.33it/s]Entry type software not standard. Not considered.\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18741/34704 [03:03<02:03, 129.10it/s]Entry type software not standard. Not considered.\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18769/34704 [03:04<02:00, 132.63it/s]Entry type software not standard. Not considered.\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18849/34704 [03:04<02:25, 108.64it/s]Entry type ainproceedings not standard. Not considered.\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18927/34704 [03:05<02:14, 117.05it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18942/34704 [03:05<02:05, 126.08it/s]Entry type software not standard. Not considered.\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19062/34704 [03:06<02:04, 125.79it/s]Entry type software not standard. Not considered.\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 19304/34704 [03:08<02:11, 117.40it/s]Entry type software not standard. Not considered.\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 19328/34704 [03:09<02:31, 101.34it/s]Entry type online not standard. Not considered.\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 19485/34704 [03:10<01:55, 131.58it/s]Entry type software not standard. Not considered.\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 19822/34704 [03:27<01:58, 125.56it/s] Entry type software not standard. Not considered.\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 19838/34704 [03:27<01:51, 133.11it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 20011/34704 [03:29<01:54, 128.51it/s]Entry type software not standard. Not considered.\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 20146/34704 [03:30<01:59, 122.16it/s]Entry type model not standard. Not considered.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 20698/34704 [03:34<01:45, 133.30it/s]Entry type software not standard. Not considered.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 20928/34704 [03:35<01:33, 147.97it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 20973/34704 [03:36<01:34, 145.78it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21019/34704 [03:36<01:34, 144.21it/s]Entry type online not standard. Not considered.\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21128/34704 [03:37<01:31, 149.13it/s]Entry type online not standard. Not considered.\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21219/34704 [03:37<01:40, 134.26it/s]Entry type model not standard. Not considered.\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 21813/34704 [03:42<01:31, 141.46it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 21905/34704 [03:42<01:28, 145.34it/s]Entry type online not standard. Not considered.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22147/34704 [03:44<01:25, 147.13it/s]Entry type software not standard. Not considered.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22177/34704 [03:44<01:26, 144.23it/s]Entry type citation not standard. Not considered.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22207/34704 [03:44<01:32, 135.66it/s]Entry type software not standard. Not considered.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22252/34704 [03:45<01:52, 110.96it/s]Entry type software not standard. Not considered.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22339/34704 [03:45<01:32, 133.94it/s]Entry type online not standard. Not considered.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22372/34704 [03:46<01:23, 147.34it/s]Entry type software not standard. Not considered.\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 22575/34704 [03:47<01:30, 133.29it/s]Entry type online not standard. Not considered.\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 23205/34704 [03:52<01:33, 123.07it/s]Entry type software not standard. Not considered.\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 23617/34704 [03:55<01:21, 135.34it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 23801/34704 [03:56<01:11, 151.60it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 23817/34704 [03:57<01:11, 151.61it/s]Entry type software not standard. Not considered.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 23896/34704 [03:57<01:23, 130.13it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 23975/34704 [03:58<01:13, 145.81it/s]Entry type software not standard. Not considered.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 23990/34704 [03:58<01:14, 143.62it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 24019/34704 [03:58<01:23, 127.87it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 24070/34704 [03:59<01:36, 110.56it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 24318/34704 [04:00<01:10, 147.75it/s]Entry type software not standard. Not considered.\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25018/34704 [04:06<01:06, 145.76it/s]Entry type software not standard. Not considered.\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 25250/34704 [04:07<01:20, 117.43it/s]Entry type online not standard. Not considered.\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 25529/34704 [04:10<01:12, 126.58it/s]Entry type software not standard. Not considered.\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 25545/34704 [04:10<01:08, 134.01it/s]Entry type software not standard. Not considered.\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 25589/34704 [04:10<01:15, 120.52it/s]Entry type software not standard. Not considered.\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 26032/34704 [04:23<01:00, 143.36it/s]Entry type model not standard. Not considered.\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 26210/34704 [04:25<01:05, 129.89it/s]Entry type software not standard. Not considered.\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 26334/34704 [04:25<00:59, 141.86it/s]Entry type model not standard. Not considered.\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 26939/34704 [04:30<01:02, 124.63it/s]Entry type software not standard. Not considered.\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 26986/34704 [04:30<00:55, 139.92it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 27318/34704 [04:33<01:02, 118.63it/s]Entry type online not standard. Not considered.\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28071/34704 [04:38<00:46, 142.64it/s]Entry type software not standard. Not considered.\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 28279/34704 [04:40<00:48, 133.00it/s]Entry type software not standard. Not considered.\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 28862/34704 [04:44<00:42, 137.31it/s]Entry type dataset not standard. Not considered.\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29021/34704 [04:45<00:37, 152.73it/s]Entry type artical not standard. Not considered.\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29054/34704 [04:46<00:36, 156.49it/s]Entry type software not standard. Not considered.\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 29628/34704 [04:50<00:37, 133.95it/s]Entry type model not standard. Not considered.\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 29954/34704 [04:52<00:35, 132.13it/s]Entry type software not standard. Not considered.\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 30100/34704 [04:53<00:32, 140.86it/s]Entry type software not standard. Not considered.\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 30116/34704 [04:53<00:33, 138.87it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 30512/34704 [04:56<00:37, 110.83it/s]Entry type software not standard. Not considered.\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 30527/34704 [04:56<00:34, 121.03it/s]Entry type software not standard. Not considered.\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 30571/34704 [04:57<00:31, 131.35it/s]Entry type software not standard. Not considered.\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 30585/34704 [04:57<00:32, 125.49it/s]Entry type software not standard. Not considered.\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 30631/34704 [04:57<00:29, 140.37it/s]Entry type thesis not standard. Not considered.\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 30663/34704 [04:57<00:30, 134.44it/s]Entry type preprint not standard. Not considered.\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 30929/34704 [04:59<00:26, 141.60it/s]Entry type software not standard. Not considered.\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 30991/34704 [05:00<00:25, 146.51it/s]Entry type preprint not standard. Not considered.\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 31140/34704 [05:01<00:24, 145.01it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 31171/34704 [05:01<00:23, 148.71it/s]Entry type software not standard. Not considered.\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 31186/34704 [05:01<00:27, 127.68it/s]Entry type online not standard. Not considered.\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31323/34704 [05:02<00:24, 137.70it/s]Entry type online not standard. Not considered.\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31468/34704 [05:03<00:21, 151.03it/s]Entry type software not standard. Not considered.\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31542/34704 [05:04<00:23, 136.10it/s]Entry type dataset not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31558/34704 [05:04<00:22, 142.33it/s]Entry type dataset not standard. Not considered.\n",
      "Entry type dataset not standard. Not considered.\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31574/34704 [05:04<00:21, 146.70it/s]Entry type model not standard. Not considered.\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31589/34704 [05:04<00:23, 131.38it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 31957/34704 [05:07<00:18, 146.20it/s]Entry type artical not standard. Not considered.\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 32181/34704 [05:17<00:32, 78.43it/s] Entry type articles not standard. Not considered.\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 32439/34704 [05:19<00:17, 126.37it/s]Entry type software not standard. Not considered.\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 33368/34704 [05:26<00:09, 143.35it/s]Entry type model not standard. Not considered.\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33429/34704 [05:27<00:09, 140.12it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33444/34704 [05:27<00:09, 131.80it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33458/34704 [05:27<00:09, 126.10it/s]Entry type contact not standard. Not considered.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33527/34704 [05:27<00:08, 131.92it/s]Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33542/34704 [05:28<00:08, 135.60it/s]Entry type dataset not standard. Not considered.\n",
      "Entry type dataset not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33662/34704 [05:28<00:07, 143.44it/s]Entry type software not standard. Not considered.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33707/34704 [05:29<00:07, 141.42it/s]Entry type software not standard. Not considered.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33785/34704 [05:29<00:06, 145.09it/s]Entry type online not standard. Not considered.\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 34008/34704 [05:31<00:05, 132.56it/s]Entry type online not standard. Not considered.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 34263/34704 [05:33<00:03, 130.72it/s]Entry type model not standard. Not considered.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34277/34704 [05:33<00:03, 132.25it/s]Entry type online not standard. Not considered.\n",
      "Entry type online not standard. Not considered.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34393/34704 [05:34<00:02, 146.95it/s]Entry type model not standard. Not considered.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34440/34704 [05:34<00:01, 149.08it/s]Entry type software not standard. Not considered.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34456/34704 [05:35<00:01, 151.51it/s]Entry type online not standard. Not considered.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34472/34704 [05:35<00:01, 151.70it/s]Entry type model not standard. Not considered.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34536/34704 [05:35<00:01, 137.37it/s]Entry type software not standard. Not considered.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34582/34704 [05:36<00:00, 128.96it/s]Entry type software not standard. Not considered.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34704/34704 [05:36<00:00, 103.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Helper functions to extract fields\n",
    "def extract_arxiv_id(bibtex):\n",
    "    if isinstance(bibtex, str):\n",
    "        # Updated regex to also capture generic arXiv ID formats (4 digits + '.' + 5 digits)\n",
    "        match = re.search(r\"(arxiv:\\d+\\.\\d+|https?://(?:www\\.)?arxiv\\.org/(abs|pdf)/\\d+\\.\\d+|\\d{4}\\.\\d{5})\", bibtex, re.IGNORECASE)\n",
    "        if match:\n",
    "            # Normalize the extracted ID by removing prefixes like 'arxiv:' or 'https://...'\n",
    "            return re.sub(r\"(arxiv:|https?://(?:www\\.)?arxiv\\.org/(abs|pdf)/)\", \"\", match.group())\n",
    "    return None\n",
    "\n",
    "def extract_doi(bibtex):\n",
    "    if isinstance(bibtex, str):\n",
    "        match = re.search(r\"(doi|DOI)\\s*=\\s*[{\\\"]([^{}\\\"]+)[}\\\"]\", bibtex)\n",
    "        return match.group(2) if match else None\n",
    "    return None\n",
    "\n",
    "def extract_url(bibtex):\n",
    "    if isinstance(bibtex, str):\n",
    "        match = re.search(r\"(url|Url|URL)\\s*=\\s*[{\\\"]([^{}\\\"]+)[}\\\"]\", bibtex)\n",
    "        return match.group(2) if match else None\n",
    "    return None\n",
    "\n",
    "def classify_domain(url):\n",
    "    if isinstance(url, str):\n",
    "        domain_match = re.search(r\"https?://(?:www\\.)?([^/]+)/\", url)\n",
    "        if domain_match:\n",
    "            domain = domain_match.group(1).lower()\n",
    "            if \"arxiv.org\" in domain:\n",
    "                return \"arxiv\"\n",
    "            elif \"doi.org\" in domain:\n",
    "                return \"doi\"\n",
    "            elif \"openreview.net\" in domain:\n",
    "                return \"openreview\"\n",
    "            elif \"aclanthology.org\" in domain:\n",
    "                return \"acl\"\n",
    "            elif \"sciencedirect.com\" in domain:\n",
    "                return \"sciencedirect\"\n",
    "            elif \"springer.com\" in domain:\n",
    "                return \"springer\"\n",
    "            elif \"wiley.com\" in domain:\n",
    "                return \"wiley\"\n",
    "            elif \"tandfonline.com\" in domain:\n",
    "                return \"tandfonline\"\n",
    "            elif \"cambridge.org\" in domain:\n",
    "                return \"cambridge\"\n",
    "            elif \"nature.com\" in domain:\n",
    "                return \"nature\"\n",
    "        if url.lower().endswith(\".pdf\"):\n",
    "            return \"others\"\n",
    "        return \"unknown\"\n",
    "    return None\n",
    "\n",
    "import bibtexparser\n",
    "from bibtexparser.bparser import BibTexParser\n",
    "from bibtexparser.customization import convert_to_unicode\n",
    "# Parse BibTeX entries using bibtexparser\n",
    "def parse_bibtex_entry(entry):\n",
    "    try:\n",
    "        parser = BibTexParser()\n",
    "        parser.customization = convert_to_unicode\n",
    "        parser.ignore_nonstandard_types = False\n",
    "        bib_database = bibtexparser.loads(entry, parser=parser)\n",
    "        #bib_database = bibtexparser.loads(entry)\n",
    "        parsed_data = []\n",
    "        for item in bib_database.entries:\n",
    "            parsed_data.append({\n",
    "                \"bibtex_title\": item.get(\"title\"),\n",
    "                \"bibtex_author\": item.get(\"author\"),\n",
    "                \"bibtex_journal\": item.get(\"journal\"),\n",
    "                \"bibtex_year\": item.get(\"year\"),\n",
    "                \"bibtex_url\": item.get(\"url\"),\n",
    "                \"bibtex_doi\": item.get(\"doi\"),\n",
    "                \"bibtex_arxiv_id\": extract_arxiv_id(item.get(\"journal\", \"\") + \" \" + item.get(\"eprint\", \"\"))  # Combine journal and eprint fields\n",
    "            })\n",
    "        return parsed_data\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Process BibTeX entries\n",
    "processed_results = []\n",
    "for index, row in tqdm(df_cleaned.iterrows(), total=len(df_cleaned)):\n",
    "    bibtex_entry = row['extracted_bibtex']\n",
    "    if not isinstance(bibtex_entry, str):\n",
    "        processed_results.append({\n",
    "            \"modelId\": row['modelId'],\n",
    "            \"regex_title\": None,\n",
    "            \"regex_author\": None,\n",
    "            \"regex_journal\": None,\n",
    "            \"regex_year\": None,\n",
    "            \"regex_url\": None,\n",
    "            \"regex_doi\": None,\n",
    "            \"regex_arxiv_id\": None,\n",
    "            \"bibtex_parsed\": None\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # BibTeX parsing\n",
    "    bibtex_parsed = parse_bibtex_entry(bibtex_entry)\n",
    "    if bibtex_parsed:\n",
    "        for parsed_item in bibtex_parsed:\n",
    "            processed_results.append({\n",
    "                \"modelId\": row['modelId'],\n",
    "                \"bibtex_title\": parsed_item.get(\"bibtex_title\"),\n",
    "                \"bibtex_author\": parsed_item.get(\"bibtex_author\"),\n",
    "                \"bibtex_journal\": parsed_item.get(\"bibtex_journal\"),\n",
    "                \"bibtex_year\": parsed_item.get(\"bibtex_year\"),\n",
    "                \"bibtex_url\": parsed_item.get(\"bibtex_url\"),\n",
    "                \"bibtex_doi\": parsed_item.get(\"bibtex_doi\"),\n",
    "                \"bibtex_arxiv_id\": parsed_item.get(\"bibtex_arxiv_id\"),\n",
    "\n",
    "                # Regex parsing fallback\n",
    "                \"regex_title\": row['bibtex_title'] or f\"paper_{index}\",\n",
    "                \"regex_author\": None,  # Extend regex parsing for author if needed\n",
    "                \"regex_journal\": None,  # Extend regex parsing for journal if needed\n",
    "                \"regex_year\": None,  # Extend regex parsing for year if needed\n",
    "                \"regex_url\": extract_url(bibtex_entry),\n",
    "                \"regex_doi\": extract_doi(bibtex_entry),\n",
    "                \"regex_arxiv_id\": extract_arxiv_id(bibtex_entry)\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame for easy visualization\n",
    "df_processed = pd.DataFrame(processed_results)\n",
    "\n",
    "# Save processed results to CSV\n",
    "df_processed.to_csv(\"processed_bibtex_links.csv\", index=False)\n",
    "\n",
    "# Join back with the original DataFrame using modelId\n",
    "#df_joined = pd.merge(df_split_temp, df_processed, on='modelId', how='left', suffixes=('', '_processed'))\n",
    "#df_joined.to_csv(\"processed_bibtex_links_with_join.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_processed = pd.read_csv(\"processed_bibtex_links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33340 entries, 0 to 33339\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   modelId          33340 non-null  object \n",
      " 1   bibtex_title     33217 non-null  object \n",
      " 2   bibtex_author    33041 non-null  object \n",
      " 3   bibtex_journal   13114 non-null  object \n",
      " 4   bibtex_year      32943 non-null  object \n",
      " 5   bibtex_url       14191 non-null  object \n",
      " 6   bibtex_doi       4799 non-null   object \n",
      " 7   bibtex_arxiv_id  12835 non-null  object \n",
      " 8   regex_title      33340 non-null  object \n",
      " 9   regex_author     0 non-null      float64\n",
      " 10  regex_journal    0 non-null      float64\n",
      " 11  regex_year       0 non-null      float64\n",
      " 12  regex_url        14200 non-null  object \n",
      " 13  regex_doi        4799 non-null   object \n",
      " 14  regex_arxiv_id   19528 non-null  object \n",
      "dtypes: float64(3), object(12)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33340 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33340/33340 [00:01<00:00, 26207.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the reliability ranking\n",
    "reliability_sources = [\n",
    "    \"bibtex_doi\", \"bibtex_arxiv_id\", \"bibtex_url\", \n",
    "    \"regex_doi\", \"regex_url\", \"regex_arxiv_id\"\n",
    "]\n",
    "\n",
    "# Helper function to construct links based on source\n",
    "def construct_pdf_link(source, value):\n",
    "    if source == \"bibtex_doi\" or source == \"regex_doi\":\n",
    "        return f\"https://doi.org/{value}\"\n",
    "    elif source == \"bibtex_arxiv_id\" or source == \"regex_arxiv_id\":\n",
    "        return f\"https://arxiv.org/pdf/{value}.pdf\"\n",
    "    elif source == \"bibtex_url\" or source == \"regex_url\":\n",
    "        return value  # Already a URL\n",
    "    return None\n",
    "\n",
    "# Function to check if a URL is valid\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        response = requests.head(url, allow_redirects=True, timeout=5)\n",
    "        return response.status_code == 200 and \"pdf\" in response.headers.get('Content-Type', '')\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Process the DataFrame to get the final_url\n",
    "final_urls = []\n",
    "for _, row in tqdm(df_processed.iterrows(), total=len(df_processed)):\n",
    "    final_url = None\n",
    "    for source in reliability_sources:\n",
    "        if pd.notna(row[source]):  # Check if the value is not null\n",
    "            candidate_url = construct_pdf_link(source, row[source])\n",
    "            #if candidate_url and is_valid_url(candidate_url):\n",
    "            if candidate_url:\n",
    "                final_url = candidate_url\n",
    "                break\n",
    "    final_urls.append(final_url)\n",
    "\n",
    "# Add the final_url column to the DataFrame\n",
    "df_processed[\"final_url\"] = final_urls\n",
    "\n",
    "# Save the processed DataFrame\n",
    "df_processed.to_csv(\"processed_final_urls.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_url\n",
      "https://doi.org/10.48550/ARXIV.2209.11055           2076\n",
      "https://arxiv.org/abs/1908.10084                    1236\n",
      "https://www.arxiv.org/abs/2408.10441                1154\n",
      "https://doi.org/10.21437/Interspeech.2018-1456       601\n",
      "https://arxiv.org/pdf/2407.00066.pdf                 502\n",
      "                                                    ... \n",
      "https://arxiv.org/pdf/2409.20196.pdf                   1\n",
      "https://hal.science/hal-04160733                       1\n",
      "https://doi.org/10.48550/arXiv.2212.10440              1\n",
      "https://doi.org/10.1109/INISTA52262.2021.9548335       1\n",
      "https://aclanthology.org/2022.parlaclarin-1.13         1\n",
      "Name: count, Length: 3275, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_processed['final_url'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `df_processed['final_url'].value_counts()` is already calculated\n",
    "# Get the value counts as a DataFrame\n",
    "citation_counts = df_processed['final_url'].value_counts().reset_index()\n",
    "citation_counts.columns = ['final_url', 'count']\n",
    "\n",
    "# Plot the histogram of citation counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(citation_counts['count'], bins=range(1, citation_counts['count'].max() + 1), edgecolor='black', log=True)\n",
    "plt.title('Histogram of Citation Counts', fontsize=16)\n",
    "plt.xlabel('Citation Count', fontsize=14)\n",
    "plt.ylabel('Frequency (Log Scale)', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08635272945410918"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(citation_counts[citation_counts['count']<10])/len(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_url</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>https://doi.org/10.18653/v1/2023.acl-short.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>https://arxiv.org/pdf/arXiv:2409.10819.pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>https://doi.org/10.48550/arXiv.2012.02951</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>https://arxiv.org/pdf/2410.12490.pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>https://arxiv.org/pdf/arXiv:2410.06885.pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>https://arxiv.org/pdf/2409.20196.pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>https://hal.science/hal-04160733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>https://doi.org/10.48550/arXiv.2212.10440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>https://doi.org/10.1109/INISTA52262.2021.9548335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>https://aclanthology.org/2022.parlaclarin-1.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             final_url  count\n",
       "1815     https://doi.org/10.18653/v1/2023.acl-short.75      1\n",
       "1816        https://arxiv.org/pdf/arXiv:2409.10819.pdf      1\n",
       "1817         https://doi.org/10.48550/arXiv.2012.02951      1\n",
       "1818              https://arxiv.org/pdf/2410.12490.pdf      1\n",
       "1819        https://arxiv.org/pdf/arXiv:2410.06885.pdf      1\n",
       "...                                                ...    ...\n",
       "3270              https://arxiv.org/pdf/2409.20196.pdf      1\n",
       "3271                  https://hal.science/hal-04160733      1\n",
       "3272         https://doi.org/10.48550/arXiv.2212.10440      1\n",
       "3273  https://doi.org/10.1109/INISTA52262.2021.9548335      1\n",
       "3274    https://aclanthology.org/2022.parlaclarin-1.13      1\n",
       "\n",
       "[1460 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_counts[citation_counts['count']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_url\n",
       "https://doi.org/10.48550/ARXIV.2209.11055           2076\n",
       "https://arxiv.org/abs/1908.10084                    1236\n",
       "https://www.arxiv.org/abs/2408.10441                1154\n",
       "https://doi.org/10.21437/Interspeech.2018-1456       601\n",
       "https://arxiv.org/pdf/2407.00066.pdf                 502\n",
       "                                                    ... \n",
       "https://arxiv.org/pdf/2409.20196.pdf                   1\n",
       "https://hal.science/hal-04160733                       1\n",
       "https://doi.org/10.48550/arXiv.2212.10440              1\n",
       "https://doi.org/10.1109/INISTA52262.2021.9548335       1\n",
       "https://aclanthology.org/2022.parlaclarin-1.13         1\n",
       "Name: count, Length: 3275, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed['final_url'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 1878/33340 [14:19<3:59:56,  2.19it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, file_name)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Download the PDF\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m downloaded_path \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Append to download info\u001b[39;00m\n\u001b[1;32m     49\u001b[0m download_info\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelId\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_id,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: url,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: downloaded_path \u001b[38;5;28;01mif\u001b[39;00m downloaded_path \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     53\u001b[0m })\n",
      "Cell \u001b[0;32mIn[26], line 20\u001b[0m, in \u001b[0;36mdownload_pdf\u001b[0;34m(url, output_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_path\n",
      "File \u001b[0;32m~/.conda/envs/clake/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.conda/envs/clake/lib/python3.11/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/clake/lib/python3.11/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/clake/lib/python3.11/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/clake/lib/python3.11/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.conda/envs/clake/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.conda/envs/clake/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming df_processed is available and contains 'final_url' and 'modelId' columns\n",
    "# Replace this with the actual DataFrame loading logic if necessary\n",
    "df_processed = pd.read_csv(\"processed_final_urls.csv\")\n",
    "\n",
    "# Create a base directory to save the PDFs\n",
    "base_output_dir = \"downloaded_pdfs_by_model\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "# Helper function to download a PDF\n",
    "def download_pdf(url, output_path):\n",
    "    if os.path.exists(output_path):\n",
    "        return output_path  # Skip downloading if file exists\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=10)\n",
    "        if response.status_code == 200 and \"application/pdf\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    f.write(chunk)\n",
    "            return output_path\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Prepare a list to store download information\n",
    "download_info = []\n",
    "# Iterate through the final_url column and download PDFs\n",
    "for index, row in tqdm(df_processed.iterrows(), total=len(df_processed)):\n",
    "    url = row['final_url']\n",
    "    model_id = row['modelId']  # Ensure 'modelId' column exists in df_processed\n",
    "    if pd.notna(url) and pd.notna(model_id):\n",
    "        # Create a subdirectory for the current modelId\n",
    "        model_dir = os.path.join(base_output_dir, model_id)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        # Generate a safe filename\n",
    "        file_name = os.path.basename(url.split('/')[-1])\n",
    "        file_path = os.path.join(model_dir, file_name)\n",
    "        # Download the PDF\n",
    "        downloaded_path = download_pdf(url, file_path)\n",
    "        # Append to download info\n",
    "        download_info.append({\n",
    "            \"modelId\": model_id,\n",
    "            \"final_url\": url,\n",
    "            \"local_path\": downloaded_path if downloaded_path else None\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for download info and save it as a CSV\n",
    "download_info_df = pd.DataFrame(download_info)\n",
    "download_info_df.to_csv(\"downloaded_pdfs_by_model_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>author</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>library_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>card</th>\n",
       "      <th>...</th>\n",
       "      <th>card_tags_new_version</th>\n",
       "      <th>card_tags_pipeline_tag</th>\n",
       "      <th>card_tags_library_name</th>\n",
       "      <th>card_tags_tags</th>\n",
       "      <th>card_tags_datasets</th>\n",
       "      <th>analysis_keyword_status</th>\n",
       "      <th>detected_keywords</th>\n",
       "      <th>is_default_card</th>\n",
       "      <th>contains_markdown_table</th>\n",
       "      <th>extracted_markdown_table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>2024-11-05 15:25:48+00:00</td>\n",
       "      <td>391757489</td>\n",
       "      <td>874</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>[sentence-transformers, pytorch, onnx, safeten...</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>[sentence-transformers, feature-extraction, se...</td>\n",
       "      <td>s2orc, flax-sentence-embeddings/stackexchange_...</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Dataset                                     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nesaorg/benchmark_v0</td>\n",
       "      <td>nesaorg</td>\n",
       "      <td>2024-08-19 18:24:49+00:00</td>\n",
       "      <td>98012579</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>[safetensors, model_hub_mixin, pytorch_model_h...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-13</td>\n",
       "      <td>---\\ntags:\\n- model_hub_mixin\\n- pytorch_model...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[model_hub_mixin, pytorch_model_hub_mixin]</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>2024-11-01 10:26:30+00:00</td>\n",
       "      <td>74110727</td>\n",
       "      <td>2526</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>[sentence-transformers, pytorch, tf, rust, onn...</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>[sentence-transformers, feature-extraction, se...</td>\n",
       "      <td>s2orc, flax-sentence-embeddings/stackexchange_...</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Dataset                                     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google-bert/bert-base-uncased</td>\n",
       "      <td>google-bert</td>\n",
       "      <td>2024-02-19 11:06:12+00:00</td>\n",
       "      <td>61569952</td>\n",
       "      <td>1897</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, coreml,...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[exbert]</td>\n",
       "      <td>bookcorpus, wikipedia</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Model | #params | Language |\\n|-------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FacebookAI/xlm-roberta-large</td>\n",
       "      <td>FacebookAI</td>\n",
       "      <td>2024-02-19 12:48:30+00:00</td>\n",
       "      <td>57186100</td>\n",
       "      <td>345</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, onnx, safeten...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\ntags:\\n- exbert\\nlanguage:\\n- multilingua...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[exbert]</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108754</th>\n",
       "      <td>barchetta/baco-131233</td>\n",
       "      <td>barchetta</td>\n",
       "      <td>2024-11-13 01:33:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>Entry not found</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108755</th>\n",
       "      <td>saqqdy/Qwen-Qwen1.5-0.5B-1731461591</td>\n",
       "      <td>saqqdy</td>\n",
       "      <td>2024-11-13 01:33:19+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>peft</td>\n",
       "      <td>[peft, safetensors, arxiv:1910.09700, base_mod...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: Qwen/Qwen1.5-0.5B\\nlibrary_na...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>peft</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108756</th>\n",
       "      <td>minhaozhang/Llama-3.2-1B-Instruct-MBTI-JP</td>\n",
       "      <td>minhaozhang</td>\n",
       "      <td>2024-11-13 01:33:17+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>Entry not found</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108757</th>\n",
       "      <td>mradermacher/Mistral-quiet-star-demo-GGUF</td>\n",
       "      <td>mradermacher</td>\n",
       "      <td>2024-11-13 01:33:42+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: liminerity/Mistral-quiet-star...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[text-generation-inference, transformers, unsl...</td>\n",
       "      <td>gate369/Alpaca-Star</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Link | Type | Size/GB | Notes |\\n|:-----|:--...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108758</th>\n",
       "      <td>huyhoangt2201/llama-3.2-1b-chat-sql3-merged</td>\n",
       "      <td>huyhoangt2201</td>\n",
       "      <td>2024-11-13 01:34:12+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nlibrary_name: transformers\\ntags: []\\n---...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1108759 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             modelId                 author  \\\n",
       "0            sentence-transformers/all-mpnet-base-v2  sentence-transformers   \n",
       "1                               nesaorg/benchmark_v0                nesaorg   \n",
       "2             sentence-transformers/all-MiniLM-L6-v2  sentence-transformers   \n",
       "3                      google-bert/bert-base-uncased            google-bert   \n",
       "4                       FacebookAI/xlm-roberta-large             FacebookAI   \n",
       "...                                              ...                    ...   \n",
       "1108754                        barchetta/baco-131233              barchetta   \n",
       "1108755          saqqdy/Qwen-Qwen1.5-0.5B-1731461591                 saqqdy   \n",
       "1108756    minhaozhang/Llama-3.2-1B-Instruct-MBTI-JP            minhaozhang   \n",
       "1108757    mradermacher/Mistral-quiet-star-demo-GGUF           mradermacher   \n",
       "1108758  huyhoangt2201/llama-3.2-1b-chat-sql3-merged          huyhoangt2201   \n",
       "\n",
       "                    last_modified  downloads  likes           library_name  \\\n",
       "0       2024-11-05 15:25:48+00:00  391757489    874  sentence-transformers   \n",
       "1       2024-08-19 18:24:49+00:00   98012579      1                   None   \n",
       "2       2024-11-01 10:26:30+00:00   74110727   2526  sentence-transformers   \n",
       "3       2024-02-19 11:06:12+00:00   61569952   1897           transformers   \n",
       "4       2024-02-19 12:48:30+00:00   57186100    345           transformers   \n",
       "...                           ...        ...    ...                    ...   \n",
       "1108754 2024-11-13 01:33:00+00:00          0      0                   None   \n",
       "1108755 2024-11-13 01:33:19+00:00          0      0                   peft   \n",
       "1108756 2024-11-13 01:33:17+00:00          0      0                   None   \n",
       "1108757 2024-11-13 01:33:42+00:00          0      0                   None   \n",
       "1108758 2024-11-13 01:34:12+00:00          0      0                   None   \n",
       "\n",
       "                                                      tags  \\\n",
       "0        [sentence-transformers, pytorch, onnx, safeten...   \n",
       "1        [safetensors, model_hub_mixin, pytorch_model_h...   \n",
       "2        [sentence-transformers, pytorch, tf, rust, onn...   \n",
       "3        [transformers, pytorch, tf, jax, rust, coreml,...   \n",
       "4        [transformers, pytorch, tf, jax, onnx, safeten...   \n",
       "...                                                    ...   \n",
       "1108754                                        [region:us]   \n",
       "1108755  [peft, safetensors, arxiv:1910.09700, base_mod...   \n",
       "1108756                                        [region:us]   \n",
       "1108757                                        [region:us]   \n",
       "1108758                                        [region:us]   \n",
       "\n",
       "                pipeline_tag   createdAt  \\\n",
       "0        sentence-similarity  2022-03-02   \n",
       "1                       None  2024-08-13   \n",
       "2        sentence-similarity  2022-03-02   \n",
       "3                  fill-mask  2022-03-02   \n",
       "4                  fill-mask  2022-03-02   \n",
       "...                      ...         ...   \n",
       "1108754                 None  2024-11-13   \n",
       "1108755                 None  2024-11-13   \n",
       "1108756                 None  2024-11-13   \n",
       "1108757                 None  2024-11-13   \n",
       "1108758                 None  2024-11-13   \n",
       "\n",
       "                                                      card  ...  \\\n",
       "0        ---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...  ...   \n",
       "1        ---\\ntags:\\n- model_hub_mixin\\n- pytorch_model...  ...   \n",
       "2        ---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...  ...   \n",
       "3        ---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...  ...   \n",
       "4        ---\\ntags:\\n- exbert\\nlanguage:\\n- multilingua...  ...   \n",
       "...                                                    ...  ...   \n",
       "1108754                                    Entry not found  ...   \n",
       "1108755  ---\\nbase_model: Qwen/Qwen1.5-0.5B\\nlibrary_na...  ...   \n",
       "1108756                                    Entry not found  ...   \n",
       "1108757  ---\\nbase_model: liminerity/Mistral-quiet-star...  ...   \n",
       "1108758  ---\\nlibrary_name: transformers\\ntags: []\\n---...  ...   \n",
       "\n",
       "        card_tags_new_version card_tags_pipeline_tag card_tags_library_name  \\\n",
       "0                        None    sentence-similarity  sentence-transformers   \n",
       "1                        None                   None                   None   \n",
       "2                        None    sentence-similarity  sentence-transformers   \n",
       "3                        None                   None                   None   \n",
       "4                        None                   None                   None   \n",
       "...                       ...                    ...                    ...   \n",
       "1108754                  None                   None                   None   \n",
       "1108755                  None                   None                   peft   \n",
       "1108756                  None                   None                   None   \n",
       "1108757                  None                   None           transformers   \n",
       "1108758                  None                   None           transformers   \n",
       "\n",
       "                                            card_tags_tags  \\\n",
       "0        [sentence-transformers, feature-extraction, se...   \n",
       "1               [model_hub_mixin, pytorch_model_hub_mixin]   \n",
       "2        [sentence-transformers, feature-extraction, se...   \n",
       "3                                                 [exbert]   \n",
       "4                                                 [exbert]   \n",
       "...                                                    ...   \n",
       "1108754                                                 []   \n",
       "1108755                                                 []   \n",
       "1108756                                                 []   \n",
       "1108757  [text-generation-inference, transformers, unsl...   \n",
       "1108758                                               [[]]   \n",
       "\n",
       "                                        card_tags_datasets  \\\n",
       "0        s2orc, flax-sentence-embeddings/stackexchange_...   \n",
       "1                                                     None   \n",
       "2        s2orc, flax-sentence-embeddings/stackexchange_...   \n",
       "3                                    bookcorpus, wikipedia   \n",
       "4                                                     None   \n",
       "...                                                    ...   \n",
       "1108754                                               None   \n",
       "1108755                                               None   \n",
       "1108756                                               None   \n",
       "1108757                                gate369/Alpaca-Star   \n",
       "1108758                                               None   \n",
       "\n",
       "        analysis_keyword_status detected_keywords is_default_card  \\\n",
       "0                          none                []           False   \n",
       "1                          none                []           False   \n",
       "2                          none                []           False   \n",
       "3                          none                []           False   \n",
       "4                          none                []           False   \n",
       "...                         ...               ...             ...   \n",
       "1108754                    none                []           False   \n",
       "1108755                    none                []            True   \n",
       "1108756                    none                []           False   \n",
       "1108757                    none                []           False   \n",
       "1108758                    none                []            True   \n",
       "\n",
       "        contains_markdown_table  \\\n",
       "0                          True   \n",
       "1                         False   \n",
       "2                          True   \n",
       "3                          True   \n",
       "4                         False   \n",
       "...                         ...   \n",
       "1108754                   False   \n",
       "1108755                   False   \n",
       "1108756                   False   \n",
       "1108757                    True   \n",
       "1108758                   False   \n",
       "\n",
       "                                  extracted_markdown_table  \n",
       "0        | Dataset                                     ...  \n",
       "1                                                     None  \n",
       "2        | Dataset                                     ...  \n",
       "3        | Model | #params | Language |\\n|-------------...  \n",
       "4                                                     None  \n",
       "...                                                    ...  \n",
       "1108754                                               None  \n",
       "1108755                                               None  \n",
       "1108756                                               None  \n",
       "1108757  | Link | Type | Size/GB | Notes |\\n|:-----|:--...  \n",
       "1108758                                               None  \n",
       "\n",
       "[1108759 rows x 26 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_temp = pd.read_parquet('data/modelcard_step3_markdown_gated.parquet')\n",
    "df_split_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cards with all links: 580229/1108759 = 52.33%\n",
      "Model cards with GitHub links: 194586/1108759 = 17.55%\n",
      "Model cards with PDF links: 226015/1108759 = 20.38%\n",
      "Model cards with NO PDF but HAS GitHub links: 158688/1108759 = 14.31%\n"
     ]
    }
   ],
   "source": [
    "# then download the github readme files for all files\n",
    "# first get github links\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Valid PDF link domains\n",
    "VALID_PDF_LINKS = [\n",
    "    \"arxiv.org\",\n",
    "    \"biorxiv.org\",\n",
    "    \"medrxiv.org\",\n",
    "    \"dl.acm.org\",\n",
    "    \"dblp.uni-trier.de\",\n",
    "    \"scholar.google.com\",\n",
    "    \"pubmed.ncbi.nlm.nih.gov\",\n",
    "    \"frontiersin.org\",\n",
    "    \"mdpi.com\",\n",
    "    \"cvpr.thecvf.com\",\n",
    "    \"nips.cc\",\n",
    "    \"icml.cc\",\n",
    "    \"ijcai.org\",\n",
    "    \"webofscience.com\",\n",
    "    \"journals.plos.org\",\n",
    "    \"nature.com\",\n",
    "    \"semanticscholar.org\",\n",
    "    \"chemrxiv.org\",\n",
    "    \"link.springer.com\",\n",
    "    \"ieeexplore.ieee.org\",\n",
    "    \"aaai.org\",\n",
    "    \"openaccess.thecvf.com\",\n",
    "]\n",
    "\n",
    "# Function to extract links from text\n",
    "def extract_links(text):\n",
    "    \"\"\"Extract PDF and GitHub links from the text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {\"pdf_link\": None, \"github_link\": None, \"all_links\": []}\n",
    "    \n",
    "    # Find all links (match https://, http://, and www.)\n",
    "    all_links = [link.strip(\".,)\") for link in re.findall(r\"(https?://\\S+|www\\.\\S+)\", text)]\n",
    "    # Function to check if the link is a valid PDF link, excluding specific ones\n",
    "    def is_valid_pdf_link(link):\n",
    "        \"\"\"\n",
    "        Check if a link is a valid PDF link:\n",
    "        1. Matches one of the predefined VALID_PDF_LINKS domains;\n",
    "        2. Ends with \".pdf\";\n",
    "        3. Allows 'arxiv:1910.09700' but still requires other valid PDF links.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parsed_url = urlparse(link)\n",
    "            domain = parsed_url.netloc.lstrip(\"www.\")  # Remove \"www.\" prefix\n",
    "        except Exception:\n",
    "            return False  # Invalid link\n",
    "        # If the link is 'arxiv:1910.09700', allow it but still need another valid link\n",
    "        return (domain in VALID_PDF_LINKS or link.lower().endswith(\".pdf\"))\n",
    "\n",
    "    # Filter PDF and GitHub links\n",
    "    pdf_links = [link for link in all_links if is_valid_pdf_link(link)]\n",
    "    github_links = [link for link in all_links if \"github.com\" in link]\n",
    "    # If 'arxiv:1910.09700' is found, ensure there are other valid PDF links\n",
    "    has_arxiv_1910 = any(\"arxiv:1910.09700\" in link for link in all_links)\n",
    "    if has_arxiv_1910 and len(pdf_links) == 1 and \"arxiv:1910.09700\" in pdf_links:\n",
    "        pdf_links = []  # Exclude arxiv:1910.09700 if no other valid link exists\n",
    "    # Return results\n",
    "    return {\n",
    "        \"pdf_link\": pdf_links if pdf_links else None,\n",
    "        \"github_link\": github_links if github_links else None,\n",
    "        \"all_links\": all_links if all_links else None\n",
    "    }\n",
    "\n",
    "# Ensure the 'card_readme' column is filled with an empty string if it's NaN\n",
    "df_split_temp['combined_text'] = df_split_temp['card_readme'].fillna('')\n",
    "\n",
    "# Apply the link extraction function\n",
    "results = df_split_temp['combined_text'].apply(extract_links)\n",
    "\n",
    "# Extract the results into separate columns\n",
    "df_split_temp['pdf_link'] = results.apply(lambda x: x[\"pdf_link\"] if x[\"pdf_link\"] else None)\n",
    "df_split_temp['github_link'] = results.apply(lambda x: x[\"github_link\"] if x[\"github_link\"] else None)\n",
    "df_split_temp['all_links'] = results.apply(lambda x: ', '.join(x[\"all_links\"]) if x[\"all_links\"] else None)\n",
    "\n",
    "# Check if each link type column has a non-empty value (avoids empty strings, lists, or NaNs)\n",
    "df_split_temp['all_links_non_empty'] = df_split_temp['all_links'].apply(lambda x: bool(x) and x != '[]' and x != '')\n",
    "df_split_temp['pdf_link_non_empty'] = df_split_temp['pdf_link'].apply(lambda x: bool(x) and x != '[]' and x != '')\n",
    "df_split_temp['github_link_non_empty'] = df_split_temp['github_link'].apply(lambda x: bool(x) and x != '[]' and x != '')\n",
    "\n",
    "# Count the occurrences and calculate the ratios\n",
    "pdf_link_count = df_split_temp['pdf_link_non_empty'].sum()\n",
    "total_count = len(df_split_temp)\n",
    "pdf_link_ratio = (pdf_link_count / total_count) * 100\n",
    "\n",
    "# Count rows where 'all_links' is non-empty\n",
    "all_link_count = df_split_temp['all_links_non_empty'].sum()\n",
    "all_link_ratio = (all_link_count / total_count) * 100\n",
    "\n",
    "github_link_count = df_split_temp['github_link_non_empty'].sum()\n",
    "github_link_ratio = (github_link_count / total_count) * 100\n",
    "\n",
    "# Count entries with no PDF but with GitHub links\n",
    "no_pdf_has_github_count = df_split_temp[~df_split_temp['pdf_link_non_empty'] & df_split_temp['github_link_non_empty']].shape[0]\n",
    "no_pdf_has_github_ratio = (no_pdf_has_github_count / total_count) * 100\n",
    "\n",
    "# Output the results\n",
    "print(f\"Model cards with all links: {all_link_count}/{total_count} = {all_link_ratio:.2f}%\")\n",
    "print(f\"Model cards with GitHub links: {github_link_count}/{total_count} = {github_link_ratio:.2f}%\")\n",
    "print(f\"Model cards with PDF links: {pdf_link_count}/{total_count} = {pdf_link_ratio:.2f}%\")\n",
    "print(f\"Model cards with NO PDF but HAS GitHub links: \"\n",
    "      f\"{no_pdf_has_github_count}/{total_count} = {no_pdf_has_github_ratio:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>author</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>library_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>card</th>\n",
       "      <th>...</th>\n",
       "      <th>is_default_card</th>\n",
       "      <th>contains_markdown_table</th>\n",
       "      <th>extracted_markdown_table</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>github_link</th>\n",
       "      <th>all_links</th>\n",
       "      <th>all_links_non_empty</th>\n",
       "      <th>pdf_link_non_empty</th>\n",
       "      <th>github_link_non_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>2024-11-05 15:25:48+00:00</td>\n",
       "      <td>391757489</td>\n",
       "      <td>874</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>[sentence-transformers, pytorch, onnx, safeten...</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Dataset                                     ...</td>\n",
       "      <td># all-mpnet-base-v2\\nThis is a [sentence-trans...</td>\n",
       "      <td>[https://arxiv.org/abs/1904.06472, https://arx...</td>\n",
       "      <td>[https://github.com/PolyAI-LDN/conversational-...</td>\n",
       "      <td>https://www.SBERT.net, https://www.SBERT.net, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>2024-11-01 10:26:30+00:00</td>\n",
       "      <td>74110727</td>\n",
       "      <td>2526</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>[sentence-transformers, pytorch, tf, rust, onn...</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Dataset                                     ...</td>\n",
       "      <td># all-MiniLM-L6-v2\\nThis is a [sentence-transf...</td>\n",
       "      <td>[https://arxiv.org/abs/1904.06472, https://arx...</td>\n",
       "      <td>[https://github.com/PolyAI-LDN/conversational-...</td>\n",
       "      <td>https://www.SBERT.net, https://www.SBERT.net, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google-bert/bert-base-uncased</td>\n",
       "      <td>google-bert</td>\n",
       "      <td>2024-02-19 11:06:12+00:00</td>\n",
       "      <td>61569952</td>\n",
       "      <td>1897</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, coreml,...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Model | #params | Language |\\n|-------------...</td>\n",
       "      <td># BERT base model (uncased)\\n\\nPretrained mode...</td>\n",
       "      <td>[https://arxiv.org/abs/1810.04805, http://arxi...</td>\n",
       "      <td>[https://github.com/google-research/bert, http...</td>\n",
       "      <td>https://arxiv.org/abs/1810.04805, https://gith...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FacebookAI/xlm-roberta-large</td>\n",
       "      <td>FacebookAI</td>\n",
       "      <td>2024-02-19 12:48:30+00:00</td>\n",
       "      <td>57186100</td>\n",
       "      <td>345</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, onnx, safeten...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\ntags:\\n- exbert\\nlanguage:\\n- multilingua...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td># XLM-RoBERTa (large-sized model) \\n\\nXLM-RoBE...</td>\n",
       "      <td>[https://arxiv.org/abs/1911.02116, http://arxi...</td>\n",
       "      <td>[https://github.com/pytorch/fairseq/tree/maste...</td>\n",
       "      <td>https://arxiv.org/abs/1911.02116, https://gith...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>openai/whisper-large-v2</td>\n",
       "      <td>openai</td>\n",
       "      <td>2024-02-29 10:57:50+00:00</td>\n",
       "      <td>44239856</td>\n",
       "      <td>1649</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, safetensors, ...</td>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>---\\nlanguage: \\n- en\\n- zh\\n- de\\n- es\\n- ru\\...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Size     | Parameters | English-only        ...</td>\n",
       "      <td># Whisper\\n\\nWhisper is a pre-trained model fo...</td>\n",
       "      <td>[https://arxiv.org/abs/2212.04356, https://cdn...</td>\n",
       "      <td>[https://github.com/openai/whisper]</td>\n",
       "      <td>https://arxiv.org/abs/2212.04356, https://gith...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108731</th>\n",
       "      <td>bau0221/1113_model_1</td>\n",
       "      <td>bau0221</td>\n",
       "      <td>2024-11-13 01:25:10+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, safetensors, text-generation-in...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: unsloth/phi-3.5-mini-instruct...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td># Uploaded  model\\n\\n- **Developed by:** bau02...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://github.com/unslothai/unsloth, https:/...</td>\n",
       "      <td>https://github.com/unslothai/unsloth, https://...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108736</th>\n",
       "      <td>benito14/1B_finetuned_llama3.2</td>\n",
       "      <td>benito14</td>\n",
       "      <td>2024-11-13 01:27:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, safetensors, text-generation-in...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: unsloth/Llama-3.2-1B-bnb-4bit...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td># Uploaded  model\\n\\n- **Developed by:** benit...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://github.com/unslothai/unsloth, https:/...</td>\n",
       "      <td>https://github.com/unslothai/unsloth, https://...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108740</th>\n",
       "      <td>async0x42/Qwen2.5-Coder-32B-Instruct-exl2_4.0bpw</td>\n",
       "      <td>async0x42</td>\n",
       "      <td>2024-11-13 01:28:25+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlicense_link: https:...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td># Qwen2.5-Coder-32B-Instruct\\n\\n## Introductio...</td>\n",
       "      <td>[https://arxiv.org/abs/2409.12186, https://arx...</td>\n",
       "      <td>[https://github.com/QwenLM/Qwen2.5-Coder]</td>\n",
       "      <td>https://qwenlm.github.io/blog/qwen2.5-coder-fa...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108750</th>\n",
       "      <td>benito14/SOIT_Llama3.2_model4</td>\n",
       "      <td>benito14</td>\n",
       "      <td>2024-11-13 01:33:26+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, gguf, llama, text-generation-in...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: unsloth/Llama-3.2-1B-bnb-4bit...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td># Uploaded  model\\n\\n- **Developed by:** benit...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://github.com/unslothai/unsloth, https:/...</td>\n",
       "      <td>https://github.com/unslothai/unsloth, https://...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108757</th>\n",
       "      <td>mradermacher/Mistral-quiet-star-demo-GGUF</td>\n",
       "      <td>mradermacher</td>\n",
       "      <td>2024-11-13 01:33:42+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: liminerity/Mistral-quiet-star...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Link | Type | Size/GB | Notes |\\n|:-----|:--...</td>\n",
       "      <td>## About\\n\\n&lt;!-- ### quantize_version: 2 --&gt;\\n...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://gist.github.com/Artefact2/b5f81060077...</td>\n",
       "      <td>https://huggingface.co/liminerity/Mistral-quie...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194586 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  modelId  \\\n",
       "0                 sentence-transformers/all-mpnet-base-v2   \n",
       "2                  sentence-transformers/all-MiniLM-L6-v2   \n",
       "3                           google-bert/bert-base-uncased   \n",
       "4                            FacebookAI/xlm-roberta-large   \n",
       "5                                 openai/whisper-large-v2   \n",
       "...                                                   ...   \n",
       "1108731                              bau0221/1113_model_1   \n",
       "1108736                    benito14/1B_finetuned_llama3.2   \n",
       "1108740  async0x42/Qwen2.5-Coder-32B-Instruct-exl2_4.0bpw   \n",
       "1108750                     benito14/SOIT_Llama3.2_model4   \n",
       "1108757         mradermacher/Mistral-quiet-star-demo-GGUF   \n",
       "\n",
       "                        author             last_modified  downloads  likes  \\\n",
       "0        sentence-transformers 2024-11-05 15:25:48+00:00  391757489    874   \n",
       "2        sentence-transformers 2024-11-01 10:26:30+00:00   74110727   2526   \n",
       "3                  google-bert 2024-02-19 11:06:12+00:00   61569952   1897   \n",
       "4                   FacebookAI 2024-02-19 12:48:30+00:00   57186100    345   \n",
       "5                       openai 2024-02-29 10:57:50+00:00   44239856   1649   \n",
       "...                        ...                       ...        ...    ...   \n",
       "1108731                bau0221 2024-11-13 01:25:10+00:00          0      0   \n",
       "1108736               benito14 2024-11-13 01:27:44+00:00          0      0   \n",
       "1108740              async0x42 2024-11-13 01:28:25+00:00          0      0   \n",
       "1108750               benito14 2024-11-13 01:33:26+00:00          0      0   \n",
       "1108757           mradermacher 2024-11-13 01:33:42+00:00          0      0   \n",
       "\n",
       "                  library_name  \\\n",
       "0        sentence-transformers   \n",
       "2        sentence-transformers   \n",
       "3                 transformers   \n",
       "4                 transformers   \n",
       "5                 transformers   \n",
       "...                        ...   \n",
       "1108731           transformers   \n",
       "1108736           transformers   \n",
       "1108740                   None   \n",
       "1108750           transformers   \n",
       "1108757                   None   \n",
       "\n",
       "                                                      tags  \\\n",
       "0        [sentence-transformers, pytorch, onnx, safeten...   \n",
       "2        [sentence-transformers, pytorch, tf, rust, onn...   \n",
       "3        [transformers, pytorch, tf, jax, rust, coreml,...   \n",
       "4        [transformers, pytorch, tf, jax, onnx, safeten...   \n",
       "5        [transformers, pytorch, tf, jax, safetensors, ...   \n",
       "...                                                    ...   \n",
       "1108731  [transformers, safetensors, text-generation-in...   \n",
       "1108736  [transformers, safetensors, text-generation-in...   \n",
       "1108740                                        [region:us]   \n",
       "1108750  [transformers, gguf, llama, text-generation-in...   \n",
       "1108757                                        [region:us]   \n",
       "\n",
       "                         pipeline_tag   createdAt  \\\n",
       "0                 sentence-similarity  2022-03-02   \n",
       "2                 sentence-similarity  2022-03-02   \n",
       "3                           fill-mask  2022-03-02   \n",
       "4                           fill-mask  2022-03-02   \n",
       "5        automatic-speech-recognition  2022-12-05   \n",
       "...                               ...         ...   \n",
       "1108731                          None  2024-11-13   \n",
       "1108736                          None  2024-11-13   \n",
       "1108740                          None  2024-11-13   \n",
       "1108750                          None  2024-11-13   \n",
       "1108757                          None  2024-11-13   \n",
       "\n",
       "                                                      card  ...  \\\n",
       "0        ---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...  ...   \n",
       "2        ---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...  ...   \n",
       "3        ---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...  ...   \n",
       "4        ---\\ntags:\\n- exbert\\nlanguage:\\n- multilingua...  ...   \n",
       "5        ---\\nlanguage: \\n- en\\n- zh\\n- de\\n- es\\n- ru\\...  ...   \n",
       "...                                                    ...  ...   \n",
       "1108731  ---\\nbase_model: unsloth/phi-3.5-mini-instruct...  ...   \n",
       "1108736  ---\\nbase_model: unsloth/Llama-3.2-1B-bnb-4bit...  ...   \n",
       "1108740  ---\\nlicense: apache-2.0\\nlicense_link: https:...  ...   \n",
       "1108750  ---\\nbase_model: unsloth/Llama-3.2-1B-bnb-4bit...  ...   \n",
       "1108757  ---\\nbase_model: liminerity/Mistral-quiet-star...  ...   \n",
       "\n",
       "        is_default_card contains_markdown_table  \\\n",
       "0                 False                    True   \n",
       "2                 False                    True   \n",
       "3                 False                    True   \n",
       "4                 False                   False   \n",
       "5                 False                    True   \n",
       "...                 ...                     ...   \n",
       "1108731           False                   False   \n",
       "1108736           False                   False   \n",
       "1108740           False                   False   \n",
       "1108750           False                   False   \n",
       "1108757           False                    True   \n",
       "\n",
       "                                  extracted_markdown_table  \\\n",
       "0        | Dataset                                     ...   \n",
       "2        | Dataset                                     ...   \n",
       "3        | Model | #params | Language |\\n|-------------...   \n",
       "4                                                     None   \n",
       "5        | Size     | Parameters | English-only        ...   \n",
       "...                                                    ...   \n",
       "1108731                                               None   \n",
       "1108736                                               None   \n",
       "1108740                                               None   \n",
       "1108750                                               None   \n",
       "1108757  | Link | Type | Size/GB | Notes |\\n|:-----|:--...   \n",
       "\n",
       "                                             combined_text  \\\n",
       "0        # all-mpnet-base-v2\\nThis is a [sentence-trans...   \n",
       "2        # all-MiniLM-L6-v2\\nThis is a [sentence-transf...   \n",
       "3        # BERT base model (uncased)\\n\\nPretrained mode...   \n",
       "4        # XLM-RoBERTa (large-sized model) \\n\\nXLM-RoBE...   \n",
       "5        # Whisper\\n\\nWhisper is a pre-trained model fo...   \n",
       "...                                                    ...   \n",
       "1108731  # Uploaded  model\\n\\n- **Developed by:** bau02...   \n",
       "1108736  # Uploaded  model\\n\\n- **Developed by:** benit...   \n",
       "1108740  # Qwen2.5-Coder-32B-Instruct\\n\\n## Introductio...   \n",
       "1108750  # Uploaded  model\\n\\n- **Developed by:** benit...   \n",
       "1108757  ## About\\n\\n<!-- ### quantize_version: 2 -->\\n...   \n",
       "\n",
       "                                                  pdf_link  \\\n",
       "0        [https://arxiv.org/abs/1904.06472, https://arx...   \n",
       "2        [https://arxiv.org/abs/1904.06472, https://arx...   \n",
       "3        [https://arxiv.org/abs/1810.04805, http://arxi...   \n",
       "4        [https://arxiv.org/abs/1911.02116, http://arxi...   \n",
       "5        [https://arxiv.org/abs/2212.04356, https://cdn...   \n",
       "...                                                    ...   \n",
       "1108731                                               None   \n",
       "1108736                                               None   \n",
       "1108740  [https://arxiv.org/abs/2409.12186, https://arx...   \n",
       "1108750                                               None   \n",
       "1108757                                               None   \n",
       "\n",
       "                                               github_link  \\\n",
       "0        [https://github.com/PolyAI-LDN/conversational-...   \n",
       "2        [https://github.com/PolyAI-LDN/conversational-...   \n",
       "3        [https://github.com/google-research/bert, http...   \n",
       "4        [https://github.com/pytorch/fairseq/tree/maste...   \n",
       "5                      [https://github.com/openai/whisper]   \n",
       "...                                                    ...   \n",
       "1108731  [https://github.com/unslothai/unsloth, https:/...   \n",
       "1108736  [https://github.com/unslothai/unsloth, https:/...   \n",
       "1108740          [https://github.com/QwenLM/Qwen2.5-Coder]   \n",
       "1108750  [https://github.com/unslothai/unsloth, https:/...   \n",
       "1108757  [https://gist.github.com/Artefact2/b5f81060077...   \n",
       "\n",
       "                                                 all_links  \\\n",
       "0        https://www.SBERT.net, https://www.SBERT.net, ...   \n",
       "2        https://www.SBERT.net, https://www.SBERT.net, ...   \n",
       "3        https://arxiv.org/abs/1810.04805, https://gith...   \n",
       "4        https://arxiv.org/abs/1911.02116, https://gith...   \n",
       "5        https://arxiv.org/abs/2212.04356, https://gith...   \n",
       "...                                                    ...   \n",
       "1108731  https://github.com/unslothai/unsloth, https://...   \n",
       "1108736  https://github.com/unslothai/unsloth, https://...   \n",
       "1108740  https://qwenlm.github.io/blog/qwen2.5-coder-fa...   \n",
       "1108750  https://github.com/unslothai/unsloth, https://...   \n",
       "1108757  https://huggingface.co/liminerity/Mistral-quie...   \n",
       "\n",
       "        all_links_non_empty pdf_link_non_empty github_link_non_empty  \n",
       "0                      True               True                  True  \n",
       "2                      True               True                  True  \n",
       "3                      True               True                  True  \n",
       "4                      True               True                  True  \n",
       "5                      True               True                  True  \n",
       "...                     ...                ...                   ...  \n",
       "1108731                True              False                  True  \n",
       "1108736                True              False                  True  \n",
       "1108740                True               True                  True  \n",
       "1108750                True              False                  True  \n",
       "1108757                True              False                  True  \n",
       "\n",
       "[194586 rows x 33 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_temp[~df_split_temp['github_link'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_temp.to_csv('data/tmp_df_split_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_temp = pd.read_csv('data/tmp_df_split_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming df_split_temp is available and contains 'github_link' and 'modelId' columns\n",
    "# Replace this with actual DataFrame loading if necessary\n",
    "#df_split_temp = pd.read_parquet('data/modelcard_step3_markdown_gated.parquet')\n",
    "df_split_temp = pd.read_csv('data/tmp_df_split_temp.csv')\n",
    "\n",
    "# Create a base directory to save the READMEs\n",
    "base_output_dir = \"github_readmes\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "# Helper function to download the README file\n",
    "def download_readme(github_url, output_path):\n",
    "    try:\n",
    "        # Ensure the URL is valid and not a list\n",
    "        if isinstance(github_url, list):\n",
    "            github_url = github_url[0]  # Take the first URL from the list\n",
    "        # Construct raw.githubusercontent.com link\n",
    "        raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").rstrip(\"/\") + \"/main/README.md\"\n",
    "        response = requests.get(raw_url, timeout=10)\n",
    "        if response.status_code == 200:  # File exists\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            return output_path\n",
    "        else:\n",
    "            print(f\"Error: Unable to download README from {raw_url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Exception occurred while downloading {github_url} - {e}\")\n",
    "        return None\n",
    "\n",
    "# Prepare a list to store download information\n",
    "download_info = []\n",
    "# Iterate through the GitHub links and download READMEs\n",
    "for index, row in tqdm(df_split_temp.iterrows(), total=len(df_split_temp)):\n",
    "    github_link = row['github_link']\n",
    "    model_id = row['modelId']\n",
    "    # Ensure github_link is a string or take the first element if it's a list\n",
    "    if isinstance(github_link, list) and len(github_link) > 0:\n",
    "        github_link = github_link[0]  # Take the first URL from the list\n",
    "    elif not isinstance(github_link, str):\n",
    "        print(f\"Skipping invalid github_link for modelId {model_id}: {github_link}\")\n",
    "        continue\n",
    "    if pd.notna(github_link) and pd.notna(model_id):\n",
    "        # Create a subdirectory for the current modelId\n",
    "        model_dir = os.path.join(base_output_dir, model_id)\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "            print(f\"Directory created: {model_dir}\")\n",
    "        else:\n",
    "            print(f\"Directory exists: {model_dir}\")\n",
    "        # Target file path\n",
    "        readme_path = os.path.join(model_dir, \"README.md\")\n",
    "        # Check if the README file already exists\n",
    "        if os.path.exists(readme_path):\n",
    "            print(f\"README already exists for modelId {model_id}, skipping download.\")\n",
    "            downloaded_path = readme_path\n",
    "        else:\n",
    "            # Download the README file\n",
    "            downloaded_path = download_readme(github_link, readme_path)\n",
    "        # Save download information\n",
    "        download_info.append({\n",
    "            \"modelId\": model_id,\n",
    "            \"github_link\": github_link,\n",
    "            \"readme_path\": downloaded_path if downloaded_path else None\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for download information and save it to a CSV\n",
    "download_info_df = pd.DataFrame(download_info)\n",
    "download_info_df.to_csv(\"github_readmes_info.csv\", index=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Downloaded {len([d for d in download_info if d['readme_path']])} READMEs.\")\n",
    "print(f\"Skipped {len([d for d in download_info if not d['readme_path']])} READMEs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>author</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>library_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>card</th>\n",
       "      <th>...</th>\n",
       "      <th>is_default_card</th>\n",
       "      <th>contains_markdown_table</th>\n",
       "      <th>extracted_markdown_table</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>github_link</th>\n",
       "      <th>all_links</th>\n",
       "      <th>all_links_non_empty</th>\n",
       "      <th>pdf_link_non_empty</th>\n",
       "      <th>github_link_non_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>2024-11-05 15:25:48+00:00</td>\n",
       "      <td>391757489</td>\n",
       "      <td>874</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>[sentence-transformers, pytorch, onnx, safeten...</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Dataset                                     ...</td>\n",
       "      <td># all-mpnet-base-v2\\nThis is a [sentence-trans...</td>\n",
       "      <td>[https://arxiv.org/abs/1904.06472, https://arx...</td>\n",
       "      <td>[https://github.com/PolyAI-LDN/conversational-...</td>\n",
       "      <td>https://www.SBERT.net, https://www.SBERT.net, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>2024-11-01 10:26:30+00:00</td>\n",
       "      <td>74110727</td>\n",
       "      <td>2526</td>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>[sentence-transformers, pytorch, tf, rust, onn...</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Dataset                                     ...</td>\n",
       "      <td># all-MiniLM-L6-v2\\nThis is a [sentence-transf...</td>\n",
       "      <td>[https://arxiv.org/abs/1904.06472, https://arx...</td>\n",
       "      <td>[https://github.com/PolyAI-LDN/conversational-...</td>\n",
       "      <td>https://www.SBERT.net, https://www.SBERT.net, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google-bert/bert-base-uncased</td>\n",
       "      <td>google-bert</td>\n",
       "      <td>2024-02-19 11:06:12+00:00</td>\n",
       "      <td>61569952</td>\n",
       "      <td>1897</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, coreml,...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Model | #params | Language |\\n|-------------...</td>\n",
       "      <td># BERT base model (uncased)\\n\\nPretrained mode...</td>\n",
       "      <td>[https://arxiv.org/abs/1810.04805, http://arxi...</td>\n",
       "      <td>[https://github.com/google-research/bert, http...</td>\n",
       "      <td>https://arxiv.org/abs/1810.04805, https://gith...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FacebookAI/xlm-roberta-large</td>\n",
       "      <td>FacebookAI</td>\n",
       "      <td>2024-02-19 12:48:30+00:00</td>\n",
       "      <td>57186100</td>\n",
       "      <td>345</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, onnx, safeten...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>---\\ntags:\\n- exbert\\nlanguage:\\n- multilingua...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td># XLM-RoBERTa (large-sized model) \\n\\nXLM-RoBE...</td>\n",
       "      <td>[https://arxiv.org/abs/1911.02116, http://arxi...</td>\n",
       "      <td>[https://github.com/pytorch/fairseq/tree/maste...</td>\n",
       "      <td>https://arxiv.org/abs/1911.02116, https://gith...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>openai/whisper-large-v2</td>\n",
       "      <td>openai</td>\n",
       "      <td>2024-02-29 10:57:50+00:00</td>\n",
       "      <td>44239856</td>\n",
       "      <td>1649</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, safetensors, ...</td>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>---\\nlanguage: \\n- en\\n- zh\\n- de\\n- es\\n- ru\\...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Size     | Parameters | English-only        ...</td>\n",
       "      <td># Whisper\\n\\nWhisper is a pre-trained model fo...</td>\n",
       "      <td>[https://arxiv.org/abs/2212.04356, https://cdn...</td>\n",
       "      <td>[https://github.com/openai/whisper]</td>\n",
       "      <td>https://arxiv.org/abs/2212.04356, https://gith...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108731</th>\n",
       "      <td>bau0221/1113_model_1</td>\n",
       "      <td>bau0221</td>\n",
       "      <td>2024-11-13 01:25:10+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, safetensors, text-generation-in...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: unsloth/phi-3.5-mini-instruct...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td># Uploaded  model\\n\\n- **Developed by:** bau02...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://github.com/unslothai/unsloth, https:/...</td>\n",
       "      <td>https://github.com/unslothai/unsloth, https://...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108736</th>\n",
       "      <td>benito14/1B_finetuned_llama3.2</td>\n",
       "      <td>benito14</td>\n",
       "      <td>2024-11-13 01:27:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, safetensors, text-generation-in...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: unsloth/Llama-3.2-1B-bnb-4bit...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td># Uploaded  model\\n\\n- **Developed by:** benit...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://github.com/unslothai/unsloth, https:/...</td>\n",
       "      <td>https://github.com/unslothai/unsloth, https://...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108740</th>\n",
       "      <td>async0x42/Qwen2.5-Coder-32B-Instruct-exl2_4.0bpw</td>\n",
       "      <td>async0x42</td>\n",
       "      <td>2024-11-13 01:28:25+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nlicense_link: https:...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td># Qwen2.5-Coder-32B-Instruct\\n\\n## Introductio...</td>\n",
       "      <td>[https://arxiv.org/abs/2409.12186, https://arx...</td>\n",
       "      <td>[https://github.com/QwenLM/Qwen2.5-Coder]</td>\n",
       "      <td>https://qwenlm.github.io/blog/qwen2.5-coder-fa...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108750</th>\n",
       "      <td>benito14/SOIT_Llama3.2_model4</td>\n",
       "      <td>benito14</td>\n",
       "      <td>2024-11-13 01:33:26+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, gguf, llama, text-generation-in...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: unsloth/Llama-3.2-1B-bnb-4bit...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td># Uploaded  model\\n\\n- **Developed by:** benit...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://github.com/unslothai/unsloth, https:/...</td>\n",
       "      <td>https://github.com/unslothai/unsloth, https://...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108757</th>\n",
       "      <td>mradermacher/Mistral-quiet-star-demo-GGUF</td>\n",
       "      <td>mradermacher</td>\n",
       "      <td>2024-11-13 01:33:42+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[region:us]</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>---\\nbase_model: liminerity/Mistral-quiet-star...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>| Link | Type | Size/GB | Notes |\\n|:-----|:--...</td>\n",
       "      <td>## About\\n\\n&lt;!-- ### quantize_version: 2 --&gt;\\n...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://gist.github.com/Artefact2/b5f81060077...</td>\n",
       "      <td>https://huggingface.co/liminerity/Mistral-quie...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194586 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  modelId  \\\n",
       "0                 sentence-transformers/all-mpnet-base-v2   \n",
       "2                  sentence-transformers/all-MiniLM-L6-v2   \n",
       "3                           google-bert/bert-base-uncased   \n",
       "4                            FacebookAI/xlm-roberta-large   \n",
       "5                                 openai/whisper-large-v2   \n",
       "...                                                   ...   \n",
       "1108731                              bau0221/1113_model_1   \n",
       "1108736                    benito14/1B_finetuned_llama3.2   \n",
       "1108740  async0x42/Qwen2.5-Coder-32B-Instruct-exl2_4.0bpw   \n",
       "1108750                     benito14/SOIT_Llama3.2_model4   \n",
       "1108757         mradermacher/Mistral-quiet-star-demo-GGUF   \n",
       "\n",
       "                        author             last_modified  downloads  likes  \\\n",
       "0        sentence-transformers 2024-11-05 15:25:48+00:00  391757489    874   \n",
       "2        sentence-transformers 2024-11-01 10:26:30+00:00   74110727   2526   \n",
       "3                  google-bert 2024-02-19 11:06:12+00:00   61569952   1897   \n",
       "4                   FacebookAI 2024-02-19 12:48:30+00:00   57186100    345   \n",
       "5                       openai 2024-02-29 10:57:50+00:00   44239856   1649   \n",
       "...                        ...                       ...        ...    ...   \n",
       "1108731                bau0221 2024-11-13 01:25:10+00:00          0      0   \n",
       "1108736               benito14 2024-11-13 01:27:44+00:00          0      0   \n",
       "1108740              async0x42 2024-11-13 01:28:25+00:00          0      0   \n",
       "1108750               benito14 2024-11-13 01:33:26+00:00          0      0   \n",
       "1108757           mradermacher 2024-11-13 01:33:42+00:00          0      0   \n",
       "\n",
       "                  library_name  \\\n",
       "0        sentence-transformers   \n",
       "2        sentence-transformers   \n",
       "3                 transformers   \n",
       "4                 transformers   \n",
       "5                 transformers   \n",
       "...                        ...   \n",
       "1108731           transformers   \n",
       "1108736           transformers   \n",
       "1108740                   None   \n",
       "1108750           transformers   \n",
       "1108757                   None   \n",
       "\n",
       "                                                      tags  \\\n",
       "0        [sentence-transformers, pytorch, onnx, safeten...   \n",
       "2        [sentence-transformers, pytorch, tf, rust, onn...   \n",
       "3        [transformers, pytorch, tf, jax, rust, coreml,...   \n",
       "4        [transformers, pytorch, tf, jax, onnx, safeten...   \n",
       "5        [transformers, pytorch, tf, jax, safetensors, ...   \n",
       "...                                                    ...   \n",
       "1108731  [transformers, safetensors, text-generation-in...   \n",
       "1108736  [transformers, safetensors, text-generation-in...   \n",
       "1108740                                        [region:us]   \n",
       "1108750  [transformers, gguf, llama, text-generation-in...   \n",
       "1108757                                        [region:us]   \n",
       "\n",
       "                         pipeline_tag   createdAt  \\\n",
       "0                 sentence-similarity  2022-03-02   \n",
       "2                 sentence-similarity  2022-03-02   \n",
       "3                           fill-mask  2022-03-02   \n",
       "4                           fill-mask  2022-03-02   \n",
       "5        automatic-speech-recognition  2022-12-05   \n",
       "...                               ...         ...   \n",
       "1108731                          None  2024-11-13   \n",
       "1108736                          None  2024-11-13   \n",
       "1108740                          None  2024-11-13   \n",
       "1108750                          None  2024-11-13   \n",
       "1108757                          None  2024-11-13   \n",
       "\n",
       "                                                      card  ...  \\\n",
       "0        ---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...  ...   \n",
       "2        ---\\nlanguage: en\\nlicense: apache-2.0\\nlibrar...  ...   \n",
       "3        ---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...  ...   \n",
       "4        ---\\ntags:\\n- exbert\\nlanguage:\\n- multilingua...  ...   \n",
       "5        ---\\nlanguage: \\n- en\\n- zh\\n- de\\n- es\\n- ru\\...  ...   \n",
       "...                                                    ...  ...   \n",
       "1108731  ---\\nbase_model: unsloth/phi-3.5-mini-instruct...  ...   \n",
       "1108736  ---\\nbase_model: unsloth/Llama-3.2-1B-bnb-4bit...  ...   \n",
       "1108740  ---\\nlicense: apache-2.0\\nlicense_link: https:...  ...   \n",
       "1108750  ---\\nbase_model: unsloth/Llama-3.2-1B-bnb-4bit...  ...   \n",
       "1108757  ---\\nbase_model: liminerity/Mistral-quiet-star...  ...   \n",
       "\n",
       "        is_default_card contains_markdown_table  \\\n",
       "0                 False                    True   \n",
       "2                 False                    True   \n",
       "3                 False                    True   \n",
       "4                 False                   False   \n",
       "5                 False                    True   \n",
       "...                 ...                     ...   \n",
       "1108731           False                   False   \n",
       "1108736           False                   False   \n",
       "1108740           False                   False   \n",
       "1108750           False                   False   \n",
       "1108757           False                    True   \n",
       "\n",
       "                                  extracted_markdown_table  \\\n",
       "0        | Dataset                                     ...   \n",
       "2        | Dataset                                     ...   \n",
       "3        | Model | #params | Language |\\n|-------------...   \n",
       "4                                                     None   \n",
       "5        | Size     | Parameters | English-only        ...   \n",
       "...                                                    ...   \n",
       "1108731                                               None   \n",
       "1108736                                               None   \n",
       "1108740                                               None   \n",
       "1108750                                               None   \n",
       "1108757  | Link | Type | Size/GB | Notes |\\n|:-----|:--...   \n",
       "\n",
       "                                             combined_text  \\\n",
       "0        # all-mpnet-base-v2\\nThis is a [sentence-trans...   \n",
       "2        # all-MiniLM-L6-v2\\nThis is a [sentence-transf...   \n",
       "3        # BERT base model (uncased)\\n\\nPretrained mode...   \n",
       "4        # XLM-RoBERTa (large-sized model) \\n\\nXLM-RoBE...   \n",
       "5        # Whisper\\n\\nWhisper is a pre-trained model fo...   \n",
       "...                                                    ...   \n",
       "1108731  # Uploaded  model\\n\\n- **Developed by:** bau02...   \n",
       "1108736  # Uploaded  model\\n\\n- **Developed by:** benit...   \n",
       "1108740  # Qwen2.5-Coder-32B-Instruct\\n\\n## Introductio...   \n",
       "1108750  # Uploaded  model\\n\\n- **Developed by:** benit...   \n",
       "1108757  ## About\\n\\n<!-- ### quantize_version: 2 -->\\n...   \n",
       "\n",
       "                                                  pdf_link  \\\n",
       "0        [https://arxiv.org/abs/1904.06472, https://arx...   \n",
       "2        [https://arxiv.org/abs/1904.06472, https://arx...   \n",
       "3        [https://arxiv.org/abs/1810.04805, http://arxi...   \n",
       "4        [https://arxiv.org/abs/1911.02116, http://arxi...   \n",
       "5        [https://arxiv.org/abs/2212.04356, https://cdn...   \n",
       "...                                                    ...   \n",
       "1108731                                               None   \n",
       "1108736                                               None   \n",
       "1108740  [https://arxiv.org/abs/2409.12186, https://arx...   \n",
       "1108750                                               None   \n",
       "1108757                                               None   \n",
       "\n",
       "                                               github_link  \\\n",
       "0        [https://github.com/PolyAI-LDN/conversational-...   \n",
       "2        [https://github.com/PolyAI-LDN/conversational-...   \n",
       "3        [https://github.com/google-research/bert, http...   \n",
       "4        [https://github.com/pytorch/fairseq/tree/maste...   \n",
       "5                      [https://github.com/openai/whisper]   \n",
       "...                                                    ...   \n",
       "1108731  [https://github.com/unslothai/unsloth, https:/...   \n",
       "1108736  [https://github.com/unslothai/unsloth, https:/...   \n",
       "1108740          [https://github.com/QwenLM/Qwen2.5-Coder]   \n",
       "1108750  [https://github.com/unslothai/unsloth, https:/...   \n",
       "1108757  [https://gist.github.com/Artefact2/b5f81060077...   \n",
       "\n",
       "                                                 all_links  \\\n",
       "0        https://www.SBERT.net, https://www.SBERT.net, ...   \n",
       "2        https://www.SBERT.net, https://www.SBERT.net, ...   \n",
       "3        https://arxiv.org/abs/1810.04805, https://gith...   \n",
       "4        https://arxiv.org/abs/1911.02116, https://gith...   \n",
       "5        https://arxiv.org/abs/2212.04356, https://gith...   \n",
       "...                                                    ...   \n",
       "1108731  https://github.com/unslothai/unsloth, https://...   \n",
       "1108736  https://github.com/unslothai/unsloth, https://...   \n",
       "1108740  https://qwenlm.github.io/blog/qwen2.5-coder-fa...   \n",
       "1108750  https://github.com/unslothai/unsloth, https://...   \n",
       "1108757  https://huggingface.co/liminerity/Mistral-quie...   \n",
       "\n",
       "        all_links_non_empty pdf_link_non_empty github_link_non_empty  \n",
       "0                      True               True                  True  \n",
       "2                      True               True                  True  \n",
       "3                      True               True                  True  \n",
       "4                      True               True                  True  \n",
       "5                      True               True                  True  \n",
       "...                     ...                ...                   ...  \n",
       "1108731                True              False                  True  \n",
       "1108736                True              False                  True  \n",
       "1108740                True               True                  True  \n",
       "1108750                True              False                  True  \n",
       "1108757                True              False                  True  \n",
       "\n",
       "[194586 rows x 33 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_split_temp[~df_split_temp['github_link'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1144                  Decius/botpress_Vaganet_new_model\n",
       "1362     tstadel/answer-classification-setfit-v2-binary\n",
       "1855                          nikcheerla/amd-partial-v1\n",
       "1858                nikcheerla/amd-partial-phonetree-v1\n",
       "1864                   nikcheerla/amd-full-phonetree-v1\n",
       "                              ...                      \n",
       "32962                nghodki/setfit-sre-task-classifier\n",
       "33259                           adriansanz/greeetings-2\n",
       "33273                                  adriansanz/gret3\n",
       "33275                                  adriansanz/gret4\n",
       "33279                                  adriansanz/gret5\n",
       "Name: modelId, Length: 2076, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed[df_processed['final_url']=='https://doi.org/10.48550/ARXIV.2209.11055']['modelId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries without pdf_type: 9076\n",
      "Sample BibTeX Entry Without PDF Type:\n",
      "@misc{louisbrulenaudet2024, author =       {Louis BrulÃ© Naudet}, title =        {Maxine-7B-0401-stock, an xtraordinary 7B model}, year =         {2024} howpublished = {\\url{https://huggingface.co/louisbrulenaudet/Maxine-7B-0401-stock}}, }\n"
     ]
    }
   ],
   "source": [
    "# Filter rows without pdf_type\n",
    "missing_pdf_type = df_processed[df_processed['pdf_type'].isnull()]\n",
    "\n",
    "# Print the number of rows without pdf_type\n",
    "print(f\"Entries without pdf_type: {len(missing_pdf_type)}\")\n",
    "\n",
    "# Print a sample row to inspect the bibtex_entry\n",
    "sample_row = missing_pdf_type.sample(1)\n",
    "print(\"Sample BibTeX Entry Without PDF Type:\")\n",
    "print(sample_row['bibtex_entry'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>bibtex_entry</th>\n",
       "      <th>pdf_type</th>\n",
       "      <th>pdf_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>@online{emb2024mxbai,\\n  title={Open Source St...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://www.mixedbread.ai/blog/mxbai-embed-lar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>121</td>\n",
       "      <td>@article{llama3modelcard,\\n\\n  title={Llama 3 ...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://github.com/meta-llama/llama3/blob/main...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>@article{llama3modelcard,\\n\\n  title={Llama 3 ...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://github.com/meta-llama/llama3/blob/main...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>220</td>\n",
       "      <td>@software{text2vec,\\n  author = {Xu Ming},\\n  ...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://github.com/shibing624/text2vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>345</td>\n",
       "      <td>@article{llama3modelcard,\\n\\n  title={Llama 3 ...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://github.com/meta-llama/llama3/blob/main...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100899</th>\n",
       "      <td>1100899</td>\n",
       "      <td>@software{bayram_2024_tr_tokenizer,\\n  author ...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://huggingface.co/alibayram/tr_tokenizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104817</th>\n",
       "      <td>1104817</td>\n",
       "      <td>@online{kexuefm-8847,\\n    title={CoSENT: A mo...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://kexue.fm/archives/8847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106196</th>\n",
       "      <td>1106196</td>\n",
       "      <td>@misc{sebastian_gabarain_2024,\\n  title = {Hyp...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://huggingface.co/datasets/Locutusque/hyp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106813</th>\n",
       "      <td>1106813</td>\n",
       "      <td>@misc{elyzallama2024,\\n      title={elyza/Llam...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107756</th>\n",
       "      <td>1107756</td>\n",
       "      <td>@misc{rinna-llama-3-youko-8b-instruct,\\n    ti...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://huggingface.co/rinna/llama-3-youko-8b-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2041 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index                                       bibtex_entry pdf_type  \\\n",
       "88            88  @online{emb2024mxbai,\\n  title={Open Source St...  unknown   \n",
       "121          121  @article{llama3modelcard,\\n\\n  title={Llama 3 ...  unknown   \n",
       "176          176  @article{llama3modelcard,\\n\\n  title={Llama 3 ...  unknown   \n",
       "220          220  @software{text2vec,\\n  author = {Xu Ming},\\n  ...  unknown   \n",
       "345          345  @article{llama3modelcard,\\n\\n  title={Llama 3 ...  unknown   \n",
       "...          ...                                                ...      ...   \n",
       "1100899  1100899  @software{bayram_2024_tr_tokenizer,\\n  author ...  unknown   \n",
       "1104817  1104817  @online{kexuefm-8847,\\n    title={CoSENT: A mo...  unknown   \n",
       "1106196  1106196  @misc{sebastian_gabarain_2024,\\n  title = {Hyp...  unknown   \n",
       "1106813  1106813  @misc{elyzallama2024,\\n      title={elyza/Llam...  unknown   \n",
       "1107756  1107756  @misc{rinna-llama-3-youko-8b-instruct,\\n    ti...  unknown   \n",
       "\n",
       "                                                  pdf_link  \n",
       "88       https://www.mixedbread.ai/blog/mxbai-embed-lar...  \n",
       "121      https://github.com/meta-llama/llama3/blob/main...  \n",
       "176      https://github.com/meta-llama/llama3/blob/main...  \n",
       "220                 https://github.com/shibing624/text2vec  \n",
       "345      https://github.com/meta-llama/llama3/blob/main...  \n",
       "...                                                    ...  \n",
       "1100899      https://huggingface.co/alibayram/tr_tokenizer  \n",
       "1104817                     https://kexue.fm/archives/8847  \n",
       "1106196  https://huggingface.co/datasets/Locutusque/hyp...  \n",
       "1106813   https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B  \n",
       "1107756  https://huggingface.co/rinna/llama-3-youko-8b-...  \n",
       "\n",
       "[2041 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed[(df_processed['pdf_type'].notna()) & (df_processed['pdf_type'].isin(['unknown']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['github.com', 'huggingface.co', '', 'www.aclweb.org', 'kexue.fm',\n",
       "       'www.databricks.com', 'nexusflow.ai', 'api.semanticscholar.org',\n",
       "       'cohere.com', 'sol.sbc.org.br', 'fauconnier.github.io',\n",
       "       'llava-vl.github.io', 'drive.google.com', 'www.microsoft.com',\n",
       "       'vicuna.lmsys.org', 'inria.hal.science', 'www.mixedbread.ai',\n",
       "       'riffusion.com', 'goo.gle', 'paloma.allen.ai', 'gradient.ai',\n",
       "       'arcinstitute.org', 'esemi.org', 'blog.salesforceairesearch.com',\n",
       "       'aihealth.site', 'www.silma.ai', 'swallow-llm.github.io',\n",
       "       'datacommons.org', 'https:', 'kb-labb.github.io', 'medium.com',\n",
       "       'ai.meta.com', 'clin33.uantwerpen.be', 'hal.science',\n",
       "       'www.techmahindra.com', 'getlinq.com', 'jordandarefsky.com', 'hf.co',\n",
       "       'www.github.com', 'www.clinjournal.org', 'kangaroogroup.github.io',\n",
       "       'hal.inria.fr', 'papers.ssrn.com', 'www.igi-global.com',\n",
       "       'huggingface.coLowerated', 'dh2022.dhii.asia', 'devmount.github.io',\n",
       "       'scholars.cityu.edu.hk', 'blog.devgenius.io', 'danjacobellis.net'],\n",
       "      dtype='object', name='domain')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "df_processed_clean = df_processed.copy()\n",
    "df_processed_clean['domain'] = df_processed_clean[df_processed_clean['pdf_type']=='unknown']['pdf_link'].apply(lambda x: urlparse(x).netloc if pd.notna(x) else None)\n",
    "# Count the domains\n",
    "domain_counts = df_processed_clean['domain'].value_counts()\n",
    "domain_counts.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### findings\n",
    "arxiv links can trust, doi links can trust, links starts with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Valid PDF link domains\n",
    "VALID_PDF_LINKS = [\n",
    "    \"arxiv.org\",\n",
    "    \"biorxiv.org\",\n",
    "    \"medrxiv.org\",\n",
    "    \"dl.acm.org\",\n",
    "    \"dblp.uni-trier.de\",\n",
    "    \"scholar.google.com\",\n",
    "    \"pubmed.ncbi.nlm.nih.gov\",\n",
    "    \"frontiersin.org\",\n",
    "    \"mdpi.com\",\n",
    "    \"cvpr.thecvf.com\",\n",
    "    \"nips.cc\",\n",
    "    \"icml.cc\",\n",
    "    \"ijcai.org\",\n",
    "    \"webofscience.com\",\n",
    "    \"journals.plos.org\",\n",
    "    \"nature.com\",\n",
    "    \"semanticscholar.org\",\n",
    "    \"chemrxiv.org\",\n",
    "    \"link.springer.com\",\n",
    "    \"ieeexplore.ieee.org\",\n",
    "    \"aaai.org\",\n",
    "    \"openaccess.thecvf.com\",\n",
    "]\n",
    "\n",
    "# Function to extract links from text\n",
    "def extract_links(text):\n",
    "    \"\"\"Extract PDF and GitHub links from the text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {\"pdf_link\": None, \"github_link\": None, \"all_links\": []}\n",
    "    \n",
    "    # Find all links (match https://, http://, and www.)\n",
    "    all_links = [link.strip(\".,)\") for link in re.findall(r\"(https?://\\S+|www\\.\\S+)\", text)]\n",
    "    \n",
    "    # Function to check if the link is a valid PDF link, excluding specific ones\n",
    "    def is_valid_pdf_link(link):\n",
    "        \"\"\"\n",
    "        Check if a link is a valid PDF link:\n",
    "        1. Matches one of the predefined VALID_PDF_LINKS domains;\n",
    "        2. Ends with \".pdf\";\n",
    "        3. Allows 'arxiv:1910.09700' but still requires other valid PDF links.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parsed_url = urlparse(link)\n",
    "            domain = parsed_url.netloc.lstrip(\"www.\")  # Remove \"www.\" prefix\n",
    "        except Exception:\n",
    "            return False  # Invalid link\n",
    "\n",
    "        return (domain in VALID_PDF_LINKS or link.lower().endswith(\".pdf\"))\n",
    "    pdf_links = [link for link in all_links if is_valid_pdf_link(link)]\n",
    "    github_links = [link for link in all_links if \"github.com\" in link]\n",
    "    has_arxiv_1910 = any(\"arxiv:1910.09700\" in link for link in all_links)\n",
    "    if has_arxiv_1910 and len(pdf_links) == 1 and \"arxiv:1910.09700\" in pdf_links:\n",
    "        pdf_links = []\n",
    "    return {\n",
    "        \"pdf_link\": pdf_links if pdf_links else None,\n",
    "        \"github_link\": github_links if github_links else None,\n",
    "        \"all_links\": all_links if all_links else None\n",
    "    }\n",
    "\n",
    "df_split_temp['combined_text'] = df_split_temp['card_readme'].fillna('')\n",
    "\n",
    "results = df_split_temp['combined_text'].apply(extract_links)\n",
    "\n",
    "df_split_temp['pdf_link'] = results.apply(lambda x: x[\"pdf_link\"] if x[\"pdf_link\"] else None)\n",
    "df_split_temp['github_link'] = results.apply(lambda x: x[\"github_link\"] if x[\"github_link\"] else None)\n",
    "df_split_temp['all_links'] = results.apply(lambda x: ', '.join(x[\"all_links\"]) if x[\"all_links\"] else None)\n",
    "\n",
    "df_split_temp['all_links_non_empty'] = df_split_temp['all_links'].apply(lambda x: bool(x) and x != '[]' and x != '')\n",
    "df_split_temp['pdf_link_non_empty'] = df_split_temp['pdf_link'].apply(lambda x: bool(x) and x != '[]' and x != '')\n",
    "df_split_temp['github_link_non_empty'] = df_split_temp['github_link'].apply(lambda x: bool(x) and x != '[]' and x != '')\n",
    "\n",
    "pdf_link_count = df_split_temp['pdf_link_non_empty'].sum()\n",
    "total_count = len(df_split_temp)\n",
    "pdf_link_ratio = (pdf_link_count / total_count) * 100\n",
    "\n",
    "all_link_count = df_split_temp['all_links_non_empty'].sum()\n",
    "all_link_ratio = (all_link_count / total_count) * 100\n",
    "\n",
    "github_link_count = df_split_temp['github_link_non_empty'].sum()\n",
    "github_link_ratio = (github_link_count / total_count) * 100\n",
    "\n",
    "no_pdf_has_github_count = df_split_temp[~df_split_temp['pdf_link_non_empty'] & df_split_temp['github_link_non_empty']].shape[0]\n",
    "no_pdf_has_github_ratio = (no_pdf_has_github_count / total_count) * 100\n",
    "\n",
    "print(f\"Model cards with all links: {all_link_count}/{total_count} = {all_link_ratio:.2f}%\")\n",
    "print(f\"Model cards with GitHub links: {github_link_count}/{total_count} = {github_link_ratio:.2f}%\")\n",
    "print(f\"Model cards with PDF links: {pdf_link_count}/{total_count} = {pdf_link_ratio:.2f}%\")\n",
    "print(f\"Model cards with NO PDF but HAS GitHub links: \"\n",
    "      f\"{no_pdf_has_github_count}/{total_count} = {no_pdf_has_github_ratio:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pdf link\n",
    "df_split_temp['pdf_link_flat'] = df_split_temp['pdf_link'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [x] if pd.notna(x) else []\n",
    ")\n",
    "df_split_temp_exploded = df_split_temp.explode('pdf_link_flat')\n",
    "link_counts = df_split_temp_exploded['pdf_link_flat'].value_counts()\n",
    "print(link_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_temp['extracted_markdown_table'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modellake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
