Since our tables come from a mix of unaudited GitHub and HuggingFace uploads, peer-reviewed arXiv papers, and plain-text S2ORC exports that can lose structural cues, we apply a simple yet rigorous Quality Control (QC) pipeline. This process includes dropping any table with zero rows or columns or those with negative dimensions. We also treat single-row HTML tables as equations, deduplicate tables across sources where authors repost or fork the same data, and strip out rows made up solely of dashed separators.

Following this QC process, we conduct a systematic analysis of our curated dataset through several processing stages. First, we identify all tables present in model cards, including duplicates, labeled as \textsc{Ours-dup} in Figure \ref{fig:stats}. To address redundancy, we implement a two-level deduplication strategy: first within individual resources to remove internal duplicates, then across resources to identify and analyze style overlaps between different sources. We then focus on tables with citation or URL titles, as these are crucial for establishing relatedness relationships in our ground truth dataset. These tables are further validated through the Semantic Scholar API to ensure their academic relevance and existence. The remaining crawled tables, while not included in our current ground truth, are preserved for potential future research. This multi-stage processing yields several key statistics: the total number of unique tables, the distribution of table sizes across different sources, and the overlap patterns between resources. These metrics, presented in Figure \ref{fig:stats}, provide insights into the composition and characteristics of our academic data lake.

The analysis reveals significant differences between our academic data lake and existing benchmarks such as \textsc{Santos-small}, \textsc{Santos-large}, \textsc{TUS-small}, and \textsc{TUS-large}. Our dataset, encompassing all tables from model cards and their associated URL resources, demonstrates a broader coverage but with distinct structural characteristics. While our total table count exceeds traditional benchmarks, individual tables tend to have fewer rows and columns. This structural difference stems from the academic nature of our sources: tables in research papers and model cards are designed to be concise and information-dense, often combining configuration details, performance metrics, and resource information in a compact format suitable for peer review and publication.

Our multi-stage filtering process reveals interesting patterns in data quality. The average number of rows actually increases after filtering, suggesting that high-quality academic tables require a minimum threshold of information density. The deduplication process further highlights the heterogeneous nature of our sources. Within Hugging Face, we identified 27,939 internal duplicates (16\% of total tables), while GitHub showed a higher deduplication rate of 40\% (1,628 duplicates out of 4,085 tables). Cross-resource analysis reveals minimal overlap between different sources, with only 1,344 tables shared between Hugging Face and GitHub, and no significant overlap between HTML and S2ORC+LLM sources. This low cross-source duplication rate (less than 1\% in most cases) suggests that each resource contributes unique content to our academic data lake.

The four primary sources in our dataset each serve distinct purposes: Hugging Face tables (146,270 unique) focus on model configurations and performance metrics, GitHub tables (2,457 unique) often contain implementation details and experimental results, HTML tables (41,917 unique) capture web-based documentation and tutorials, and S2ORC+LLM tables (31,539 unique) represent academic paper content. This diverse composition enables our dataset to capture the full spectrum of tabular data in machine learning research, from detailed technical specifications to concise experimental summaries. 